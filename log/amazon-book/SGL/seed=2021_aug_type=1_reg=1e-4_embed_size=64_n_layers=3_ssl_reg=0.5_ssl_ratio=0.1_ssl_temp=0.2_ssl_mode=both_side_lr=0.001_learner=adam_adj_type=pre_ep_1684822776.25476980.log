2023-05-23 13:19:36.255: Dataset name: amazon-book
The number of users: 52643
The number of items: 91599
The number of ratings: 2984108
Average actions of users: 56.69
Average actions of items: 32.58
The sparsity of the dataset: 99.938115%
2023-05-23 13:19:36.255: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=0.5
data.input.path=dataset
data.input.dataset=amazon-book
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=1
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

2023-05-23 13:19:42.835: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-05-23 13:19:58.190: 		0.00014628  	0.00027713  	0.00021710  	0.00053357  	0.00053357  
2023-05-23 13:25:38.146: [iter 1 : loss : 7.0296 = 0.6931 + 6.3365 + 0.0000, time: 334.076573]
2023-05-23 13:25:51.375: epoch 1:	0.00232150  	0.00492130  	0.00411532  	0.00777605  	0.00825032  
2023-05-23 13:25:51.376: Found a better model.
2023-05-23 13:25:51.376: Save model to file as pretrain.
2023-05-23 13:31:31.987: [iter 2 : loss : 7.0205 = 0.6929 + 6.3276 + 0.0000, time: 334.435012]
2023-05-23 13:31:43.559: epoch 2:	0.00278315  	0.00589164  	0.00504557  	0.01012243  	0.01100120  
2023-05-23 13:31:43.559: Found a better model.
2023-05-23 13:31:43.559: Save model to file as pretrain.
2023-05-23 13:37:19.754: [iter 3 : loss : 7.0194 = 0.6927 + 6.3267 + 0.0000, time: 329.889324]
2023-05-23 13:37:33.200: epoch 3:	0.00311467  	0.00652908  	0.00546648  	0.01041257  	0.01127350  
2023-05-23 13:37:33.200: Found a better model.
2023-05-23 13:37:33.200: Save model to file as pretrain.
2023-05-23 13:43:12.727: [iter 4 : loss : 7.0185 = 0.6923 + 6.3262 + 0.0000, time: 333.235813]
2023-05-23 13:43:25.855: epoch 4:	0.00307287  	0.00672200  	0.00536773  	0.00984643  	0.01062032  
2023-05-23 13:43:25.855: Found a better model.
2023-05-23 13:43:25.856: Save model to file as pretrain.
2023-05-23 13:49:00.740: [iter 5 : loss : 7.0177 = 0.6917 + 6.3260 + 0.0000, time: 328.594754]
2023-05-23 13:49:14.157: epoch 5:	0.00233859  	0.00533785  	0.00411511  	0.00693689  	0.00752170  
2023-05-23 13:54:52.778: [iter 6 : loss : 7.0164 = 0.6904 + 6.3260 + 0.0000, time: 332.991216]
2023-05-23 13:55:06.052: epoch 6:	0.00117403  	0.00282479  	0.00208684  	0.00313971  	0.00344814  
2023-05-23 14:00:42.843: [iter 7 : loss : 7.0141 = 0.6876 + 6.3265 + 0.0000, time: 331.190766]
2023-05-23 14:00:56.324: epoch 7:	0.00132506  	0.00317202  	0.00253099  	0.00343363  	0.00379048  
2023-05-23 14:06:35.025: [iter 8 : loss : 7.0078 = 0.6797 + 6.3280 + 0.0001, time: 333.090389]
2023-05-23 14:06:48.479: epoch 8:	0.00280878  	0.00677612  	0.00529595  	0.00759199  	0.00833396  
2023-05-23 14:06:48.479: Found a better model.
2023-05-23 14:06:48.479: Save model to file as pretrain.
2023-05-23 14:12:25.025: [iter 9 : loss : 6.9858 = 0.6526 + 6.3331 + 0.0001, time: 330.337036]
2023-05-23 14:12:36.627: epoch 9:	0.00497263  	0.01200421  	0.00945641  	0.01441229  	0.01585725  
2023-05-23 14:12:36.627: Found a better model.
2023-05-23 14:12:36.627: Save model to file as pretrain.
2023-05-23 14:18:13.974: [iter 10 : loss : 6.9286 = 0.5867 + 6.3415 + 0.0004, time: 331.167706]
2023-05-23 14:18:26.709: epoch 10:	0.00996941  	0.02368549  	0.01938807  	0.03364050  	0.03681165  
2023-05-23 14:18:26.709: Found a better model.
2023-05-23 14:18:26.709: Save model to file as pretrain.
2023-05-23 14:24:04.885: [iter 11 : loss : 6.8231 = 0.4696 + 6.3527 + 0.0008, time: 331.856682]
2023-05-23 14:24:16.734: epoch 11:	0.01553954  	0.03644754  	0.02989491  	0.05259540  	0.05802646  
2023-05-23 14:24:16.734: Found a better model.
2023-05-23 14:24:16.734: Save model to file as pretrain.
2023-05-23 14:29:54.557: [iter 12 : loss : 6.7056 = 0.3430 + 6.3612 + 0.0015, time: 331.614151]
2023-05-23 14:30:07.146: epoch 12:	0.01843334  	0.04326288  	0.03511479  	0.06146461  	0.06789996  
2023-05-23 14:30:07.146: Found a better model.
2023-05-23 14:30:07.146: Save model to file as pretrain.
2023-05-23 14:35:44.471: [iter 13 : loss : 6.6118 = 0.2458 + 6.3638 + 0.0022, time: 331.047356]
2023-05-23 14:35:57.328: epoch 13:	0.01938017  	0.04584155  	0.03679743  	0.06386223  	0.07048844  
2023-05-23 14:35:57.328: Found a better model.
2023-05-23 14:35:57.328: Save model to file as pretrain.
2023-05-23 14:41:36.309: [iter 14 : loss : 6.5451 = 0.1794 + 6.3627 + 0.0030, time: 332.646271]
2023-05-23 14:41:47.127: epoch 14:	0.01959199  	0.04676280  	0.03739278  	0.06493400  	0.07179952  
2023-05-23 14:41:47.127: Found a better model.
2023-05-23 14:41:47.127: Save model to file as pretrain.
2023-05-23 14:47:25.313: [iter 15 : loss : 6.4985 = 0.1352 + 6.3596 + 0.0036, time: 331.927263]
2023-05-23 14:47:38.410: epoch 15:	0.01966134  	0.04716170  	0.03756998  	0.06487056  	0.07179391  
2023-05-23 14:47:38.410: Found a better model.
2023-05-23 14:47:38.410: Save model to file as pretrain.
2023-05-23 14:53:16.845: [iter 16 : loss : 6.4659 = 0.1055 + 6.3562 + 0.0043, time: 332.165943]
2023-05-23 14:53:28.149: epoch 16:	0.01964038  	0.04735476  	0.03765564  	0.06520276  	0.07203185  
2023-05-23 14:53:28.149: Found a better model.
2023-05-23 14:53:28.149: Save model to file as pretrain.
2023-05-23 14:59:05.422: [iter 17 : loss : 6.4425 = 0.0846 + 6.3530 + 0.0049, time: 331.046275]
2023-05-23 14:59:17.394: epoch 17:	0.01963658  	0.04736662  	0.03762970  	0.06526454  	0.07209521  
2023-05-23 14:59:17.394: Found a better model.
2023-05-23 14:59:17.394: Save model to file as pretrain.
2023-05-23 15:04:54.919: [iter 18 : loss : 6.4252 = 0.0697 + 6.3501 + 0.0054, time: 331.341186]
2023-05-23 15:05:05.709: epoch 18:	0.01956910  	0.04724623  	0.03745614  	0.06472498  	0.07163353  
2023-05-23 15:10:42.969: [iter 19 : loss : 6.4124 = 0.0587 + 6.3477 + 0.0060, time: 331.678057]
2023-05-23 15:10:54.428: epoch 19:	0.01939240  	0.04681657  	0.03721403  	0.06466004  	0.07157815  
2023-05-23 15:16:32.171: [iter 20 : loss : 6.4025 = 0.0503 + 6.3457 + 0.0065, time: 332.127461]
2023-05-23 15:16:42.861: epoch 20:	0.01925278  	0.04649447  	0.03694336  	0.06430528  	0.07109202  
2023-05-23 15:22:20.325: [iter 21 : loss : 6.3950 = 0.0441 + 6.3440 + 0.0069, time: 331.871613]
2023-05-23 15:22:33.442: epoch 21:	0.01910273  	0.04611331  	0.03668818  	0.06408034  	0.07075465  
2023-05-23 15:28:10.489: [iter 22 : loss : 6.3887 = 0.0389 + 6.3425 + 0.0073, time: 331.440116]
2023-05-23 15:28:20.432: epoch 22:	0.01889376  	0.04567237  	0.03638162  	0.06377836  	0.07035461  
2023-05-23 15:33:56.722: [iter 23 : loss : 6.3842 = 0.0350 + 6.3414 + 0.0077, time: 331.175560]
2023-05-23 15:34:08.581: epoch 23:	0.01874845  	0.04531284  	0.03607907  	0.06334832  	0.06972520  
2023-05-23 15:39:45.898: [iter 24 : loss : 6.3801 = 0.0317 + 6.3404 + 0.0081, time: 331.736660]
2023-05-23 15:39:59.167: epoch 24:	0.01859743  	0.04493080  	0.03576875  	0.06289447  	0.06918360  
2023-05-23 15:45:38.573: [iter 25 : loss : 6.3769 = 0.0290 + 6.3394 + 0.0084, time: 333.806275]
2023-05-23 15:45:51.912: epoch 25:	0.01842077  	0.04445306  	0.03536946  	0.06234250  	0.06846987  
2023-05-23 15:51:31.589: [iter 26 : loss : 6.3742 = 0.0267 + 6.3387 + 0.0087, time: 334.017346]
2023-05-23 15:51:44.145: epoch 26:	0.01829920  	0.04413717  	0.03514027  	0.06204154  	0.06814210  
2023-05-23 15:57:23.912: [iter 27 : loss : 6.3719 = 0.0248 + 6.3381 + 0.0090, time: 334.083229]
2023-05-23 15:57:37.229: epoch 27:	0.01815674  	0.04379898  	0.03486132  	0.06161128  	0.06772531  
2023-05-23 16:03:12.841: [iter 28 : loss : 6.3702 = 0.0233 + 6.3375 + 0.0093, time: 329.964714]
2023-05-23 16:03:27.478: epoch 28:	0.01798389  	0.04328560  	0.03449281  	0.06109446  	0.06709906  
2023-05-23 16:09:06.063: [iter 29 : loss : 6.3683 = 0.0218 + 6.3369 + 0.0096, time: 332.906507]
2023-05-23 16:09:17.767: epoch 29:	0.01783670  	0.04293200  	0.03414845  	0.06050023  	0.06626983  
2023-05-23 16:14:55.254: [iter 30 : loss : 6.3667 = 0.0206 + 6.3364 + 0.0098, time: 331.758126]
2023-05-23 16:15:09.481: epoch 30:	0.01770943  	0.04256336  	0.03394595  	0.06040628  	0.06611127  
2023-05-23 16:20:43.990: [iter 31 : loss : 6.3657 = 0.0196 + 6.3360 + 0.0100, time: 329.313366]
2023-05-23 16:20:55.659: epoch 31:	0.01755937  	0.04224914  	0.03366463  	0.06000487  	0.06555721  
2023-05-23 16:26:32.762: [iter 32 : loss : 6.3645 = 0.0186 + 6.3356 + 0.0103, time: 331.386524]
2023-05-23 16:26:43.478: epoch 32:	0.01739791  	0.04183643  	0.03336972  	0.05967063  	0.06515747  
2023-05-23 16:32:22.661: [iter 33 : loss : 6.3637 = 0.0179 + 6.3353 + 0.0105, time: 333.521720]
2023-05-23 16:32:34.466: epoch 33:	0.01732004  	0.04165755  	0.03322561  	0.05953892  	0.06500730  
2023-05-23 16:38:16.311: [iter 34 : loss : 6.3629 = 0.0171 + 6.3351 + 0.0106, time: 336.091173]
2023-05-23 16:38:29.004: epoch 34:	0.01718897  	0.04134142  	0.03297009  	0.05906683  	0.06447129  
2023-05-23 16:44:10.266: [iter 35 : loss : 6.3620 = 0.0165 + 6.3347 + 0.0108, time: 335.546227]
2023-05-23 16:44:21.456: epoch 35:	0.01703987  	0.04091048  	0.03272632  	0.05887034  	0.06421299  
2023-05-23 16:50:00.458: [iter 36 : loss : 6.3613 = 0.0159 + 6.3344 + 0.0110, time: 333.281868]
2023-05-23 16:50:13.575: epoch 36:	0.01695154  	0.04064195  	0.03248091  	0.05842236  	0.06373417  
2023-05-23 16:55:54.495: [iter 37 : loss : 6.3610 = 0.0154 + 6.3344 + 0.0112, time: 335.287574]
2023-05-23 16:56:05.733: epoch 37:	0.01686415  	0.04048420  	0.03233253  	0.05822272  	0.06344524  
2023-05-23 17:01:46.098: [iter 38 : loss : 6.3603 = 0.0150 + 6.3341 + 0.0113, time: 334.686598]
2023-05-23 17:01:57.798: epoch 38:	0.01677678  	0.04021276  	0.03213270  	0.05786275  	0.06318166  
2023-05-23 17:07:37.300: [iter 39 : loss : 6.3598 = 0.0144 + 6.3339 + 0.0114, time: 333.855983]
2023-05-23 17:07:50.214: epoch 39:	0.01669320  	0.04001030  	0.03199463  	0.05775823  	0.06302559  
2023-05-23 17:13:30.748: [iter 40 : loss : 6.3592 = 0.0140 + 6.3335 + 0.0116, time: 334.839497]
2023-05-23 17:13:42.458: epoch 40:	0.01660393  	0.03975666  	0.03178444  	0.05739352  	0.06255706  
2023-05-23 17:19:23.555: [iter 41 : loss : 6.3589 = 0.0137 + 6.3335 + 0.0117, time: 335.447417]
2023-05-23 17:19:36.765: epoch 41:	0.01648807  	0.03947684  	0.03162547  	0.05726676  	0.06245182  
2023-05-23 17:25:15.740: [iter 42 : loss : 6.3586 = 0.0134 + 6.3334 + 0.0118, time: 333.269715]
2023-05-23 17:25:27.169: epoch 42:	0.01645197  	0.03936317  	0.03153385  	0.05718398  	0.06233810  
2023-05-23 17:31:09.996: [iter 43 : loss : 6.3582 = 0.0131 + 6.3332 + 0.0119, time: 337.216390]
2023-05-23 17:31:23.003: epoch 43:	0.01641968  	0.03922717  	0.03139309  	0.05686562  	0.06197420  
2023-05-23 17:37:02.217: [iter 44 : loss : 6.3578 = 0.0127 + 6.3331 + 0.0121, time: 333.629837]
2023-05-23 17:37:15.441: epoch 44:	0.01632756  	0.03902414  	0.03121417  	0.05660652  	0.06162222  
2023-05-23 17:42:56.568: [iter 45 : loss : 6.3576 = 0.0125 + 6.3330 + 0.0122, time: 335.494915]
2023-05-23 17:43:08.259: epoch 45:	0.01627913  	0.03890041  	0.03108951  	0.05627160  	0.06132289  
2023-05-23 17:48:47.735: [iter 46 : loss : 6.3575 = 0.0124 + 6.3329 + 0.0122, time: 333.949063]
2023-05-23 17:48:59.248: epoch 46:	0.01623543  	0.03878902  	0.03102914  	0.05638278  	0.06134746  
2023-05-23 17:54:37.342: [iter 47 : loss : 6.3569 = 0.0120 + 6.3326 + 0.0123, time: 332.494105]
2023-05-23 17:54:48.127: epoch 47:	0.01615660  	0.03858352  	0.03084064  	0.05604614  	0.06099749  
2023-05-23 18:00:25.435: [iter 48 : loss : 6.3568 = 0.0118 + 6.3325 + 0.0124, time: 331.621198]
2023-05-23 18:00:37.297: epoch 48:	0.01606543  	0.03834690  	0.03070843  	0.05592248  	0.06086058  
2023-05-23 18:06:15.826: [iter 49 : loss : 6.3568 = 0.0116 + 6.3326 + 0.0125, time: 332.934280]
2023-05-23 18:06:26.391: epoch 49:	0.01603504  	0.03828669  	0.03064629  	0.05573183  	0.06073691  
2023-05-23 18:12:03.237: [iter 50 : loss : 6.3565 = 0.0114 + 6.3324 + 0.0126, time: 331.254684]
2023-05-23 18:12:14.760: epoch 50:	0.01598756  	0.03816635  	0.03055981  	0.05561426  	0.06061016  
2023-05-23 18:17:54.400: [iter 51 : loss : 6.3564 = 0.0113 + 6.3324 + 0.0127, time: 334.033195]
2023-05-23 18:18:06.470: epoch 51:	0.01592583  	0.03800946  	0.03038755  	0.05525174  	0.06015075  
2023-05-23 18:23:43.282: [iter 52 : loss : 6.3561 = 0.0111 + 6.3322 + 0.0127, time: 331.625573]
2023-05-23 18:23:55.308: epoch 52:	0.01585555  	0.03782304  	0.03024895  	0.05500359  	0.05989290  
2023-05-23 18:29:35.227: [iter 53 : loss : 6.3560 = 0.0110 + 6.3322 + 0.0128, time: 334.247743]
2023-05-23 18:29:47.850: epoch 53:	0.01582799  	0.03769254  	0.03015037  	0.05494194  	0.05974651  
2023-05-23 18:35:26.142: [iter 54 : loss : 6.3560 = 0.0108 + 6.3323 + 0.0129, time: 332.638694]
2023-05-23 18:35:39.519: epoch 54:	0.01576721  	0.03754668  	0.03001324  	0.05467641  	0.05951952  
2023-05-23 18:41:15.476: [iter 55 : loss : 6.3556 = 0.0106 + 6.3321 + 0.0129, time: 330.291397]
2023-05-23 18:41:28.671: epoch 55:	0.01574158  	0.03743756  	0.02992980  	0.05463575  	0.05944182  
2023-05-23 18:47:07.414: [iter 56 : loss : 6.3555 = 0.0107 + 6.3319 + 0.0130, time: 333.017316]
2023-05-23 18:47:20.975: epoch 56:	0.01569883  	0.03732425  	0.02988618  	0.05452946  	0.05942989  
2023-05-23 18:53:00.975: [iter 57 : loss : 6.3553 = 0.0104 + 6.3318 + 0.0130, time: 334.306450]
2023-05-23 18:53:14.069: epoch 57:	0.01560196  	0.03709251  	0.02972934  	0.05445640  	0.05932695  
2023-05-23 18:58:52.292: [iter 58 : loss : 6.3553 = 0.0103 + 6.3319 + 0.0131, time: 332.570240]
2023-05-23 18:59:03.910: epoch 58:	0.01554307  	0.03693913  	0.02961579  	0.05431381  	0.05908947  
2023-05-23 19:04:42.974: [iter 59 : loss : 6.3551 = 0.0101 + 6.3319 + 0.0131, time: 333.367046]
2023-05-23 19:04:55.888: epoch 59:	0.01556396  	0.03697694  	0.02962634  	0.05426719  	0.05901547  
2023-05-23 19:10:32.288: [iter 60 : loss : 6.3551 = 0.0101 + 6.3318 + 0.0132, time: 330.669887]
2023-05-23 19:10:45.507: epoch 60:	0.01549367  	0.03680788  	0.02952577  	0.05411689  	0.05892370  
2023-05-23 19:16:25.767: [iter 61 : loss : 6.3550 = 0.0100 + 6.3318 + 0.0132, time: 334.516978]
2023-05-23 19:16:37.560: epoch 61:	0.01540440  	0.03658002  	0.02938372  	0.05401845  	0.05875070  
2023-05-23 19:22:16.396: [iter 62 : loss : 6.3549 = 0.0099 + 6.3317 + 0.0133, time: 333.125403]
2023-05-23 19:22:29.634: epoch 62:	0.01539396  	0.03654343  	0.02935095  	0.05379212  	0.05861272  
2023-05-23 19:28:08.863: [iter 63 : loss : 6.3549 = 0.0099 + 6.3318 + 0.0133, time: 333.537435]
2023-05-23 19:28:20.968: epoch 63:	0.01535311  	0.03646124  	0.02921628  	0.05347940  	0.05814758  
2023-05-23 19:34:00.287: [iter 64 : loss : 6.3547 = 0.0096 + 6.3317 + 0.0134, time: 333.554908]
2023-05-23 19:34:15.211: epoch 64:	0.01533792  	0.03642190  	0.02919299  	0.05358097  	0.05819383  
2023-05-23 19:39:53.442: [iter 65 : loss : 6.3547 = 0.0097 + 6.3316 + 0.0134, time: 332.446361]
2023-05-23 19:40:06.539: epoch 65:	0.01531703  	0.03639501  	0.02915255  	0.05333574  	0.05810712  
2023-05-23 19:45:44.872: [iter 66 : loss : 6.3545 = 0.0095 + 6.3315 + 0.0134, time: 332.625169]
2023-05-23 19:45:59.689: epoch 66:	0.01527713  	0.03628937  	0.02903478  	0.05319843  	0.05785940  
2023-05-23 19:51:39.041: [iter 67 : loss : 6.3543 = 0.0094 + 6.3315 + 0.0135, time: 333.560205]
2023-05-23 19:51:50.985: epoch 67:	0.01524673  	0.03618382  	0.02898653  	0.05315979  	0.05780513  
2023-05-23 19:51:50.985: Early stopping is trigger at epoch: 67
2023-05-23 19:51:50.985: best_result@epoch 17:

2023-05-23 19:51:50.985: Loading from the saved model.
2023-05-23 19:52:02.988: 		0.01963658  	0.04736662  	0.03762970  	0.06526454  	0.07209521  
