2023-06-01 20:20:30.296: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-01 20:20:30.296: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=0.5
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=1
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=2
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

2023-06-01 20:20:33.989: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-01 20:20:38.547: 		0.00028578  	0.00059422  	0.00044439  	0.00098633  	0.00098831  
2023-06-01 20:22:13.854: [iter 1 : loss : 1.8082 = 0.6931 + 1.1151 + 0.0000, time: 92.447252]
2023-06-01 20:22:18.325: epoch 1:	0.00200042  	0.00406605  	0.00341162  	0.00774359  	0.00794650  
2023-06-01 20:22:18.325: Found a better model.
2023-06-01 20:22:18.325: Save model to file as pretrain.
2023-06-01 20:23:52.322: [iter 2 : loss : 1.8060 = 0.6930 + 1.1130 + 0.0000, time: 90.128448]
2023-06-01 20:23:56.481: epoch 2:	0.00306780  	0.00623939  	0.00524413  	0.01177362  	0.01237395  
2023-06-01 20:23:56.481: Found a better model.
2023-06-01 20:23:56.481: Save model to file as pretrain.
2023-06-01 20:25:30.921: [iter 3 : loss : 1.8057 = 0.6929 + 1.1128 + 0.0000, time: 90.433341]
2023-06-01 20:25:35.167: epoch 3:	0.00405305  	0.00825258  	0.00697021  	0.01516457  	0.01621406  
2023-06-01 20:25:35.167: Found a better model.
2023-06-01 20:25:35.167: Save model to file as pretrain.
2023-06-01 20:27:10.275: [iter 4 : loss : 1.8054 = 0.6927 + 1.1127 + 0.0000, time: 90.988792]
2023-06-01 20:27:14.461: epoch 4:	0.00497990  	0.01034647  	0.00866946  	0.01811907  	0.01944532  
2023-06-01 20:27:14.461: Found a better model.
2023-06-01 20:27:14.461: Save model to file as pretrain.
2023-06-01 20:28:50.899: [iter 5 : loss : 1.8051 = 0.6924 + 1.1127 + 0.0000, time: 92.563277]
2023-06-01 20:28:54.727: epoch 5:	0.00551516  	0.01181860  	0.00969142  	0.01925546  	0.02087326  
2023-06-01 20:28:54.727: Found a better model.
2023-06-01 20:28:54.727: Save model to file as pretrain.
2023-06-01 20:30:29.997: [iter 6 : loss : 1.8044 = 0.6916 + 1.1128 + 0.0000, time: 91.509452]
2023-06-01 20:30:33.179: epoch 6:	0.00400725  	0.00892103  	0.00687996  	0.01259258  	0.01374914  
2023-06-01 20:32:06.407: [iter 7 : loss : 1.8018 = 0.6885 + 1.1133 + 0.0000, time: 90.754760]
2023-06-01 20:32:10.450: epoch 7:	0.00467040  	0.01049087  	0.00823067  	0.01470238  	0.01641595  
2023-06-01 20:33:44.332: [iter 8 : loss : 1.7932 = 0.6777 + 1.1155 + 0.0000, time: 91.069031]
2023-06-01 20:33:47.945: epoch 8:	0.00815984  	0.01845657  	0.01517435  	0.02673206  	0.03064414  
2023-06-01 20:33:47.945: Found a better model.
2023-06-01 20:33:47.946: Save model to file as pretrain.
2023-06-01 20:35:23.943: [iter 9 : loss : 1.7549 = 0.6305 + 1.1242 + 0.0002, time: 92.033559]
2023-06-01 20:35:28.051: epoch 9:	0.01354799  	0.03045722  	0.02499547  	0.04479693  	0.05092058  
2023-06-01 20:35:28.051: Found a better model.
2023-06-01 20:35:28.051: Save model to file as pretrain.
2023-06-01 20:37:03.506: [iter 10 : loss : 1.6761 = 0.5373 + 1.1383 + 0.0004, time: 91.589342]
2023-06-01 20:37:07.519: epoch 10:	0.02312554  	0.05017361  	0.04159975  	0.07732004  	0.08704108  
2023-06-01 20:37:07.519: Found a better model.
2023-06-01 20:37:07.519: Save model to file as pretrain.
2023-06-01 20:38:42.430: [iter 11 : loss : 1.5669 = 0.4097 + 1.1562 + 0.0009, time: 90.786383]
2023-06-01 20:38:46.718: epoch 11:	0.02695364  	0.05957134  	0.04889333  	0.09016973  	0.10126566  
2023-06-01 20:38:46.718: Found a better model.
2023-06-01 20:38:46.718: Save model to file as pretrain.
2023-06-01 20:40:23.092: [iter 12 : loss : 1.4456 = 0.2699 + 1.1741 + 0.0015, time: 92.464771]
2023-06-01 20:40:27.408: epoch 12:	0.02802557  	0.06192472  	0.05086405  	0.09376395  	0.10503577  
2023-06-01 20:40:27.408: Found a better model.
2023-06-01 20:40:27.408: Save model to file as pretrain.
2023-06-01 20:42:02.254: [iter 13 : loss : 1.3618 = 0.1832 + 1.1765 + 0.0022, time: 91.092446]
2023-06-01 20:42:06.395: epoch 13:	0.02874864  	0.06379337  	0.05244011  	0.09667795  	0.10833915  
2023-06-01 20:42:06.395: Found a better model.
2023-06-01 20:42:06.396: Save model to file as pretrain.
2023-06-01 20:43:41.076: [iter 14 : loss : 1.3113 = 0.1383 + 1.1702 + 0.0027, time: 90.847575]
2023-06-01 20:43:45.346: epoch 14:	0.02941958  	0.06534585  	0.05372490  	0.09867874  	0.11066663  
2023-06-01 20:43:45.346: Found a better model.
2023-06-01 20:43:45.346: Save model to file as pretrain.
2023-06-01 20:45:20.141: [iter 15 : loss : 1.2778 = 0.1115 + 1.1631 + 0.0033, time: 90.935124]
2023-06-01 20:45:24.482: epoch 15:	0.02979057  	0.06622707  	0.05450619  	0.10002368  	0.11230738  
2023-06-01 20:45:24.482: Found a better model.
2023-06-01 20:45:24.482: Save model to file as pretrain.
2023-06-01 20:47:00.599: [iter 16 : loss : 1.2539 = 0.0934 + 1.1568 + 0.0037, time: 92.181265]
2023-06-01 20:47:04.343: epoch 16:	0.03001950  	0.06669097  	0.05495693  	0.10082643  	0.11318375  
2023-06-01 20:47:04.343: Found a better model.
2023-06-01 20:47:04.343: Save model to file as pretrain.
2023-06-01 20:48:39.915: [iter 17 : loss : 1.2367 = 0.0808 + 1.1517 + 0.0042, time: 91.596223]
2023-06-01 20:48:43.418: epoch 17:	0.03009843  	0.06708515  	0.05522981  	0.10132690  	0.11391444  
2023-06-01 20:48:43.418: Found a better model.
2023-06-01 20:48:43.418: Save model to file as pretrain.
2023-06-01 20:50:21.294: [iter 18 : loss : 1.2228 = 0.0707 + 1.1475 + 0.0046, time: 94.090259]
2023-06-01 20:50:25.546: epoch 18:	0.03027364  	0.06753736  	0.05548657  	0.10166935  	0.11427653  
2023-06-01 20:50:25.547: Found a better model.
2023-06-01 20:50:25.547: Save model to file as pretrain.
2023-06-01 20:52:00.988: [iter 19 : loss : 1.2120 = 0.0631 + 1.1440 + 0.0050, time: 91.534759]
2023-06-01 20:52:05.215: epoch 19:	0.03039995  	0.06784521  	0.05570447  	0.10179718  	0.11472362  
2023-06-01 20:52:05.215: Found a better model.
2023-06-01 20:52:05.215: Save model to file as pretrain.
2023-06-01 20:53:41.967: [iter 20 : loss : 1.2032 = 0.0568 + 1.1411 + 0.0053, time: 92.861409]
2023-06-01 20:53:46.059: epoch 20:	0.03042359  	0.06785671  	0.05571824  	0.10170442  	0.11492888  
2023-06-01 20:53:46.059: Found a better model.
2023-06-01 20:53:46.059: Save model to file as pretrain.
2023-06-01 20:55:22.179: [iter 21 : loss : 1.1959 = 0.0515 + 1.1386 + 0.0057, time: 92.187267]
2023-06-01 20:55:26.269: epoch 21:	0.03042675  	0.06773216  	0.05572110  	0.10172489  	0.11504649  
2023-06-01 20:57:00.661: [iter 22 : loss : 1.1896 = 0.0470 + 1.1366 + 0.0060, time: 91.659604]
2023-06-01 20:57:04.724: epoch 22:	0.03035729  	0.06744335  	0.05552221  	0.10145596  	0.11453451  
2023-06-01 20:58:39.516: [iter 23 : loss : 1.1846 = 0.0435 + 1.1348 + 0.0063, time: 92.061222]
2023-06-01 20:58:43.537: epoch 23:	0.03028939  	0.06726450  	0.05536744  	0.10125228  	0.11423451  
2023-06-01 21:00:17.525: [iter 24 : loss : 1.1806 = 0.0408 + 1.1332 + 0.0066, time: 91.258452]
2023-06-01 21:00:21.703: epoch 24:	0.03019311  	0.06705384  	0.05519296  	0.10101611  	0.11387601  
2023-06-01 21:01:56.475: [iter 25 : loss : 1.1767 = 0.0379 + 1.1319 + 0.0069, time: 92.028019]
2023-06-01 21:02:00.536: epoch 25:	0.03015525  	0.06672678  	0.05505341  	0.10070367  	0.11368047  
2023-06-01 21:03:34.924: [iter 26 : loss : 1.1733 = 0.0354 + 1.1307 + 0.0072, time: 91.631559]
2023-06-01 21:03:38.903: epoch 26:	0.03015681  	0.06673813  	0.05497967  	0.10062053  	0.11344352  
2023-06-01 21:05:13.761: [iter 27 : loss : 1.1704 = 0.0333 + 1.1297 + 0.0074, time: 92.143063]
2023-06-01 21:05:17.660: epoch 27:	0.02999738  	0.06629077  	0.05468263  	0.10032240  	0.11301342  
2023-06-01 21:06:53.289: [iter 28 : loss : 1.1681 = 0.0317 + 1.1287 + 0.0077, time: 92.905890]
2023-06-01 21:06:57.206: epoch 28:	0.02983949  	0.06600206  	0.05438158  	0.09993013  	0.11245018  
2023-06-01 21:08:32.135: [iter 29 : loss : 1.1660 = 0.0302 + 1.1279 + 0.0079, time: 92.183070]
2023-06-01 21:08:36.188: epoch 29:	0.02970058  	0.06567989  	0.05415583  	0.09979632  	0.11231299  
2023-06-01 21:10:11.888: [iter 30 : loss : 1.1640 = 0.0288 + 1.1272 + 0.0081, time: 92.958088]
2023-06-01 21:10:15.898: epoch 30:	0.02961216  	0.06535459  	0.05395827  	0.09953842  	0.11206014  
2023-06-01 21:11:50.135: [iter 31 : loss : 1.1623 = 0.0275 + 1.1265 + 0.0083, time: 91.470495]
2023-06-01 21:11:54.101: epoch 31:	0.02952848  	0.06506906  	0.05376309  	0.09931850  	0.11173955  
2023-06-01 21:13:29.571: [iter 32 : loss : 1.1608 = 0.0264 + 1.1259 + 0.0085, time: 92.764056]
2023-06-01 21:13:33.751: epoch 32:	0.02943534  	0.06479222  	0.05353905  	0.09895211  	0.11128699  
2023-06-01 21:15:08.088: [iter 33 : loss : 1.1595 = 0.0255 + 1.1253 + 0.0087, time: 91.610528]
2023-06-01 21:15:12.366: epoch 33:	0.02932482  	0.06459056  	0.05332443  	0.09858268  	0.11092969  
2023-06-01 21:16:47.843: [iter 34 : loss : 1.1583 = 0.0245 + 1.1248 + 0.0089, time: 92.707491]
2023-06-01 21:16:51.792: epoch 34:	0.02926324  	0.06433024  	0.05313973  	0.09825103  	0.11047246  
2023-06-01 21:18:26.174: [iter 35 : loss : 1.1568 = 0.0234 + 1.1243 + 0.0091, time: 91.654303]
2023-06-01 21:18:30.557: epoch 35:	0.02916694  	0.06402771  	0.05291862  	0.09814806  	0.11011993  
2023-06-01 21:20:05.378: [iter 36 : loss : 1.1557 = 0.0226 + 1.1239 + 0.0093, time: 92.110510]
2023-06-01 21:20:09.542: epoch 36:	0.02905484  	0.06374010  	0.05271339  	0.09785208  	0.10979118  
2023-06-01 21:21:43.844: [iter 37 : loss : 1.1549 = 0.0219 + 1.1235 + 0.0094, time: 91.575033]
2023-06-01 21:21:47.510: epoch 37:	0.02895223  	0.06348060  	0.05257070  	0.09777013  	0.10979389  
2023-06-01 21:23:21.873: [iter 38 : loss : 1.1541 = 0.0213 + 1.1232 + 0.0096, time: 91.599257]
2023-06-01 21:23:25.977: epoch 38:	0.02886067  	0.06322911  	0.05233737  	0.09733951  	0.10920352  
2023-06-01 21:25:00.291: [iter 39 : loss : 1.1532 = 0.0207 + 1.1228 + 0.0097, time: 91.554827]
2023-06-01 21:25:04.291: epoch 39:	0.02875017  	0.06306632  	0.05212533  	0.09677368  	0.10879675  
2023-06-01 21:26:39.245: [iter 40 : loss : 1.1523 = 0.0199 + 1.1225 + 0.0099, time: 92.243160]
2023-06-01 21:26:43.312: epoch 40:	0.02865861  	0.06277518  	0.05192556  	0.09651917  	0.10849227  
2023-06-01 21:28:17.709: [iter 41 : loss : 1.1519 = 0.0197 + 1.1222 + 0.0100, time: 91.609389]
2023-06-01 21:28:21.665: epoch 41:	0.02852285  	0.06248020  	0.05170013  	0.09622219  	0.10812497  
2023-06-01 21:29:56.620: [iter 42 : loss : 1.1511 = 0.0191 + 1.1219 + 0.0102, time: 92.220731]
2023-06-01 21:30:00.897: epoch 42:	0.02842813  	0.06221194  	0.05145108  	0.09566674  	0.10757506  
2023-06-01 21:31:35.206: [iter 43 : loss : 1.1504 = 0.0185 + 1.1216 + 0.0103, time: 91.555794]
2023-06-01 21:31:39.288: epoch 43:	0.02823238  	0.06175135  	0.05114682  	0.09537388  	0.10723472  
2023-06-01 21:33:13.555: [iter 44 : loss : 1.1501 = 0.0184 + 1.1214 + 0.0104, time: 91.497576]
2023-06-01 21:33:17.806: epoch 44:	0.02816450  	0.06154134  	0.05097628  	0.09502170  	0.10682850  
2023-06-01 21:34:51.637: [iter 45 : loss : 1.1494 = 0.0178 + 1.1211 + 0.0105, time: 91.067466]
2023-06-01 21:34:55.708: epoch 45:	0.02804925  	0.06116585  	0.05076630  	0.09496734  	0.10658422  
2023-06-01 21:36:30.573: [iter 46 : loss : 1.1492 = 0.0176 + 1.1209 + 0.0107, time: 92.156315]
2023-06-01 21:36:34.643: epoch 46:	0.02804295  	0.06110852  	0.05068141  	0.09474111  	0.10625739  
2023-06-01 21:38:09.851: [iter 47 : loss : 1.1488 = 0.0172 + 1.1208 + 0.0108, time: 92.432483]
2023-06-01 21:38:13.817: epoch 47:	0.02794824  	0.06086661  	0.05055974  	0.09467532  	0.10624499  
2023-06-01 21:39:48.905: [iter 48 : loss : 1.1482 = 0.0168 + 1.1205 + 0.0109, time: 92.266589]
2023-06-01 21:39:52.457: epoch 48:	0.02786141  	0.06063943  	0.05029140  	0.09401349  	0.10545156  
2023-06-01 21:41:26.922: [iter 49 : loss : 1.1477 = 0.0164 + 1.1203 + 0.0110, time: 91.712182]
2023-06-01 21:41:30.906: epoch 49:	0.02782984  	0.06059320  	0.05019384  	0.09383027  	0.10514192  
2023-06-01 21:43:05.915: [iter 50 : loss : 1.1475 = 0.0162 + 1.1202 + 0.0111, time: 92.274987]
2023-06-01 21:43:09.899: epoch 50:	0.02777300  	0.06043947  	0.05005521  	0.09366103  	0.10490613  
2023-06-01 21:44:44.995: [iter 51 : loss : 1.1470 = 0.0158 + 1.1200 + 0.0112, time: 92.347757]
2023-06-01 21:44:49.249: epoch 51:	0.02772248  	0.06025732  	0.04989668  	0.09344195  	0.10462604  
2023-06-01 21:46:23.654: [iter 52 : loss : 1.1468 = 0.0156 + 1.1199 + 0.0113, time: 91.579875]
2023-06-01 21:46:27.601: epoch 52:	0.02759777  	0.05996088  	0.04971020  	0.09314270  	0.10443594  
2023-06-01 21:48:01.845: [iter 53 : loss : 1.1465 = 0.0154 + 1.1198 + 0.0114, time: 91.481312]
2023-06-01 21:48:06.014: epoch 53:	0.02750145  	0.05970221  	0.04950048  	0.09283544  	0.10399809  
2023-06-01 21:49:39.903: [iter 54 : loss : 1.1462 = 0.0152 + 1.1196 + 0.0115, time: 91.140979]
2023-06-01 21:49:43.885: epoch 54:	0.02737674  	0.05938913  	0.04931442  	0.09269340  	0.10389949  
2023-06-01 21:51:19.605: [iter 55 : loss : 1.1459 = 0.0150 + 1.1194 + 0.0115, time: 92.909572]
2023-06-01 21:51:23.471: epoch 55:	0.02734359  	0.05935103  	0.04926470  	0.09252805  	0.10384994  
2023-06-01 21:52:57.882: [iter 56 : loss : 1.1457 = 0.0147 + 1.1193 + 0.0116, time: 91.658356]
2023-06-01 21:53:02.100: epoch 56:	0.02720468  	0.05900251  	0.04904581  	0.09229832  	0.10359158  
2023-06-01 21:54:36.437: [iter 57 : loss : 1.1455 = 0.0145 + 1.1192 + 0.0117, time: 91.568036]
2023-06-01 21:54:40.301: epoch 57:	0.02718417  	0.05882531  	0.04896183  	0.09209025  	0.10345543  
2023-06-01 21:56:14.759: [iter 58 : loss : 1.1450 = 0.0142 + 1.1191 + 0.0118, time: 91.710868]
2023-06-01 21:56:19.149: epoch 58:	0.02710205  	0.05861274  	0.04879249  	0.09193430  	0.10306074  
2023-06-01 21:57:54.185: [iter 59 : loss : 1.1450 = 0.0142 + 1.1190 + 0.0119, time: 92.255050]
2023-06-01 21:57:57.680: epoch 59:	0.02705155  	0.05844900  	0.04865290  	0.09142161  	0.10278799  
2023-06-01 21:59:31.369: [iter 60 : loss : 1.1448 = 0.0140 + 1.1189 + 0.0119, time: 90.921476]
2023-06-01 21:59:35.350: epoch 60:	0.02698841  	0.05824417  	0.04852059  	0.09138384  	0.10260911  
2023-06-01 22:01:11.207: [iter 61 : loss : 1.1446 = 0.0138 + 1.1188 + 0.0120, time: 93.053037]
2023-06-01 22:01:15.443: epoch 61:	0.02690634  	0.05800437  	0.04828762  	0.09088904  	0.10199932  
2023-06-01 22:02:49.760: [iter 62 : loss : 1.1444 = 0.0137 + 1.1186 + 0.0121, time: 91.534565]
2023-06-01 22:02:53.790: epoch 62:	0.02690629  	0.05805995  	0.04829603  	0.09092765  	0.10207015  
2023-06-01 22:04:28.824: [iter 63 : loss : 1.1442 = 0.0134 + 1.1186 + 0.0121, time: 92.275373]
2023-06-01 22:04:32.739: epoch 63:	0.02682422  	0.05780222  	0.04814500  	0.09068122  	0.10180370  
2023-06-01 22:06:07.125: [iter 64 : loss : 1.1441 = 0.0134 + 1.1185 + 0.0122, time: 91.578865]
2023-06-01 22:06:11.199: epoch 64:	0.02674213  	0.05763955  	0.04800082  	0.09048106  	0.10165220  
2023-06-01 22:07:45.813: [iter 65 : loss : 1.1440 = 0.0133 + 1.1184 + 0.0123, time: 91.825371]
2023-06-01 22:07:49.901: epoch 65:	0.02672794  	0.05760258  	0.04796002  	0.09029385  	0.10149235  
2023-06-01 22:09:24.257: [iter 66 : loss : 1.1436 = 0.0129 + 1.1183 + 0.0123, time: 91.531316]
2023-06-01 22:09:28.295: epoch 66:	0.02668688  	0.05755893  	0.04789933  	0.09026300  	0.10139437  
2023-06-01 22:11:03.364: [iter 67 : loss : 1.1436 = 0.0129 + 1.1183 + 0.0124, time: 92.270131]
2023-06-01 22:11:07.551: epoch 67:	0.02659531  	0.05730304  	0.04773858  	0.09019347  	0.10121883  
2023-06-01 22:12:42.086: [iter 68 : loss : 1.1435 = 0.0129 + 1.1182 + 0.0124, time: 91.723135]
2023-06-01 22:12:46.041: epoch 68:	0.02651956  	0.05713766  	0.04758335  	0.08986382  	0.10085956  
2023-06-01 22:14:19.761: [iter 69 : loss : 1.1433 = 0.0127 + 1.1181 + 0.0125, time: 90.948928]
2023-06-01 22:14:23.986: epoch 69:	0.02645009  	0.05694613  	0.04746602  	0.08980634  	0.10075256  
2023-06-01 22:15:59.193: [iter 70 : loss : 1.1430 = 0.0124 + 1.1180 + 0.0125, time: 92.431911]
2023-06-01 22:16:02.984: epoch 70:	0.02640430  	0.05678592  	0.04733827  	0.08951040  	0.10047109  
2023-06-01 22:16:02.984: Early stopping is triggered at epoch: 70
2023-06-01 22:16:02.984: best_result@epoch 20:

2023-06-01 22:16:02.985: Loading from the saved model.
2023-06-01 22:16:06.979: 		0.03042359  	0.06785671  	0.05571824  	0.10170442  	0.11492888  
