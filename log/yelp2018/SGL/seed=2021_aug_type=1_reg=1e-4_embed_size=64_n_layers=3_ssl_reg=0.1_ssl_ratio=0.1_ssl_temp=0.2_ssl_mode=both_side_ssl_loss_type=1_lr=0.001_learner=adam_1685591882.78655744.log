2023-06-01 10:58:02.786: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-01 10:58:02.786: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=0.5
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=1
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=1
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

2023-06-01 10:58:06.289: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-01 10:58:09.208: 		0.00028736  	0.00051617  	0.00041172  	0.00101716  	0.00101612  
2023-06-01 10:59:42.608: [iter 1 : loss : 1.8204 = 0.6931 + 1.1273 + 0.0000, time: 90.887551]
2023-06-01 10:59:45.434: epoch 1:	0.00169886  	0.00342905  	0.00279727  	0.00612482  	0.00629961  
2023-06-01 10:59:45.434: Found a better model.
2023-06-01 10:59:45.434: Save model to file as pretrain.
2023-06-01 11:01:18.450: [iter 2 : loss : 1.8186 = 0.6930 + 1.1256 + 0.0000, time: 90.072461]
2023-06-01 11:01:21.267: epoch 2:	0.00272990  	0.00574819  	0.00464964  	0.00989042  	0.01035283  
2023-06-01 11:01:21.267: Found a better model.
2023-06-01 11:01:21.267: Save model to file as pretrain.
2023-06-01 11:02:54.377: [iter 3 : loss : 1.8183 = 0.6929 + 1.1254 + 0.0000, time: 90.194352]
2023-06-01 11:02:57.226: epoch 3:	0.00361885  	0.00749950  	0.00606739  	0.01297920  	0.01358172  
2023-06-01 11:02:57.226: Found a better model.
2023-06-01 11:02:57.226: Save model to file as pretrain.
2023-06-01 11:04:30.249: [iter 4 : loss : 1.8181 = 0.6927 + 1.1254 + 0.0000, time: 90.134446]
2023-06-01 11:04:33.055: epoch 4:	0.00450148  	0.00963716  	0.00776629  	0.01620765  	0.01695139  
2023-06-01 11:04:33.055: Found a better model.
2023-06-01 11:04:33.055: Save model to file as pretrain.
2023-06-01 11:06:05.585: [iter 5 : loss : 1.8179 = 0.6925 + 1.1254 + 0.0000, time: 89.661009]
2023-06-01 11:06:08.418: epoch 5:	0.00526253  	0.01135787  	0.00912890  	0.01803631  	0.01945001  
2023-06-01 11:06:08.418: Found a better model.
2023-06-01 11:06:08.418: Save model to file as pretrain.
2023-06-01 11:07:42.118: [iter 6 : loss : 1.8173 = 0.6918 + 1.1255 + 0.0000, time: 90.839002]
2023-06-01 11:07:44.955: epoch 6:	0.00368515  	0.00847831  	0.00640467  	0.01148532  	0.01249496  
2023-06-01 11:09:17.368: [iter 7 : loss : 1.8148 = 0.6889 + 1.1259 + 0.0000, time: 90.002534]
2023-06-01 11:09:21.521: epoch 7:	0.00427882  	0.00986404  	0.00770200  	0.01366372  	0.01538381  
2023-06-01 11:10:54.560: [iter 8 : loss : 1.8064 = 0.6783 + 1.1281 + 0.0000, time: 90.531456]
2023-06-01 11:10:57.426: epoch 8:	0.00808090  	0.01848673  	0.01517872  	0.02687703  	0.03060980  
2023-06-01 11:10:57.426: Found a better model.
2023-06-01 11:10:57.426: Save model to file as pretrain.
2023-06-01 11:12:31.548: [iter 9 : loss : 1.7663 = 0.6287 + 1.1374 + 0.0002, time: 91.276155]
2023-06-01 11:12:34.934: epoch 9:	0.01327015  	0.02991040  	0.02431742  	0.04341865  	0.04909530  
2023-06-01 11:12:34.935: Found a better model.
2023-06-01 11:12:34.935: Save model to file as pretrain.
2023-06-01 11:14:08.461: [iter 10 : loss : 1.6838 = 0.5313 + 1.1520 + 0.0004, time: 90.650411]
2023-06-01 11:14:11.337: epoch 10:	0.02344443  	0.05119102  	0.04209113  	0.07781736  	0.08717286  
2023-06-01 11:14:11.337: Found a better model.
2023-06-01 11:14:11.337: Save model to file as pretrain.
2023-06-01 11:15:45.519: [iter 11 : loss : 1.5694 = 0.3967 + 1.1718 + 0.0009, time: 91.321448]
2023-06-01 11:15:48.336: epoch 11:	0.02682262  	0.05926681  	0.04847492  	0.08949462  	0.10016613  
2023-06-01 11:15:48.336: Found a better model.
2023-06-01 11:15:48.336: Save model to file as pretrain.
2023-06-01 11:17:21.314: [iter 12 : loss : 1.4443 = 0.2520 + 1.1908 + 0.0016, time: 90.092522]
2023-06-01 11:17:24.202: epoch 12:	0.02783456  	0.06165102  	0.05051894  	0.09312928  	0.10434243  
2023-06-01 11:17:24.202: Found a better model.
2023-06-01 11:17:24.202: Save model to file as pretrain.
2023-06-01 11:18:57.184: [iter 13 : loss : 1.3642 = 0.1715 + 1.1904 + 0.0022, time: 90.092667]
2023-06-01 11:19:00.041: epoch 13:	0.02862231  	0.06354481  	0.05206025  	0.09537948  	0.10707344  
2023-06-01 11:19:00.041: Found a better model.
2023-06-01 11:19:00.041: Save model to file as pretrain.
2023-06-01 11:20:34.325: [iter 14 : loss : 1.3171 = 0.1316 + 1.1827 + 0.0028, time: 91.469423]
2023-06-01 11:20:38.083: epoch 14:	0.02909118  	0.06471898  	0.05312149  	0.09769812  	0.10940883  
2023-06-01 11:20:38.083: Found a better model.
2023-06-01 11:20:38.083: Save model to file as pretrain.
2023-06-01 11:22:13.045: [iter 15 : loss : 1.2856 = 0.1072 + 1.1751 + 0.0033, time: 91.820706]
2023-06-01 11:22:16.563: epoch 15:	0.02940851  	0.06550875  	0.05376342  	0.09866142  	0.11054792  
2023-06-01 11:22:16.563: Found a better model.
2023-06-01 11:22:16.563: Save model to file as pretrain.
2023-06-01 11:23:51.175: [iter 16 : loss : 1.2630 = 0.0904 + 1.1689 + 0.0038, time: 91.482764]
2023-06-01 11:23:54.764: epoch 16:	0.02958690  	0.06586712  	0.05420385  	0.09953307  	0.11164721  
2023-06-01 11:23:54.764: Found a better model.
2023-06-01 11:23:54.764: Save model to file as pretrain.
2023-06-01 11:25:28.714: [iter 17 : loss : 1.2466 = 0.0786 + 1.1638 + 0.0042, time: 90.758787]
2023-06-01 11:25:32.053: epoch 17:	0.02976373  	0.06625918  	0.05454993  	0.10009710  	0.11232726  
2023-06-01 11:25:32.053: Found a better model.
2023-06-01 11:25:32.053: Save model to file as pretrain.
2023-06-01 11:27:06.313: [iter 18 : loss : 1.2333 = 0.0690 + 1.1597 + 0.0046, time: 91.123679]
2023-06-01 11:27:09.650: epoch 18:	0.02996423  	0.06672318  	0.05481718  	0.10039459  	0.11266497  
2023-06-01 11:27:09.650: Found a better model.
2023-06-01 11:27:09.650: Save model to file as pretrain.
2023-06-01 11:28:43.106: [iter 19 : loss : 1.2230 = 0.0619 + 1.1562 + 0.0050, time: 90.487149]
2023-06-01 11:28:46.853: epoch 19:	0.02992791  	0.06662264  	0.05490682  	0.10079078  	0.11320906  
2023-06-01 11:30:20.071: [iter 20 : loss : 1.2145 = 0.0559 + 1.1533 + 0.0053, time: 90.752227]
2023-06-01 11:30:23.697: epoch 20:	0.02994687  	0.06670264  	0.05500564  	0.10100093  	0.11362556  
2023-06-01 11:31:56.671: [iter 21 : loss : 1.2075 = 0.0509 + 1.1509 + 0.0057, time: 90.396727]
2023-06-01 11:31:59.578: epoch 21:	0.02997213  	0.06676180  	0.05511372  	0.10140065  	0.11405790  
2023-06-01 11:31:59.578: Found a better model.
2023-06-01 11:31:59.578: Save model to file as pretrain.
2023-06-01 11:33:33.065: [iter 22 : loss : 1.2014 = 0.0465 + 1.1489 + 0.0060, time: 90.627010]
2023-06-01 11:33:35.935: epoch 22:	0.02988372  	0.06653698  	0.05499236  	0.10148480  	0.11402741  
2023-06-01 11:35:09.110: [iter 23 : loss : 1.1964 = 0.0430 + 1.1471 + 0.0063, time: 90.784751]
2023-06-01 11:35:12.692: epoch 23:	0.02982214  	0.06643146  	0.05488946  	0.10140352  	0.11390428  
2023-06-01 11:36:45.837: [iter 24 : loss : 1.1926 = 0.0404 + 1.1456 + 0.0066, time: 90.659614]
2023-06-01 11:36:48.991: epoch 24:	0.02986320  	0.06648355  	0.05486377  	0.10132322  	0.11375310  
2023-06-01 11:38:21.884: [iter 25 : loss : 1.1888 = 0.0377 + 1.1443 + 0.0068, time: 90.291294]
2023-06-01 11:38:25.570: epoch 25:	0.02975586  	0.06614837  	0.05469982  	0.10142668  	0.11378529  
2023-06-01 11:39:56.692: [iter 26 : loss : 1.1854 = 0.0352 + 1.1431 + 0.0071, time: 88.355283]
2023-06-01 11:40:00.839: epoch 26:	0.02965955  	0.06587669  	0.05456772  	0.10119475  	0.11366116  
2023-06-01 11:41:34.854: [iter 27 : loss : 1.1826 = 0.0332 + 1.1421 + 0.0073, time: 91.230212]
2023-06-01 11:41:37.834: epoch 27:	0.02963743  	0.06577975  	0.05449772  	0.10117915  	0.11366417  
2023-06-01 11:43:11.037: [iter 28 : loss : 1.1804 = 0.0316 + 1.1412 + 0.0076, time: 90.786534]
2023-06-01 11:43:13.969: epoch 28:	0.02959320  	0.06566089  	0.05440155  	0.10111605  	0.11354004  
2023-06-01 11:44:46.570: [iter 29 : loss : 1.1783 = 0.0301 + 1.1404 + 0.0078, time: 90.181123]
2023-06-01 11:44:49.524: epoch 29:	0.02945430  	0.06526705  	0.05416296  	0.10098703  	0.11324045  
2023-06-01 11:46:23.316: [iter 30 : loss : 1.1763 = 0.0287 + 1.1396 + 0.0080, time: 91.382055]
2023-06-01 11:46:26.219: epoch 30:	0.02938171  	0.06505437  	0.05395590  	0.10062096  	0.11277173  
2023-06-01 11:47:59.386: [iter 31 : loss : 1.1746 = 0.0274 + 1.1389 + 0.0082, time: 90.756147]
2023-06-01 11:48:02.307: epoch 31:	0.02931225  	0.06491917  	0.05384284  	0.10050411  	0.11274423  
2023-06-01 11:49:35.437: [iter 32 : loss : 1.1730 = 0.0263 + 1.1383 + 0.0084, time: 90.702690]
2023-06-01 11:49:38.435: epoch 32:	0.02921594  	0.06464960  	0.05367399  	0.10026076  	0.11268938  
2023-06-01 11:51:11.770: [iter 33 : loss : 1.1718 = 0.0254 + 1.1378 + 0.0086, time: 90.909227]
2023-06-01 11:51:14.715: epoch 33:	0.02914489  	0.06447104  	0.05349538  	0.09996001  	0.11231135  
2023-06-01 11:52:47.948: [iter 34 : loss : 1.1707 = 0.0246 + 1.1373 + 0.0088, time: 90.798890]
2023-06-01 11:52:50.877: epoch 34:	0.02907229  	0.06425618  	0.05335273  	0.09983345  	0.11229605  
2023-06-01 11:54:24.219: [iter 35 : loss : 1.1691 = 0.0233 + 1.1368 + 0.0090, time: 90.922065]
2023-06-01 11:54:27.139: epoch 35:	0.02895232  	0.06388147  	0.05311894  	0.09937228  	0.11184011  
2023-06-01 11:56:00.419: [iter 36 : loss : 1.1681 = 0.0226 + 1.1364 + 0.0092, time: 90.860616]
2023-06-01 11:56:03.305: epoch 36:	0.02881020  	0.06352757  	0.05284677  	0.09904618  	0.11133807  
2023-06-01 11:57:36.510: [iter 37 : loss : 1.1673 = 0.0219 + 1.1360 + 0.0093, time: 90.753282]
2023-06-01 11:57:39.442: epoch 37:	0.02879287  	0.06336769  	0.05280442  	0.09897465  	0.11144246  
2023-06-01 11:59:13.185: [iter 38 : loss : 1.1664 = 0.0212 + 1.1357 + 0.0095, time: 91.325537]
2023-06-01 11:59:16.095: epoch 38:	0.02863816  	0.06306703  	0.05250917  	0.09844763  	0.11072169  
2023-06-01 12:00:49.240: [iter 39 : loss : 1.1655 = 0.0206 + 1.1353 + 0.0096, time: 90.713227]
2023-06-01 12:00:52.186: epoch 39:	0.02860660  	0.06288776  	0.05231339  	0.09797119  	0.11003372  
2023-06-01 12:02:25.609: [iter 40 : loss : 1.1647 = 0.0200 + 1.1350 + 0.0098, time: 90.991950]
2023-06-01 12:02:28.542: epoch 40:	0.02836980  	0.06232922  	0.05204586  	0.09794999  	0.11009372  
2023-06-01 12:04:01.190: [iter 41 : loss : 1.1643 = 0.0197 + 1.1347 + 0.0099, time: 90.197000]
2023-06-01 12:04:04.068: epoch 41:	0.02826876  	0.06205679  	0.05179381  	0.09759235  	0.10940409  
2023-06-01 12:05:36.821: [iter 42 : loss : 1.1637 = 0.0192 + 1.1345 + 0.0101, time: 90.319082]
2023-06-01 12:05:39.740: epoch 42:	0.02818825  	0.06184688  	0.05159237  	0.09715608  	0.10888928  
2023-06-01 12:07:12.434: [iter 43 : loss : 1.1630 = 0.0186 + 1.1341 + 0.0102, time: 90.264524]
2023-06-01 12:07:15.347: epoch 43:	0.02811248  	0.06165805  	0.05145585  	0.09714440  	0.10871692  
2023-06-01 12:08:48.148: [iter 44 : loss : 1.1626 = 0.0184 + 1.1339 + 0.0103, time: 90.366823]
2023-06-01 12:08:51.060: epoch 44:	0.02797672  	0.06134395  	0.05118481  	0.09661113  	0.10823473  
2023-06-01 12:10:23.051: [iter 45 : loss : 1.1619 = 0.0179 + 1.1336 + 0.0104, time: 89.553909]
2023-06-01 12:10:25.993: epoch 45:	0.02794515  	0.06125697  	0.05108965  	0.09636411  	0.10806043  
2023-06-01 12:11:59.241: [iter 46 : loss : 1.1616 = 0.0176 + 1.1335 + 0.0105, time: 90.799453]
2023-06-01 12:12:02.140: epoch 46:	0.02788199  	0.06106889  	0.05093486  	0.09612984  	0.10769856  
2023-06-01 12:13:35.309: [iter 47 : loss : 1.1611 = 0.0172 + 1.1333 + 0.0107, time: 90.734605]
2023-06-01 12:13:38.214: epoch 47:	0.02793725  	0.06112801  	0.05089496  	0.09580717  	0.10738926  
2023-06-01 12:15:11.437: [iter 48 : loss : 1.1607 = 0.0169 + 1.1331 + 0.0108, time: 90.783128]
2023-06-01 12:15:14.403: epoch 48:	0.02783466  	0.06087917  	0.05074133  	0.09571888  	0.10727715  
2023-06-01 12:16:47.354: [iter 49 : loss : 1.1602 = 0.0164 + 1.1329 + 0.0109, time: 90.508136]
2023-06-01 12:16:50.298: epoch 49:	0.02777466  	0.06068060  	0.05060379  	0.09547763  	0.10713140  
2023-06-01 12:18:23.568: [iter 50 : loss : 1.1600 = 0.0162 + 1.1327 + 0.0110, time: 90.824261]
2023-06-01 12:18:26.482: epoch 50:	0.02764360  	0.06037666  	0.05039469  	0.09528967  	0.10681497  
2023-06-01 12:19:59.263: [iter 51 : loss : 1.1595 = 0.0159 + 1.1326 + 0.0111, time: 90.321007]
2023-06-01 12:20:02.179: epoch 51:	0.02760415  	0.06030994  	0.05033116  	0.09516057  	0.10683568  
2023-06-01 12:21:35.562: [iter 52 : loss : 1.1591 = 0.0155 + 1.1324 + 0.0112, time: 90.911827]
2023-06-01 12:21:38.510: epoch 52:	0.02751576  	0.06004609  	0.05015127  	0.09492783  	0.10650975  
2023-06-01 12:23:12.169: [iter 53 : loss : 1.1590 = 0.0154 + 1.1323 + 0.0113, time: 91.184640]
2023-06-01 12:23:15.604: epoch 53:	0.02746996  	0.05987883  	0.04995591  	0.09438124  	0.10596357  
2023-06-01 12:24:48.312: [iter 54 : loss : 1.1588 = 0.0153 + 1.1322 + 0.0114, time: 90.002732]
2023-06-01 12:24:51.214: epoch 54:	0.02740838  	0.05973738  	0.04987078  	0.09438550  	0.10591730  
2023-06-01 12:26:24.513: [iter 55 : loss : 1.1584 = 0.0149 + 1.1320 + 0.0114, time: 90.864549]
2023-06-01 12:26:27.450: epoch 55:	0.02729787  	0.05952407  	0.04969453  	0.09408322  	0.10549089  
2023-06-01 12:28:00.801: [iter 56 : loss : 1.1581 = 0.0147 + 1.1319 + 0.0115, time: 90.861568]
2023-06-01 12:28:03.675: epoch 56:	0.02730261  	0.05963872  	0.04964346  	0.09380235  	0.10519598  
2023-06-01 12:29:37.222: [iter 57 : loss : 1.1580 = 0.0146 + 1.1318 + 0.0116, time: 91.088668]
2023-06-01 12:29:40.808: epoch 57:	0.02722525  	0.05935392  	0.04950200  	0.09394626  	0.10517468  
2023-06-01 12:31:15.315: [iter 58 : loss : 1.1575 = 0.0142 + 1.1317 + 0.0117, time: 91.847218]
2023-06-01 12:31:18.281: epoch 58:	0.02714001  	0.05912205  	0.04933375  	0.09346458  	0.10475269  
2023-06-01 12:32:51.553: [iter 59 : loss : 1.1575 = 0.0142 + 1.1316 + 0.0117, time: 90.803134]
2023-06-01 12:32:54.476: epoch 59:	0.02704529  	0.05898166  	0.04918473  	0.09326157  	0.10431101  
2023-06-01 12:34:28.032: [iter 60 : loss : 1.1573 = 0.0140 + 1.1315 + 0.0118, time: 91.102638]
2023-06-01 12:34:30.962: epoch 60:	0.02697897  	0.05873750  	0.04902076  	0.09308151  	0.10412408  
2023-06-01 12:36:05.017: [iter 61 : loss : 1.1569 = 0.0137 + 1.1314 + 0.0119, time: 91.592165]
2023-06-01 12:36:08.044: epoch 61:	0.02693634  	0.05852730  	0.04886022  	0.09294596  	0.10381439  
2023-06-01 12:37:41.122: [iter 62 : loss : 1.1570 = 0.0138 + 1.1312 + 0.0120, time: 90.607797]
2023-06-01 12:37:44.238: epoch 62:	0.02688267  	0.05845828  	0.04877812  	0.09301484  	0.10375547  
2023-06-01 12:39:17.518: [iter 63 : loss : 1.1567 = 0.0135 + 1.1312 + 0.0120, time: 90.795835]
2023-06-01 12:39:20.414: epoch 63:	0.02680846  	0.05827120  	0.04862367  	0.09260917  	0.10337986  
2023-06-01 12:40:53.852: [iter 64 : loss : 1.1565 = 0.0133 + 1.1311 + 0.0121, time: 90.964010]
2023-06-01 12:40:57.255: epoch 64:	0.02678637  	0.05812115  	0.04856388  	0.09264608  	0.10338808  
2023-06-01 12:42:29.590: [iter 65 : loss : 1.1565 = 0.0133 + 1.1310 + 0.0122, time: 89.544185]
2023-06-01 12:42:34.064: epoch 65:	0.02675795  	0.05801789  	0.04846357  	0.09256478  	0.10315789  
2023-06-01 12:44:14.674: [iter 66 : loss : 1.1561 = 0.0130 + 1.1309 + 0.0122, time: 97.799243]
2023-06-01 12:44:18.997: epoch 66:	0.02667114  	0.05782530  	0.04833259  	0.09241252  	0.10287374  
2023-06-01 12:45:56.590: [iter 67 : loss : 1.1561 = 0.0129 + 1.1309 + 0.0123, time: 94.738051]
2023-06-01 12:46:00.803: epoch 67:	0.02662851  	0.05775028  	0.04825784  	0.09210455  	0.10279880  
2023-06-01 12:47:45.721: [iter 68 : loss : 1.1561 = 0.0130 + 1.1308 + 0.0123, time: 102.113299]
2023-06-01 12:47:49.514: epoch 68:	0.02661272  	0.05773409  	0.04818868  	0.09200639  	0.10265295  
2023-06-01 12:49:22.932: [iter 69 : loss : 1.1558 = 0.0127 + 1.1307 + 0.0124, time: 90.626400]
2023-06-01 12:49:25.814: epoch 69:	0.02654641  	0.05762960  	0.04804743  	0.09169978  	0.10238038  
2023-06-01 12:50:59.436: [iter 70 : loss : 1.1555 = 0.0125 + 1.1306 + 0.0124, time: 91.142089]
2023-06-01 12:51:02.633: epoch 70:	0.02647380  	0.05743938  	0.04795288  	0.09182468  	0.10228115  
2023-06-01 12:52:35.850: [iter 71 : loss : 1.1555 = 0.0124 + 1.1306 + 0.0125, time: 90.403071]
2023-06-01 12:52:39.186: epoch 71:	0.02637593  	0.05712194  	0.04778708  	0.09164567  	0.10206384  
2023-06-01 12:52:39.186: Early stopping is triggered at epoch: 71
2023-06-01 12:52:39.186: best_result@epoch 21:

2023-06-01 12:52:39.186: Loading from the saved model.
2023-06-01 12:52:42.677: 		0.02997213  	0.06676180  	0.05511372  	0.10140065  	0.11405790  
