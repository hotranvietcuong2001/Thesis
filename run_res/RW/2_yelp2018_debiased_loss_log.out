seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-05 17:29:48.005: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-05 17:29:48.005: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=2
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=2
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using debiased loss
2023-06-05 17:29:51.778: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-05 17:29:56.270: 		0.00027946  	0.00052957  	0.00044709  	0.00112470  	0.00112470  
2023-06-05 17:33:20.434: [iter 1 : loss : 1.8079 = 0.6931 + 1.1148 + 0.0000, time: 195.596402]
2023-06-05 17:33:24.715: epoch 1:	0.00246148  	0.00492157  	0.00417230  	0.00935937  	0.00971173  
2023-06-05 17:33:24.715: Found a better model.
2023-06-05 17:33:24.715: Save model to file as pretrain.
2023-06-05 17:36:47.977: [iter 2 : loss : 1.8056 = 0.6930 + 1.1126 + 0.0000, time: 193.538439]
2023-06-05 17:36:52.136: epoch 2:	0.00361569  	0.00719815  	0.00610730  	0.01336240  	0.01402157  
2023-06-05 17:36:52.137: Found a better model.
2023-06-05 17:36:52.137: Save model to file as pretrain.
2023-06-05 17:40:17.507: [iter 3 : loss : 1.8051 = 0.6928 + 1.1123 + 0.0000, time: 195.580005]
2023-06-05 17:40:21.745: epoch 3:	0.00453621  	0.00940184  	0.00789680  	0.01677793  	0.01793907  
2023-06-05 17:40:21.745: Found a better model.
2023-06-05 17:40:21.745: Save model to file as pretrain.
2023-06-05 17:43:45.445: [iter 4 : loss : 1.8048 = 0.6926 + 1.1122 + 0.0000, time: 193.980381]
2023-06-05 17:43:49.361: epoch 4:	0.00544568  	0.01156277  	0.00952681  	0.01959001  	0.02085791  
2023-06-05 17:43:49.361: Found a better model.
2023-06-05 17:43:49.361: Save model to file as pretrain.
2023-06-05 17:47:13.966: [iter 5 : loss : 1.8044 = 0.6922 + 1.1122 + 0.0000, time: 194.971650]
2023-06-05 17:47:18.028: epoch 5:	0.00489305  	0.01077280  	0.00844575  	0.01622985  	0.01734549  
2023-06-05 17:50:39.969: [iter 6 : loss : 1.8028 = 0.6905 + 1.1124 + 0.0000, time: 193.419535]
2023-06-05 17:50:43.748: epoch 6:	0.00386672  	0.00868045  	0.00669174  	0.01205361  	0.01324222  
2023-06-05 17:54:06.788: [iter 7 : loss : 1.7995 = 0.6867 + 1.1128 + 0.0000, time: 194.627484]
2023-06-05 17:54:10.862: epoch 7:	0.00572829  	0.01280362  	0.00998615  	0.01751028  	0.01969759  
2023-06-05 17:54:10.862: Found a better model.
2023-06-05 17:54:10.862: Save model to file as pretrain.
2023-06-05 17:57:34.809: [iter 8 : loss : 1.7844 = 0.6680 + 1.1163 + 0.0001, time: 194.156973]
2023-06-05 17:57:39.033: epoch 8:	0.00971804  	0.02194721  	0.01815051  	0.03254266  	0.03712583  
2023-06-05 17:57:39.033: Found a better model.
2023-06-05 17:57:39.033: Save model to file as pretrain.
2023-06-05 18:01:02.695: [iter 9 : loss : 1.7284 = 0.5999 + 1.1282 + 0.0002, time: 194.105511]
2023-06-05 18:01:06.739: epoch 9:	0.01697533  	0.03743842  	0.03069621  	0.05585807  	0.06290240  
2023-06-05 18:01:06.740: Found a better model.
2023-06-05 18:01:06.740: Save model to file as pretrain.
2023-06-05 18:04:30.394: [iter 10 : loss : 1.6387 = 0.4956 + 1.1425 + 0.0006, time: 194.098661]
2023-06-05 18:04:34.278: epoch 10:	0.02610121  	0.05698131  	0.04676115  	0.08608284  	0.09638664  
2023-06-05 18:04:34.279: Found a better model.
2023-06-05 18:04:34.279: Save model to file as pretrain.
2023-06-05 18:07:59.147: [iter 11 : loss : 1.5151 = 0.3503 + 1.1637 + 0.0011, time: 195.388527]
2023-06-05 18:08:03.231: epoch 11:	0.02807453  	0.06190651  	0.05061017  	0.09231068  	0.10380933  
2023-06-05 18:08:03.232: Found a better model.
2023-06-05 18:08:03.232: Save model to file as pretrain.
2023-06-05 18:11:27.502: [iter 12 : loss : 1.4035 = 0.2255 + 1.1763 + 0.0018, time: 194.718082]
2023-06-05 18:11:31.277: epoch 12:	0.02863971  	0.06329007  	0.05202341  	0.09529757  	0.10708205  
2023-06-05 18:11:31.277: Found a better model.
2023-06-05 18:11:31.277: Save model to file as pretrain.
2023-06-05 18:14:56.740: [iter 13 : loss : 1.3353 = 0.1597 + 1.1732 + 0.0024, time: 195.964131]
2023-06-05 18:15:00.948: epoch 13:	0.02932330  	0.06513868  	0.05341556  	0.09765396  	0.10949426  
2023-06-05 18:15:00.948: Found a better model.
2023-06-05 18:15:00.948: Save model to file as pretrain.
2023-06-05 18:18:24.332: [iter 14 : loss : 1.2928 = 0.1241 + 1.1658 + 0.0029, time: 193.929427]
2023-06-05 18:18:28.427: epoch 14:	0.02960587  	0.06583995  	0.05419056  	0.09931574  	0.11141039  
2023-06-05 18:18:28.427: Found a better model.
2023-06-05 18:18:28.427: Save model to file as pretrain.
2023-06-05 18:21:52.548: [iter 15 : loss : 1.2645 = 0.1023 + 1.1587 + 0.0034, time: 194.896812]
2023-06-05 18:21:56.796: epoch 15:	0.02989479  	0.06658506  	0.05485845  	0.10061390  	0.11287344  
2023-06-05 18:21:56.796: Found a better model.
2023-06-05 18:21:56.796: Save model to file as pretrain.
2023-06-05 18:25:20.090: [iter 16 : loss : 1.2435 = 0.0867 + 1.1529 + 0.0039, time: 194.642831]
2023-06-05 18:25:24.203: epoch 16:	0.03007477  	0.06698541  	0.05520032  	0.10123933  	0.11361238  
2023-06-05 18:25:24.203: Found a better model.
2023-06-05 18:25:24.203: Save model to file as pretrain.
2023-06-05 18:28:48.085: [iter 17 : loss : 1.2279 = 0.0755 + 1.1481 + 0.0043, time: 195.351134]
2023-06-05 18:28:52.319: epoch 17:	0.03023264  	0.06736124  	0.05547912  	0.10150830  	0.11407905  
2023-06-05 18:28:52.319: Found a better model.
2023-06-05 18:28:52.319: Save model to file as pretrain.
2023-06-05 18:32:16.083: [iter 18 : loss : 1.2156 = 0.0666 + 1.1443 + 0.0047, time: 195.181666]
2023-06-05 18:32:20.159: epoch 18:	0.03029264  	0.06748725  	0.05563819  	0.10208092  	0.11458711  
2023-06-05 18:32:20.159: Found a better model.
2023-06-05 18:32:20.159: Save model to file as pretrain.
2023-06-05 18:35:44.657: [iter 19 : loss : 1.2059 = 0.0598 + 1.1410 + 0.0051, time: 195.860882]
2023-06-05 18:35:48.924: epoch 19:	0.03033211  	0.06761268  	0.05564264  	0.10172244  	0.11437698  
2023-06-05 18:35:48.925: Found a better model.
2023-06-05 18:35:48.925: Save model to file as pretrain.
2023-06-05 18:39:12.063: [iter 20 : loss : 1.1976 = 0.0537 + 1.1384 + 0.0055, time: 194.543995]
2023-06-05 18:39:16.297: epoch 20:	0.03032577  	0.06754193  	0.05562503  	0.10163745  	0.11440051  
2023-06-05 18:42:40.326: [iter 21 : loss : 1.1910 = 0.0491 + 1.1361 + 0.0058, time: 195.830569]
2023-06-05 18:42:44.422: epoch 21:	0.03025948  	0.06739145  	0.05543650  	0.10113053  	0.11378850  
2023-06-05 18:46:07.156: [iter 22 : loss : 1.1856 = 0.0452 + 1.1342 + 0.0061, time: 194.547122]
2023-06-05 18:46:10.971: epoch 22:	0.03016791  	0.06708321  	0.05530656  	0.10107603  	0.11380997  
2023-06-05 18:49:33.832: [iter 23 : loss : 1.1807 = 0.0417 + 1.1325 + 0.0064, time: 194.588331]
2023-06-05 18:49:37.471: epoch 23:	0.03007321  	0.06689993  	0.05516355  	0.10092006  	0.11350626  
2023-06-05 18:52:59.939: [iter 24 : loss : 1.1769 = 0.0390 + 1.1312 + 0.0067, time: 194.180896]
2023-06-05 18:53:04.036: epoch 24:	0.02997218  	0.06656748  	0.05500188  	0.10091195  	0.11356208  
2023-06-05 18:56:27.445: [iter 25 : loss : 1.1734 = 0.0365 + 1.1299 + 0.0070, time: 195.168585]
2023-06-05 18:56:31.272: epoch 25:	0.02989639  	0.06640307  	0.05484837  	0.10084134  	0.11326978  
2023-06-05 18:59:55.622: [iter 26 : loss : 1.1702 = 0.0341 + 1.1288 + 0.0073, time: 196.026646]
2023-06-05 18:59:59.748: epoch 26:	0.02982533  	0.06613028  	0.05467374  	0.10065243  	0.11294515  
2023-06-05 19:03:23.683: [iter 27 : loss : 1.1678 = 0.0325 + 1.1278 + 0.0075, time: 195.746450]
2023-06-05 19:03:27.830: epoch 27:	0.02973376  	0.06593643  	0.05449780  	0.10029222  	0.11258361  
2023-06-05 19:06:51.210: [iter 28 : loss : 1.1654 = 0.0307 + 1.1269 + 0.0078, time: 195.162329]
2023-06-05 19:06:55.327: epoch 28:	0.02965168  	0.06569396  	0.05433758  	0.10018585  	0.11247852  
2023-06-05 19:10:18.623: [iter 29 : loss : 1.1633 = 0.0291 + 1.1262 + 0.0080, time: 195.142712]
2023-06-05 19:10:22.813: epoch 29:	0.02956171  	0.06540659  	0.05414187  	0.09997019  	0.11216807  
2023-06-05 19:13:46.464: [iter 30 : loss : 1.1615 = 0.0278 + 1.1255 + 0.0082, time: 195.459288]
2023-06-05 19:13:50.415: epoch 30:	0.02950012  	0.06515186  	0.05403535  	0.09981952  	0.11211222  
2023-06-05 19:17:14.542: [iter 31 : loss : 1.1598 = 0.0265 + 1.1249 + 0.0084, time: 195.787138]
2023-06-05 19:17:18.548: epoch 31:	0.02936912  	0.06482069  	0.05381233  	0.09949522  	0.11176366  
2023-06-05 19:20:41.525: [iter 32 : loss : 1.1585 = 0.0256 + 1.1243 + 0.0086, time: 194.664024]
2023-06-05 19:20:45.589: epoch 32:	0.02922702  	0.06443344  	0.05361428  	0.09940685  	0.11164929  
2023-06-05 19:24:09.752: [iter 33 : loss : 1.1571 = 0.0245 + 1.1238 + 0.0088, time: 195.965598]
2023-06-05 19:24:13.997: epoch 33:	0.02910232  	0.06403362  	0.05331560  	0.09880428  	0.11100291  
2023-06-05 19:27:38.008: [iter 34 : loss : 1.1558 = 0.0235 + 1.1233 + 0.0090, time: 195.772043]
2023-06-05 19:27:41.584: epoch 34:	0.02902657  	0.06379703  	0.05314836  	0.09857333  	0.11070821  
2023-06-05 19:31:04.871: [iter 35 : loss : 1.1549 = 0.0229 + 1.1228 + 0.0092, time: 195.080731]
2023-06-05 19:31:09.017: epoch 35:	0.02892869  	0.06357418  	0.05297167  	0.09828153  	0.11035990  
2023-06-05 19:34:31.696: [iter 36 : loss : 1.1539 = 0.0221 + 1.1224 + 0.0093, time: 194.503662]
2023-06-05 19:34:35.530: epoch 36:	0.02887342  	0.06340823  	0.05281984  	0.09798377  	0.11002623  
2023-06-05 19:37:59.227: [iter 37 : loss : 1.1529 = 0.0213 + 1.1220 + 0.0095, time: 195.346229]
2023-06-05 19:38:03.313: epoch 37:	0.02881975  	0.06326200  	0.05269288  	0.09798262  	0.10990595  
2023-06-05 19:41:25.925: [iter 38 : loss : 1.1520 = 0.0206 + 1.1217 + 0.0097, time: 194.383688]
2023-06-05 19:41:30.251: epoch 38:	0.02872027  	0.06302676  	0.05253856  	0.09783734  	0.10965265  
2023-06-05 19:44:53.540: [iter 39 : loss : 1.1513 = 0.0202 + 1.1213 + 0.0098, time: 195.068597]
2023-06-05 19:44:56.877: epoch 39:	0.02860346  	0.06273878  	0.05232402  	0.09749227  	0.10932837  
2023-06-05 19:48:17.179: [iter 40 : loss : 1.1509 = 0.0199 + 1.1211 + 0.0100, time: 192.059069]
2023-06-05 19:48:21.305: epoch 40:	0.02852767  	0.06261691  	0.05214486  	0.09715722  	0.10893092  
2023-06-05 19:51:40.257: [iter 41 : loss : 1.1502 = 0.0193 + 1.1208 + 0.0101, time: 190.623193]
2023-06-05 19:51:44.279: epoch 41:	0.02840613  	0.06234730  	0.05189094  	0.09678559  	0.10840856  
2023-06-05 19:55:03.138: [iter 42 : loss : 1.1495 = 0.0187 + 1.1205 + 0.0102, time: 190.520006]
2023-06-05 19:55:07.139: epoch 42:	0.02825615  	0.06194403  	0.05165531  	0.09657843  	0.10806561  
2023-06-05 19:58:24.523: [iter 43 : loss : 1.1491 = 0.0184 + 1.1203 + 0.0104, time: 189.101328]
2023-06-05 19:58:28.804: epoch 43:	0.02824668  	0.06182726  	0.05157982  	0.09634398  	0.10795012  
2023-06-05 20:01:47.112: [iter 44 : loss : 1.1483 = 0.0177 + 1.1200 + 0.0105, time: 189.936014]
2023-06-05 20:01:51.312: epoch 44:	0.02816301  	0.06164953  	0.05139014  	0.09603893  	0.10771732  
2023-06-05 20:05:10.186: [iter 45 : loss : 1.1478 = 0.0174 + 1.1198 + 0.0106, time: 190.569220]
2023-06-05 20:05:14.149: epoch 45:	0.02804144  	0.06138349  	0.05124282  	0.09608112  	0.10761921  
2023-06-05 20:08:32.088: [iter 46 : loss : 1.1476 = 0.0172 + 1.1197 + 0.0107, time: 189.612848]
2023-06-05 20:08:36.089: epoch 46:	0.02805248  	0.06139756  	0.05110157  	0.09560904  	0.10701909  
2023-06-05 20:11:53.544: [iter 47 : loss : 1.1471 = 0.0168 + 1.1194 + 0.0108, time: 189.044783]
2023-06-05 20:11:57.699: epoch 47:	0.02798774  	0.06124422  	0.05096421  	0.09551949  	0.10672849  
2023-06-05 20:15:14.464: [iter 48 : loss : 1.1466 = 0.0163 + 1.1193 + 0.0109, time: 188.450189]
2023-06-05 20:15:18.559: epoch 48:	0.02785673  	0.06086132  	0.05078433  	0.09525881  	0.10661961  
2023-06-05 20:18:35.870: [iter 49 : loss : 1.1462 = 0.0160 + 1.1191 + 0.0111, time: 188.959985]
2023-06-05 20:18:39.803: epoch 49:	0.02777149  	0.06059382  	0.05056499  	0.09467214  	0.10615508  
2023-06-05 20:21:57.092: [iter 50 : loss : 1.1459 = 0.0158 + 1.1189 + 0.0112, time: 188.962353]
2023-06-05 20:22:01.773: epoch 50:	0.02764836  	0.06030139  	0.05039070  	0.09447979  	0.10589934  
2023-06-05 20:25:20.408: [iter 51 : loss : 1.1455 = 0.0155 + 1.1188 + 0.0112, time: 190.155029]
2023-06-05 20:25:24.867: epoch 51:	0.02756626  	0.06002874  	0.05025329  	0.09443665  	0.10584549  
2023-06-05 20:28:42.373: [iter 52 : loss : 1.1453 = 0.0154 + 1.1186 + 0.0113, time: 188.950061]
2023-06-05 20:28:46.325: epoch 52:	0.02745576  	0.05982198  	0.05004986  	0.09415177  	0.10539147  
2023-06-05 20:32:06.485: [iter 53 : loss : 1.1450 = 0.0151 + 1.1185 + 0.0114, time: 191.631197]
2023-06-05 20:32:11.223: epoch 53:	0.02741314  	0.05968304  	0.04993946  	0.09393192  	0.10522086  
2023-06-05 20:35:33.142: [iter 54 : loss : 1.1449 = 0.0150 + 1.1184 + 0.0115, time: 193.420800]
2023-06-05 20:35:37.526: epoch 54:	0.02733264  	0.05947865  	0.04973143  	0.09336345  	0.10464219  
2023-06-05 20:39:02.572: [iter 55 : loss : 1.1445 = 0.0147 + 1.1182 + 0.0116, time: 196.450815]
2023-06-05 20:39:07.197: epoch 55:	0.02732158  	0.05947184  	0.04967812  	0.09316827  	0.10460223  
2023-06-05 20:42:30.850: [iter 56 : loss : 1.1443 = 0.0146 + 1.1181 + 0.0117, time: 195.141038]
2023-06-05 20:42:35.401: epoch 56:	0.02721896  	0.05920264  	0.04946136  	0.09286285  	0.10405314  
2023-06-05 20:46:00.086: [iter 57 : loss : 1.1439 = 0.0142 + 1.1180 + 0.0118, time: 196.088035]
2023-06-05 20:46:04.646: epoch 57:	0.02712424  	0.05896450  	0.04931025  	0.09277974  	0.10394724  
2023-06-05 20:49:26.822: [iter 58 : loss : 1.1439 = 0.0142 + 1.1179 + 0.0118, time: 193.706027]
2023-06-05 20:49:31.650: epoch 58:	0.02707057  	0.05879037  	0.04921301  	0.09274288  	0.10377605  
2023-06-05 20:52:56.930: [iter 59 : loss : 1.1435 = 0.0137 + 1.1178 + 0.0119, time: 196.608028]
2023-06-05 20:53:01.585: epoch 59:	0.02701534  	0.05862951  	0.04907955  	0.09248320  	0.10353594  
2023-06-05 20:56:25.418: [iter 60 : loss : 1.1433 = 0.0137 + 1.1176 + 0.0120, time: 195.288840]
2023-06-05 20:56:30.046: epoch 60:	0.02691904  	0.05841540  	0.04900855  	0.09255186  	0.10375828  
2023-06-05 20:59:54.417: [iter 61 : loss : 1.1433 = 0.0137 + 1.1176 + 0.0120, time: 195.865850]
2023-06-05 20:59:59.153: epoch 61:	0.02683378  	0.05825346  	0.04879317  	0.09217045  	0.10307945  
2023-06-05 21:03:23.499: [iter 62 : loss : 1.1429 = 0.0133 + 1.1175 + 0.0121, time: 195.779552]
2023-06-05 21:03:28.232: epoch 62:	0.02679746  	0.05811059  	0.04873485  	0.09218067  	0.10304673  
2023-06-05 21:06:50.834: [iter 63 : loss : 1.1430 = 0.0134 + 1.1174 + 0.0122, time: 194.069350]
2023-06-05 21:06:54.939: epoch 63:	0.02671853  	0.05794351  	0.04852198  	0.09156649  	0.10249924  
2023-06-05 21:10:18.631: [iter 64 : loss : 1.1427 = 0.0131 + 1.1173 + 0.0122, time: 195.132568]
2023-06-05 21:10:23.029: epoch 64:	0.02665538  	0.05778486  	0.04844793  	0.09158773  	0.10250752  
2023-06-05 21:13:46.003: [iter 65 : loss : 1.1425 = 0.0130 + 1.1172 + 0.0123, time: 194.357496]
2023-06-05 21:13:50.734: epoch 65:	0.02663014  	0.05770361  	0.04844103  	0.09172834  	0.10271282  
2023-06-05 21:17:15.113: [iter 66 : loss : 1.1423 = 0.0128 + 1.1171 + 0.0124, time: 195.831305]
2023-06-05 21:17:20.096: epoch 66:	0.02653067  	0.05747936  	0.04827504  	0.09156001  	0.10238621  
2023-06-05 21:20:42.339: [iter 67 : loss : 1.1423 = 0.0128 + 1.1171 + 0.0124, time: 193.648469]
2023-06-05 21:20:46.701: epoch 67:	0.02644226  	0.05733042  	0.04819749  	0.09162410  	0.10236973  
2023-06-05 21:24:09.901: [iter 68 : loss : 1.1421 = 0.0126 + 1.1170 + 0.0125, time: 194.718773]
2023-06-05 21:24:13.831: epoch 68:	0.02648330  	0.05735558  	0.04821433  	0.09150200  	0.10246138  
2023-06-05 21:27:37.942: [iter 69 : loss : 1.1420 = 0.0125 + 1.1169 + 0.0125, time: 195.722852]
2023-06-05 21:27:41.990: epoch 69:	0.02637595  	0.05712735  	0.04802229  	0.09117769  	0.10205467  
2023-06-05 21:27:41.990: Early stopping is triggered at epoch: 69
2023-06-05 21:27:41.990: best_result@epoch 19:

2023-06-05 21:27:41.990: Loading from the saved model.
2023-06-05 21:27:46.057: 		0.03033211  	0.06761268  	0.05564264  	0.10172244  	0.11437698  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
