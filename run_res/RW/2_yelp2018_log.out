seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-05 13:16:57.049: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-05 13:16:57.049: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=2
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=0
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using default loss
2023-06-05 13:17:00.663: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-05 13:17:05.015: 		0.00025262  	0.00046350  	0.00042333  	0.00116740  	0.00116628  
2023-06-05 13:20:30.742: [iter 1 : loss : 1.8211 = 0.6931 + 1.1280 + 0.0000, time: 197.139948]
2023-06-05 13:20:34.896: epoch 1:	0.00220727  	0.00445135  	0.00366288  	0.00804495  	0.00846565  
2023-06-05 13:20:34.896: Found a better model.
2023-06-05 13:20:34.896: Save model to file as pretrain.
2023-06-05 13:23:57.719: [iter 2 : loss : 1.8190 = 0.6930 + 1.1260 + 0.0000, time: 193.853596]
2023-06-05 13:24:01.639: epoch 2:	0.00358253  	0.00735894  	0.00608057  	0.01315444  	0.01383355  
2023-06-05 13:24:01.639: Found a better model.
2023-06-05 13:24:01.639: Save model to file as pretrain.
2023-06-05 13:27:24.671: [iter 3 : loss : 1.8186 = 0.6928 + 1.1258 + 0.0000, time: 194.152192]
2023-06-05 13:27:28.486: epoch 3:	0.00445411  	0.00940092  	0.00783527  	0.01659874  	0.01766154  
2023-06-05 13:27:28.486: Found a better model.
2023-06-05 13:27:28.486: Save model to file as pretrain.
2023-06-05 13:30:51.219: [iter 4 : loss : 1.8183 = 0.6926 + 1.1257 + 0.0000, time: 193.901544]
2023-06-05 13:30:54.611: epoch 4:	0.00532726  	0.01143414  	0.00936773  	0.01912826  	0.02046699  
2023-06-05 13:30:54.612: Found a better model.
2023-06-05 13:30:54.612: Save model to file as pretrain.
2023-06-05 13:34:16.202: [iter 5 : loss : 1.8179 = 0.6922 + 1.1257 + 0.0000, time: 193.540887]
2023-06-05 13:34:20.478: epoch 5:	0.00473831  	0.01060047  	0.00821694  	0.01541966  	0.01672770  
2023-06-05 13:37:42.318: [iter 6 : loss : 1.8163 = 0.6904 + 1.1258 + 0.0000, time: 193.256231]
2023-06-05 13:37:46.506: epoch 6:	0.00404514  	0.00908676  	0.00702917  	0.01256463  	0.01376964  
2023-06-05 13:41:08.496: [iter 7 : loss : 1.8126 = 0.6862 + 1.1264 + 0.0000, time: 193.445200]
2023-06-05 13:41:12.689: epoch 7:	0.00594145  	0.01348637  	0.01070444  	0.01876113  	0.02119673  
2023-06-05 13:41:12.689: Found a better model.
2023-06-05 13:41:12.689: Save model to file as pretrain.
2023-06-05 13:44:35.339: [iter 8 : loss : 1.7936 = 0.6627 + 1.1308 + 0.0001, time: 193.862264]
2023-06-05 13:44:39.550: epoch 8:	0.01038581  	0.02353927  	0.01944264  	0.03489838  	0.03984006  
2023-06-05 13:44:39.550: Found a better model.
2023-06-05 13:44:39.550: Save model to file as pretrain.
2023-06-05 13:48:03.156: [iter 9 : loss : 1.7297 = 0.5853 + 1.1441 + 0.0003, time: 194.713905]
2023-06-05 13:48:07.095: epoch 9:	0.01896753  	0.04157554  	0.03462382  	0.06479103  	0.07272369  
2023-06-05 13:48:07.095: Found a better model.
2023-06-05 13:48:07.095: Save model to file as pretrain.
2023-06-05 13:51:29.297: [iter 10 : loss : 1.6320 = 0.4718 + 1.1595 + 0.0006, time: 193.378170]
2023-06-05 13:51:33.358: epoch 10:	0.02687948  	0.05903574  	0.04848890  	0.08936332  	0.10022736  
2023-06-05 13:51:33.358: Found a better model.
2023-06-05 13:51:33.358: Save model to file as pretrain.
2023-06-05 13:54:55.734: [iter 11 : loss : 1.4996 = 0.3158 + 1.1825 + 0.0012, time: 193.395602]
2023-06-05 13:54:59.381: epoch 11:	0.02804139  	0.06202871  	0.05088955  	0.09354221  	0.10519212  
2023-06-05 13:54:59.381: Found a better model.
2023-06-05 13:54:59.381: Save model to file as pretrain.
2023-06-05 13:58:21.685: [iter 12 : loss : 1.3949 = 0.2018 + 1.1912 + 0.0019, time: 193.381312]
2023-06-05 13:58:25.974: epoch 12:	0.02865549  	0.06355546  	0.05221125  	0.09579043  	0.10777272  
2023-06-05 13:58:25.974: Found a better model.
2023-06-05 13:58:25.974: Save model to file as pretrain.
2023-06-05 14:01:49.473: [iter 13 : loss : 1.3343 = 0.1462 + 1.1857 + 0.0025, time: 194.707694]
2023-06-05 14:01:53.640: epoch 13:	0.02918751  	0.06486964  	0.05335985  	0.09799811  	0.11019858  
2023-06-05 14:01:53.640: Found a better model.
2023-06-05 14:01:53.640: Save model to file as pretrain.
2023-06-05 14:05:17.062: [iter 14 : loss : 1.2962 = 0.1155 + 1.1777 + 0.0030, time: 194.686755]
2023-06-05 14:05:21.024: epoch 14:	0.02955850  	0.06592107  	0.05428281  	0.10004799  	0.11246156  
2023-06-05 14:05:21.024: Found a better model.
2023-06-05 14:05:21.024: Save model to file as pretrain.
2023-06-05 14:08:44.017: [iter 15 : loss : 1.2706 = 0.0963 + 1.1707 + 0.0035, time: 194.377272]
2023-06-05 14:08:47.970: epoch 15:	0.02987580  	0.06666296  	0.05492380  	0.10095439  	0.11373974  
2023-06-05 14:08:47.970: Found a better model.
2023-06-05 14:08:47.970: Save model to file as pretrain.
2023-06-05 14:12:12.577: [iter 16 : loss : 1.2515 = 0.0824 + 1.1651 + 0.0040, time: 196.008146]
2023-06-05 14:12:16.600: epoch 16:	0.03003368  	0.06693923  	0.05517784  	0.10135604  	0.11415667  
2023-06-05 14:12:16.600: Found a better model.
2023-06-05 14:12:16.601: Save model to file as pretrain.
2023-06-05 14:15:40.511: [iter 17 : loss : 1.2370 = 0.0721 + 1.1606 + 0.0044, time: 195.133897]
2023-06-05 14:15:44.584: epoch 17:	0.03007627  	0.06722023  	0.05544754  	0.10167436  	0.11465549  
2023-06-05 14:15:44.584: Found a better model.
2023-06-05 14:15:44.584: Save model to file as pretrain.
2023-06-05 14:19:09.760: [iter 18 : loss : 1.2256 = 0.0640 + 1.1569 + 0.0048, time: 196.499753]
2023-06-05 14:19:13.718: epoch 18:	0.03022469  	0.06754647  	0.05558262  	0.10176867  	0.11470129  
2023-06-05 14:19:13.718: Found a better model.
2023-06-05 14:19:13.718: Save model to file as pretrain.
2023-06-05 14:22:35.958: [iter 19 : loss : 1.2166 = 0.0577 + 1.1538 + 0.0051, time: 193.276479]
2023-06-05 14:22:40.072: epoch 19:	0.03027205  	0.06770957  	0.05566972  	0.10193744  	0.11495554  
2023-06-05 14:22:40.072: Found a better model.
2023-06-05 14:22:40.072: Save model to file as pretrain.
2023-06-05 14:26:05.439: [iter 20 : loss : 1.2087 = 0.0520 + 1.1512 + 0.0055, time: 195.929187]
2023-06-05 14:26:09.374: epoch 20:	0.03026100  	0.06766159  	0.05558889  	0.10180844  	0.11456879  
2023-06-05 14:29:31.084: [iter 21 : loss : 1.2025 = 0.0477 + 1.1490 + 0.0058, time: 193.355650]
2023-06-05 14:29:35.442: epoch 21:	0.03027523  	0.06772901  	0.05558234  	0.10175154  	0.11446249  
2023-06-05 14:29:35.442: Found a better model.
2023-06-05 14:29:35.442: Save model to file as pretrain.
2023-06-05 14:33:00.204: [iter 22 : loss : 1.1973 = 0.0441 + 1.1472 + 0.0061, time: 195.066227]
2023-06-05 14:33:04.684: epoch 22:	0.03024365  	0.06765513  	0.05547664  	0.10160049  	0.11422040  
2023-06-05 14:36:27.984: [iter 23 : loss : 1.1929 = 0.0409 + 1.1456 + 0.0064, time: 195.060025]
2023-06-05 14:36:32.179: epoch 23:	0.03022631  	0.06750036  	0.05533681  	0.10125913  	0.11380042  
2023-06-05 14:39:54.342: [iter 24 : loss : 1.1891 = 0.0381 + 1.1442 + 0.0067, time: 193.789392]
2023-06-05 14:39:57.728: epoch 24:	0.03017735  	0.06729788  	0.05524691  	0.10115414  	0.11377740  
2023-06-05 14:43:20.968: [iter 25 : loss : 1.1858 = 0.0358 + 1.1430 + 0.0070, time: 195.044116]
2023-06-05 14:43:25.413: epoch 25:	0.03012528  	0.06717245  	0.05512974  	0.10104156  	0.11365541  
2023-06-05 14:46:46.566: [iter 26 : loss : 1.1827 = 0.0335 + 1.1420 + 0.0072, time: 192.868707]
2023-06-05 14:46:50.510: epoch 26:	0.02997690  	0.06681343  	0.05496782  	0.10097485  	0.11365785  
2023-06-05 14:50:12.348: [iter 27 : loss : 1.1804 = 0.0319 + 1.1410 + 0.0075, time: 193.544216]
2023-06-05 14:50:16.278: epoch 27:	0.02997532  	0.06659610  	0.05480793  	0.10048468  	0.11313854  
2023-06-05 14:53:38.016: [iter 28 : loss : 1.1781 = 0.0302 + 1.1401 + 0.0077, time: 193.484703]
2023-06-05 14:53:41.854: epoch 28:	0.02980956  	0.06618119  	0.05460205  	0.10033755  	0.11310330  
2023-06-05 14:57:02.883: [iter 29 : loss : 1.1761 = 0.0287 + 1.1394 + 0.0079, time: 192.733315]
2023-06-05 14:57:06.884: epoch 29:	0.02973063  	0.06582161  	0.05435752  	0.10024637  	0.11277188  
2023-06-05 15:00:27.772: [iter 30 : loss : 1.1745 = 0.0276 + 1.1387 + 0.0082, time: 192.613727]
2023-06-05 15:00:32.166: epoch 30:	0.02954750  	0.06551114  	0.05409700  	0.09992577  	0.11228400  
2023-06-05 15:03:54.299: [iter 31 : loss : 1.1727 = 0.0262 + 1.1381 + 0.0084, time: 193.890058]
2023-06-05 15:03:57.578: epoch 31:	0.02945593  	0.06526129  	0.05382386  	0.09944770  	0.11173072  
2023-06-05 15:07:18.763: [iter 32 : loss : 1.1714 = 0.0253 + 1.1376 + 0.0086, time: 192.932863]
2023-06-05 15:07:22.984: epoch 32:	0.02938646  	0.06513025  	0.05368911  	0.09913980  	0.11134868  
2023-06-05 15:10:43.492: [iter 33 : loss : 1.1701 = 0.0243 + 1.1371 + 0.0087, time: 192.284478]
2023-06-05 15:10:47.755: epoch 33:	0.02930279  	0.06477230  	0.05354074  	0.09898569  	0.11133391  
2023-06-05 15:14:07.379: [iter 34 : loss : 1.1688 = 0.0233 + 1.1366 + 0.0089, time: 191.312399]
2023-06-05 15:14:11.459: epoch 34:	0.02920492  	0.06451156  	0.05334083  	0.09886985  	0.11101629  
2023-06-05 15:17:32.370: [iter 35 : loss : 1.1680 = 0.0228 + 1.1361 + 0.0091, time: 192.618139]
2023-06-05 15:17:36.467: epoch 35:	0.02911338  	0.06423979  	0.05314687  	0.09848581  	0.11077030  
2023-06-05 15:20:55.878: [iter 36 : loss : 1.1670 = 0.0220 + 1.1358 + 0.0093, time: 191.185871]
2023-06-05 15:21:00.098: epoch 36:	0.02898708  	0.06389296  	0.05298669  	0.09844136  	0.11066856  
2023-06-05 15:24:20.196: [iter 37 : loss : 1.1660 = 0.0212 + 1.1354 + 0.0094, time: 191.791253]
2023-06-05 15:24:24.475: epoch 37:	0.02892236  	0.06368236  	0.05284785  	0.09818536  	0.11054314  
2023-06-05 15:27:44.618: [iter 38 : loss : 1.1653 = 0.0206 + 1.1351 + 0.0096, time: 191.885701]
2023-06-05 15:27:48.448: epoch 38:	0.02884815  	0.06352034  	0.05271843  	0.09804324  	0.11050125  
2023-06-05 15:31:09.439: [iter 39 : loss : 1.1645 = 0.0201 + 1.1347 + 0.0097, time: 192.638265]
2023-06-05 15:31:13.510: epoch 39:	0.02872185  	0.06318387  	0.05246373  	0.09764698  	0.10995586  
2023-06-05 15:34:32.964: [iter 40 : loss : 1.1640 = 0.0197 + 1.1344 + 0.0099, time: 191.149400]
2023-06-05 15:34:37.464: epoch 40:	0.02867923  	0.06304402  	0.05235893  	0.09736476  	0.10967228  
2023-06-05 15:37:58.287: [iter 41 : loss : 1.1634 = 0.0193 + 1.1341 + 0.0100, time: 192.520026]
2023-06-05 15:38:02.511: epoch 41:	0.02858609  	0.06284326  	0.05214288  	0.09695338  	0.10934985  
2023-06-05 15:41:22.840: [iter 42 : loss : 1.1627 = 0.0186 + 1.1339 + 0.0101, time: 191.983495]
2023-06-05 15:41:27.062: epoch 42:	0.02847558  	0.06253985  	0.05195705  	0.09693681  	0.10924818  
2023-06-05 15:44:47.200: [iter 43 : loss : 1.1623 = 0.0184 + 1.1336 + 0.0103, time: 191.814146]
2023-06-05 15:44:51.389: epoch 43:	0.02837453  	0.06216349  	0.05177383  	0.09667501  	0.10896223  
2023-06-05 15:48:10.532: [iter 44 : loss : 1.1615 = 0.0177 + 1.1334 + 0.0104, time: 190.704721]
2023-06-05 15:48:14.454: epoch 44:	0.02829875  	0.06198820  	0.05167417  	0.09658359  	0.10886680  
2023-06-05 15:51:34.814: [iter 45 : loss : 1.1611 = 0.0174 + 1.1332 + 0.0105, time: 192.087882]
2023-06-05 15:51:38.778: epoch 45:	0.02823718  	0.06180724  	0.05149176  	0.09633776  	0.10852034  
2023-06-05 15:54:59.871: [iter 46 : loss : 1.1607 = 0.0171 + 1.1330 + 0.0106, time: 192.835750]
2023-06-05 15:55:04.176: epoch 46:	0.02812036  	0.06147918  	0.05127978  	0.09624216  	0.10825129  
2023-06-05 15:58:24.863: [iter 47 : loss : 1.1603 = 0.0168 + 1.1328 + 0.0107, time: 192.383621]
2023-06-05 15:58:29.267: epoch 47:	0.02805721  	0.06132290  	0.05113666  	0.09591092  	0.10789627  
2023-06-05 16:01:49.680: [iter 48 : loss : 1.1599 = 0.0164 + 1.1327 + 0.0108, time: 192.069194]
2023-06-05 16:01:53.817: epoch 48:	0.02800827  	0.06122478  	0.05104397  	0.09595685  	0.10766705  
2023-06-05 16:05:14.967: [iter 49 : loss : 1.1594 = 0.0160 + 1.1325 + 0.0109, time: 192.808448]
2023-06-05 16:05:19.264: epoch 49:	0.02799564  	0.06117174  	0.05097299  	0.09577283  	0.10756814  
2023-06-05 16:08:40.197: [iter 50 : loss : 1.1591 = 0.0158 + 1.1323 + 0.0110, time: 192.593569]
2023-06-05 16:08:44.261: epoch 50:	0.02784568  	0.06073685  	0.05067014  	0.09527172  	0.10705767  
2023-06-05 16:12:05.098: [iter 51 : loss : 1.1588 = 0.0155 + 1.1322 + 0.0111, time: 192.373993]
2023-06-05 16:12:09.221: epoch 51:	0.02781249  	0.06073527  	0.05050988  	0.09459332  	0.10642981  
2023-06-05 16:15:30.534: [iter 52 : loss : 1.1586 = 0.0154 + 1.1320 + 0.0112, time: 192.926572]
2023-06-05 16:15:34.777: epoch 52:	0.02766411  	0.06034468  	0.05028610  	0.09429005  	0.10616769  
2023-06-05 16:18:54.064: [iter 53 : loss : 1.1584 = 0.0151 + 1.1319 + 0.0113, time: 191.004787]
2023-06-05 16:18:58.077: epoch 53:	0.02756940  	0.06012781  	0.05016903  	0.09425113  	0.10601819  
2023-06-05 16:22:20.613: [iter 54 : loss : 1.1581 = 0.0149 + 1.1318 + 0.0114, time: 194.033591]
2023-06-05 16:22:25.015: epoch 54:	0.02747153  	0.05985449  	0.05001843  	0.09412480  	0.10597636  
2023-06-05 16:25:45.290: [iter 55 : loss : 1.1578 = 0.0147 + 1.1316 + 0.0115, time: 191.883473]
2023-06-05 16:25:49.442: epoch 55:	0.02742575  	0.05970833  	0.04990427  	0.09399493  	0.10572688  
2023-06-05 16:29:09.803: [iter 56 : loss : 1.1577 = 0.0146 + 1.1315 + 0.0116, time: 192.002902]
2023-06-05 16:29:13.904: epoch 56:	0.02737365  	0.05955753  	0.04976122  	0.09380899  	0.10553344  
2023-06-05 16:32:35.327: [iter 57 : loss : 1.1573 = 0.0142 + 1.1314 + 0.0116, time: 192.996338]
2023-06-05 16:32:39.312: epoch 57:	0.02731364  	0.05934396  	0.04952767  	0.09309120  	0.10472809  
2023-06-05 16:35:59.131: [iter 58 : loss : 1.1573 = 0.0142 + 1.1313 + 0.0117, time: 191.417805]
2023-06-05 16:36:03.469: epoch 58:	0.02722524  	0.05906732  	0.04938428  	0.09301119  	0.10461514  
2023-06-05 16:39:25.367: [iter 59 : loss : 1.1567 = 0.0137 + 1.1312 + 0.0118, time: 193.352058]
2023-06-05 16:39:29.776: epoch 59:	0.02719052  	0.05894157  	0.04927529  	0.09288722  	0.10448612  
2023-06-05 16:42:52.139: [iter 60 : loss : 1.1565 = 0.0136 + 1.1311 + 0.0119, time: 193.968340]
2023-06-05 16:42:56.254: epoch 60:	0.02712420  	0.05883093  	0.04919611  	0.09287316  	0.10443024  
2023-06-05 16:46:19.094: [iter 61 : loss : 1.1566 = 0.0137 + 1.1310 + 0.0119, time: 194.432733]
2023-06-05 16:46:23.332: epoch 61:	0.02708947  	0.05868532  	0.04906614  	0.09259802  	0.10408591  
2023-06-05 16:49:45.062: [iter 62 : loss : 1.1562 = 0.0133 + 1.1309 + 0.0120, time: 193.319295]
2023-06-05 16:49:49.269: epoch 62:	0.02704369  	0.05851290  	0.04894578  	0.09218543  	0.10370828  
2023-06-05 16:53:12.041: [iter 63 : loss : 1.1562 = 0.0133 + 1.1308 + 0.0121, time: 194.299760]
2023-06-05 16:53:16.149: epoch 63:	0.02695529  	0.05833115  	0.04882220  	0.09224251  	0.10362155  
2023-06-05 16:56:39.295: [iter 64 : loss : 1.1559 = 0.0130 + 1.1308 + 0.0121, time: 194.624918]
2023-06-05 16:56:43.299: epoch 64:	0.02691898  	0.05822595  	0.04862840  	0.09174573  	0.10301869  
2023-06-05 17:00:06.686: [iter 65 : loss : 1.1559 = 0.0131 + 1.1307 + 0.0122, time: 194.980995]
2023-06-05 17:00:10.723: epoch 65:	0.02685742  	0.05808609  	0.04849619  	0.09136539  	0.10276645  
2023-06-05 17:03:34.311: [iter 66 : loss : 1.1556 = 0.0128 + 1.1306 + 0.0122, time: 195.003734]
2023-06-05 17:03:37.818: epoch 66:	0.02677532  	0.05783574  	0.04837838  	0.09132545  	0.10268975  
2023-06-05 17:07:02.029: [iter 67 : loss : 1.1557 = 0.0129 + 1.1305 + 0.0123, time: 195.827815]
2023-06-05 17:07:06.021: epoch 67:	0.02664744  	0.05757895  	0.04824353  	0.09129731  	0.10268530  
2023-06-05 17:10:30.393: [iter 68 : loss : 1.1554 = 0.0126 + 1.1304 + 0.0123, time: 195.802789]
2023-06-05 17:10:34.435: epoch 68:	0.02660483  	0.05745686  	0.04813210  	0.09101816  	0.10243852  
2023-06-05 17:13:56.493: [iter 69 : loss : 1.1551 = 0.0124 + 1.1304 + 0.0124, time: 193.485382]
2023-06-05 17:14:00.640: epoch 69:	0.02654801  	0.05729708  	0.04807181  	0.09107201  	0.10247731  
2023-06-05 17:17:24.859: [iter 70 : loss : 1.1550 = 0.0123 + 1.1303 + 0.0125, time: 195.644196]
2023-06-05 17:17:29.113: epoch 70:	0.02645645  	0.05712721  	0.04793618  	0.09086306  	0.10222043  
2023-06-05 17:20:52.617: [iter 71 : loss : 1.1551 = 0.0124 + 1.1302 + 0.0125, time: 195.099585]
2023-06-05 17:20:56.647: epoch 71:	0.02636012  	0.05687094  	0.04774326  	0.09063630  	0.10192116  
2023-06-05 17:20:56.647: Early stopping is triggered at epoch: 71
2023-06-05 17:20:56.647: best_result@epoch 21:

2023-06-05 17:20:56.647: Loading from the saved model.
2023-06-05 17:21:00.834: 		0.03027523  	0.06772901  	0.05558225  	0.10175098  	0.11446194  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()

seed= 2021
split and save data...
2023-06-12 17:31:07.619: yelp2018_given_u0_i0
2023-06-12 17:31:07.619: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-12 17:31:14.513: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-12 17:31:14.513: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=0
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=2
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=0
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using default loss
2023-06-12 17:31:21.073: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-12 17:31:31.927: 		0.00032683  	0.00066075  	0.00050364  	0.00110738  	0.00112973  
2023-06-12 17:36:47.358: [iter 1 : loss : 1.8210 = 0.6931 + 1.1279 + 0.0000, time: 303.217043]
2023-06-12 17:36:54.744: epoch 1:	0.00208095  	0.00412276  	0.00339184  	0.00759179  	0.00784121  
2023-06-12 17:36:54.747: Found a better model.
2023-06-12 17:36:54.747: Save model to file as pretrain.
2023-06-12 17:41:50.350: [iter 2 : loss : 1.8190 = 0.6930 + 1.1260 + 0.0000, time: 283.276052]
2023-06-12 17:41:58.916: epoch 2:	0.00347516  	0.00695774  	0.00585160  	0.01302948  	0.01373622  
2023-06-12 17:41:58.916: Found a better model.
2023-06-12 17:41:58.916: Save model to file as pretrain.
2023-06-12 17:46:55.652: [iter 3 : loss : 1.8186 = 0.6928 + 1.1258 + 0.0000, time: 284.435064]
2023-06-12 17:47:03.925: epoch 3:	0.00458516  	0.00931950  	0.00790220  	0.01726729  	0.01832298  
2023-06-12 17:47:03.926: Found a better model.
2023-06-12 17:47:03.926: Save model to file as pretrain.
2023-06-12 17:51:58.012: [iter 4 : loss : 1.8183 = 0.6926 + 1.1257 + 0.0000, time: 281.998789]
2023-06-12 17:52:05.495: epoch 4:	0.00556885  	0.01161466  	0.00978051  	0.02083461  	0.02231729  
2023-06-12 17:52:05.496: Found a better model.
2023-06-12 17:52:05.496: Save model to file as pretrain.
2023-06-12 17:56:58.996: [iter 5 : loss : 1.8179 = 0.6922 + 1.1257 + 0.0000, time: 281.601197]
2023-06-12 17:57:06.376: epoch 5:	0.00493253  	0.01064452  	0.00846960  	0.01667873  	0.01784140  
2023-06-12 18:02:01.075: [iter 6 : loss : 1.8164 = 0.6906 + 1.1258 + 0.0000, time: 282.910580]
2023-06-12 18:02:09.365: epoch 6:	0.00404199  	0.00882751  	0.00718335  	0.01361585  	0.01495417  
2023-06-12 18:07:04.162: [iter 7 : loss : 1.8130 = 0.6866 + 1.1263 + 0.0000, time: 283.269030]
2023-06-12 18:07:12.366: epoch 7:	0.00587829  	0.01312995  	0.01046587  	0.01850615  	0.02087723  
2023-06-12 18:07:12.367: Found a better model.
2023-06-12 18:07:12.367: Save model to file as pretrain.
2023-06-12 18:12:07.415: [iter 8 : loss : 1.7961 = 0.6658 + 1.1303 + 0.0001, time: 283.413932]
2023-06-12 18:12:14.204: epoch 8:	0.01033528  	0.02352344  	0.01936892  	0.03450759  	0.03941493  
2023-06-12 18:12:14.205: Found a better model.
2023-06-12 18:12:14.205: Save model to file as pretrain.
2023-06-12 18:17:07.352: [iter 9 : loss : 1.7355 = 0.5922 + 1.1430 + 0.0002, time: 281.275697]
2023-06-12 18:17:15.731: epoch 9:	0.01836450  	0.04027455  	0.03328979  	0.06198185  	0.06967695  
2023-06-12 18:17:15.731: Found a better model.
2023-06-12 18:17:15.731: Save model to file as pretrain.
2023-06-12 18:22:16.097: [iter 10 : loss : 1.6383 = 0.4789 + 1.1588 + 0.0006, time: 288.639211]
2023-06-12 18:22:23.266: epoch 10:	0.02643587  	0.05808733  	0.04765226  	0.08800929  	0.09870752  
2023-06-12 18:22:23.266: Found a better model.
2023-06-12 18:22:23.266: Save model to file as pretrain.
2023-06-12 18:27:15.151: [iter 11 : loss : 1.5052 = 0.3215 + 1.1825 + 0.0012, time: 280.003462]
2023-06-12 18:27:23.359: epoch 11:	0.02761985  	0.06090328  	0.04992653  	0.09197667  	0.10307328  
2023-06-12 18:27:23.359: Found a better model.
2023-06-12 18:27:23.359: Save model to file as pretrain.
2023-06-12 18:32:21.786: [iter 12 : loss : 1.3980 = 0.2042 + 1.1919 + 0.0019, time: 286.961326]
2023-06-12 18:32:30.100: epoch 12:	0.02839027  	0.06276061  	0.05164038  	0.09503981  	0.10693566  
2023-06-12 18:32:30.100: Found a better model.
2023-06-12 18:32:30.100: Save model to file as pretrain.
2023-06-12 18:37:23.549: [iter 13 : loss : 1.3363 = 0.1478 + 1.1861 + 0.0025, time: 281.594254]
2023-06-12 18:37:30.197: epoch 13:	0.02896966  	0.06426488  	0.05290645  	0.09720965  	0.10971039  
2023-06-12 18:37:30.197: Found a better model.
2023-06-12 18:37:30.197: Save model to file as pretrain.
2023-06-12 18:42:30.249: [iter 14 : loss : 1.2977 = 0.1167 + 1.1780 + 0.0030, time: 288.083891]
2023-06-12 18:42:38.279: epoch 14:	0.02947013  	0.06561896  	0.05392862  	0.09884301  	0.11153630  
2023-06-12 18:42:38.279: Found a better model.
2023-06-12 18:42:38.280: Save model to file as pretrain.
2023-06-12 18:47:35.528: [iter 15 : loss : 1.2718 = 0.0973 + 1.1710 + 0.0035, time: 285.653379]
2023-06-12 18:47:43.497: epoch 15:	0.02986796  	0.06661578  	0.05462904  	0.09975657  	0.11277056  
2023-06-12 18:47:43.498: Found a better model.
2023-06-12 18:47:43.498: Save model to file as pretrain.
2023-06-12 18:52:40.243: [iter 16 : loss : 1.2524 = 0.0831 + 1.1654 + 0.0039, time: 285.152385]
2023-06-12 18:52:47.665: epoch 16:	0.03004795  	0.06699176  	0.05499796  	0.10045481  	0.11351008  
2023-06-12 18:52:47.665: Found a better model.
2023-06-12 18:52:47.665: Save model to file as pretrain.
2023-06-12 18:57:46.509: [iter 17 : loss : 1.2378 = 0.0727 + 1.1608 + 0.0044, time: 287.277478]
2023-06-12 18:57:54.784: epoch 17:	0.03007794  	0.06701692  	0.05509713  	0.10084569  	0.11370052  
2023-06-12 18:57:54.785: Found a better model.
2023-06-12 18:57:54.785: Save model to file as pretrain.
2023-06-12 19:02:48.257: [iter 18 : loss : 1.2262 = 0.0644 + 1.1570 + 0.0047, time: 281.432122]
2023-06-12 19:02:54.901: epoch 18:	0.03008269  	0.06697408  	0.05511559  	0.10071541  	0.11360843  
2023-06-12 19:07:49.195: [iter 19 : loss : 1.2170 = 0.0579 + 1.1539 + 0.0051, time: 282.358263]
2023-06-12 19:07:57.427: epoch 19:	0.03023898  	0.06720241  	0.05534680  	0.10103870  	0.11412099  
2023-06-12 19:07:57.427: Found a better model.
2023-06-12 19:07:57.427: Save model to file as pretrain.
2023-06-12 19:13:03.255: [iter 20 : loss : 1.2092 = 0.0524 + 1.1514 + 0.0055, time: 293.492231]
2023-06-12 19:13:11.606: epoch 20:	0.03025005  	0.06717017  	0.05532140  	0.10101058  	0.11414098  
2023-06-12 19:18:11.981: [iter 21 : loss : 1.2029 = 0.0479 + 1.1492 + 0.0058, time: 288.600263]
2023-06-12 19:18:20.277: epoch 21:	0.03019322  	0.06701046  	0.05519190  	0.10094592  	0.11393249  
2023-06-12 19:23:15.477: [iter 22 : loss : 1.1978 = 0.0443 + 1.1473 + 0.0061, time: 283.375093]
2023-06-12 19:23:23.719: epoch 22:	0.03014112  	0.06694540  	0.05513640  	0.10092833  	0.11398882  
2023-06-12 19:28:23.126: [iter 23 : loss : 1.1932 = 0.0410 + 1.1458 + 0.0064, time: 287.840290]
2023-06-12 19:28:31.305: epoch 23:	0.03004164  	0.06667140  	0.05500479  	0.10107096  	0.11399198  
2023-06-12 19:33:34.397: [iter 24 : loss : 1.1894 = 0.0383 + 1.1444 + 0.0067, time: 291.289623]
2023-06-12 19:33:41.158: epoch 24:	0.02996742  	0.06654453  	0.05489876  	0.10099440  	0.11398980  
2023-06-12 19:38:45.286: [iter 25 : loss : 1.1863 = 0.0361 + 1.1432 + 0.0070, time: 292.812140]
2023-06-12 19:38:53.911: epoch 25:	0.02984584  	0.06621122  	0.05465090  	0.10079332  	0.11366855  
2023-06-12 19:43:58.319: [iter 26 : loss : 1.1829 = 0.0336 + 1.1421 + 0.0072, time: 292.308375]
2023-06-12 19:44:04.926: epoch 26:	0.02983163  	0.06615482  	0.05455057  	0.10021800  	0.11322685  
2023-06-12 19:48:59.467: [iter 27 : loss : 1.1806 = 0.0319 + 1.1412 + 0.0075, time: 282.933367]
2023-06-12 19:49:05.960: epoch 27:	0.02974167  	0.06583112  	0.05432072  	0.09994741  	0.11278028  
2023-06-12 19:54:11.779: [iter 28 : loss : 1.1783 = 0.0303 + 1.1403 + 0.0077, time: 294.422082]
2023-06-12 19:54:20.048: epoch 28:	0.02966116  	0.06558698  	0.05413947  	0.09963062  	0.11238775  
2023-06-12 19:59:16.827: [iter 29 : loss : 1.1762 = 0.0288 + 1.1396 + 0.0079, time: 285.218008]
2023-06-12 19:59:25.244: epoch 29:	0.02964222  	0.06538729  	0.05400581  	0.09928010  	0.11208204  
2023-06-12 20:04:31.159: [iter 30 : loss : 1.1746 = 0.0277 + 1.1389 + 0.0081, time: 294.043542]
2023-06-12 20:04:37.904: epoch 30:	0.02950171  	0.06499108  	0.05373134  	0.09912714  	0.11173471  
2023-06-12 20:09:39.821: [iter 31 : loss : 1.1729 = 0.0262 + 1.1383 + 0.0083, time: 290.241044]
2023-06-12 20:09:48.165: epoch 31:	0.02941489  	0.06481749  	0.05359972  	0.09892707  	0.11167301  
2023-06-12 20:14:49.242: [iter 32 : loss : 1.1716 = 0.0254 + 1.1377 + 0.0085, time: 289.086893]
2023-06-12 20:14:57.451: epoch 32:	0.02930596  	0.06456535  	0.05338108  	0.09859232  	0.11132700  
2023-06-12 20:20:03.476: [iter 33 : loss : 1.1702 = 0.0243 + 1.1372 + 0.0087, time: 294.032415]
2023-06-12 20:20:11.788: epoch 33:	0.02914334  	0.06415569  	0.05313538  	0.09836672  	0.11113088  
2023-06-12 20:25:13.396: [iter 34 : loss : 1.1691 = 0.0235 + 1.1367 + 0.0089, time: 289.704857]
2023-06-12 20:25:21.621: epoch 34:	0.02901705  	0.06388474  	0.05298735  	0.09838732  	0.11122684  
2023-06-12 20:30:24.213: [iter 35 : loss : 1.1680 = 0.0227 + 1.1362 + 0.0091, time: 290.870145]
2023-06-12 20:30:30.821: epoch 35:	0.02889866  	0.06349138  	0.05275799  	0.09815512  	0.11084177  
2023-06-12 20:35:38.284: [iter 36 : loss : 1.1671 = 0.0220 + 1.1359 + 0.0092, time: 295.549847]
2023-06-12 20:35:46.318: epoch 36:	0.02882763  	0.06337286  	0.05253341  	0.09746356  	0.11010918  
2023-06-12 20:40:54.930: [iter 37 : loss : 1.1661 = 0.0213 + 1.1355 + 0.0094, time: 296.018281]
2023-06-12 20:41:03.380: epoch 37:	0.02866029  	0.06290814  	0.05224425  	0.09729207  	0.10963085  
2023-06-12 20:46:13.551: [iter 38 : loss : 1.1653 = 0.0206 + 1.1352 + 0.0095, time: 297.573198]
2023-06-12 20:46:22.146: epoch 38:	0.02863975  	0.06283889  	0.05219615  	0.09721964  	0.10953993  
2023-06-12 20:51:21.649: [iter 39 : loss : 1.1647 = 0.0201 + 1.1348 + 0.0097, time: 286.496330]
2023-06-12 20:51:30.108: epoch 39:	0.02853873  	0.06264326  	0.05199378  	0.09686340  	0.10917076  
2023-06-12 20:56:37.522: [iter 40 : loss : 1.1642 = 0.0198 + 1.1345 + 0.0098, time: 294.950038]
2023-06-12 20:56:45.623: epoch 40:	0.02844085  	0.06239055  	0.05174767  	0.09629253  	0.10852966  
2023-06-12 21:01:54.442: [iter 41 : loss : 1.1635 = 0.0193 + 1.1343 + 0.0100, time: 295.944082]
2023-06-12 21:02:01.217: epoch 41:	0.02835403  	0.06205247  	0.05152901  	0.09600447  	0.10820403  
2023-06-12 21:07:05.274: [iter 42 : loss : 1.1627 = 0.0186 + 1.1340 + 0.0101, time: 291.321839]
2023-06-12 21:07:13.559: epoch 42:	0.02832559  	0.06192299  	0.05148923  	0.09615698  	0.10833275  
2023-06-12 21:12:15.396: [iter 43 : loss : 1.1625 = 0.0185 + 1.1338 + 0.0102, time: 290.016040]
2023-06-12 21:12:22.461: epoch 43:	0.02818983  	0.06162588  	0.05128648  	0.09591506  	0.10808700  
2023-06-12 21:17:24.939: [iter 44 : loss : 1.1616 = 0.0177 + 1.1335 + 0.0104, time: 289.962943]
2023-06-12 21:17:32.305: epoch 44:	0.02816458  	0.06154229  	0.05117484  	0.09567968  	0.10776569  
2023-06-12 21:22:43.049: [iter 45 : loss : 1.1612 = 0.0174 + 1.1333 + 0.0105, time: 296.730195]
2023-06-12 21:22:49.868: epoch 45:	0.02803355  	0.06123463  	0.05096575  	0.09546413  	0.10757393  
2023-06-12 21:27:54.689: [iter 46 : loss : 1.1608 = 0.0171 + 1.1331 + 0.0106, time: 292.126285]
2023-06-12 21:28:03.468: epoch 46:	0.02797988  	0.06111082  	0.05081179  	0.09509908  	0.10719547  
2023-06-12 21:33:11.012: [iter 47 : loss : 1.1604 = 0.0167 + 1.1329 + 0.0107, time: 294.960454]
2023-06-12 21:33:19.451: epoch 47:	0.02788359  	0.06085586  	0.05065279  	0.09488509  	0.10701430  
2023-06-12 21:38:26.031: [iter 48 : loss : 1.1599 = 0.0163 + 1.1328 + 0.0108, time: 294.133241]
2023-06-12 21:38:32.765: epoch 48:	0.02774150  	0.06049570  	0.05044646  	0.09475321  	0.10680994  
2023-06-12 21:43:40.603: [iter 49 : loss : 1.1595 = 0.0160 + 1.1326 + 0.0109, time: 295.425377]
2023-06-12 21:43:48.337: epoch 49:	0.02772886  	0.06040653  	0.05035424  	0.09442365  	0.10661524  
2023-06-12 21:48:51.409: [iter 50 : loss : 1.1592 = 0.0158 + 1.1324 + 0.0110, time: 290.682472]
2023-06-12 21:48:58.571: epoch 50:	0.02762783  	0.06021585  	0.05016315  	0.09414336  	0.10616899  
2023-06-12 21:54:01.941: [iter 51 : loss : 1.1589 = 0.0155 + 1.1322 + 0.0111, time: 291.923730]
2023-06-12 21:54:10.062: epoch 51:	0.02754576  	0.05993686  	0.05001128  	0.09405480  	0.10594984  
2023-06-12 21:59:16.084: [iter 52 : loss : 1.1587 = 0.0154 + 1.1321 + 0.0112, time: 294.239061]
2023-06-12 21:59:24.198: epoch 52:	0.02745104  	0.05969775  	0.04982432  	0.09388193  	0.10560695  
2023-06-12 22:04:38.978: [iter 53 : loss : 1.1584 = 0.0151 + 1.1320 + 0.0113, time: 302.151179]
2023-06-12 22:04:47.733: epoch 53:	0.02737682  	0.05960727  	0.04971470  	0.09369016  	0.10537223  
2023-06-12 22:09:53.981: [iter 54 : loss : 1.1582 = 0.0150 + 1.1319 + 0.0114, time: 293.041685]
2023-06-12 22:10:00.806: epoch 54:	0.02722054  	0.05925914  	0.04944631  	0.09347631  	0.10494616  
2023-06-12 22:15:08.242: [iter 55 : loss : 1.1578 = 0.0147 + 1.1317 + 0.0115, time: 295.004656]
2023-06-12 22:15:15.830: epoch 55:	0.02724580  	0.05923971  	0.04939893  	0.09323344  	0.10485847  
2023-06-12 22:20:25.668: [iter 56 : loss : 1.1576 = 0.0145 + 1.1316 + 0.0115, time: 297.179832]
2023-06-12 22:20:33.872: epoch 56:	0.02712423  	0.05898347  	0.04920664  	0.09305334  	0.10458852  
2023-06-12 22:25:44.783: [iter 57 : loss : 1.1572 = 0.0141 + 1.1315 + 0.0116, time: 298.083978]
2023-06-12 22:25:53.052: epoch 57:	0.02707687  	0.05900804  	0.04915696  	0.09317102  	0.10456397  
2023-06-12 22:31:02.052: [iter 58 : loss : 1.1572 = 0.0142 + 1.1314 + 0.0117, time: 296.280208]
2023-06-12 22:31:10.494: epoch 58:	0.02696951  	0.05863533  	0.04898065  	0.09299894  	0.10443731  
2023-06-12 22:36:16.622: [iter 59 : loss : 1.1568 = 0.0137 + 1.1313 + 0.0118, time: 293.096222]
2023-06-12 22:36:24.440: epoch 59:	0.02693953  	0.05852659  	0.04884086  	0.09259310  	0.10389844  
2023-06-12 22:41:22.604: [iter 60 : loss : 1.1567 = 0.0137 + 1.1311 + 0.0118, time: 285.830631]
2023-06-12 22:41:30.333: epoch 60:	0.02681324  	0.05826015  	0.04863142  	0.09228148  	0.10360514  
2023-06-12 22:46:36.002: [iter 61 : loss : 1.1566 = 0.0136 + 1.1311 + 0.0119, time: 292.816281]
2023-06-12 22:46:42.531: epoch 61:	0.02679115  	0.05816004  	0.04853366  	0.09222801  	0.10334392  
2023-06-12 22:51:33.704: [iter 62 : loss : 1.1563 = 0.0134 + 1.1310 + 0.0120, time: 279.384258]
2023-06-12 22:51:41.931: epoch 62:	0.02678799  	0.05800863  	0.04842030  	0.09180915  	0.10303192  
2023-06-12 22:56:47.765: [iter 63 : loss : 1.1562 = 0.0133 + 1.1309 + 0.0120, time: 293.908082]
2023-06-12 22:56:54.337: epoch 63:	0.02671064  	0.05791913  	0.04826611  	0.09147074  	0.10258290  
2023-06-12 23:01:56.907: [iter 64 : loss : 1.1559 = 0.0130 + 1.1308 + 0.0121, time: 290.654544]
2023-06-12 23:02:05.202: epoch 64:	0.02669328  	0.05787835  	0.04827121  	0.09159392  	0.10281779  
2023-06-12 23:07:06.052: [iter 65 : loss : 1.1561 = 0.0132 + 1.1307 + 0.0121, time: 288.959077]
2023-06-12 23:07:14.427: epoch 65:	0.02663329  	0.05774006  	0.04812524  	0.09124940  	0.10238835  
2023-06-12 23:12:23.122: [iter 66 : loss : 1.1557 = 0.0129 + 1.1306 + 0.0122, time: 296.245766]
2023-06-12 23:12:31.754: epoch 66:	0.02663486  	0.05772924  	0.04812969  	0.09137438  	0.10245732  
2023-06-12 23:17:37.691: [iter 67 : loss : 1.1557 = 0.0129 + 1.1306 + 0.0123, time: 293.322310]
2023-06-12 23:17:44.598: epoch 67:	0.02659696  	0.05760855  	0.04804359  	0.09120999  	0.10234551  
2023-06-12 23:22:52.251: [iter 68 : loss : 1.1554 = 0.0126 + 1.1305 + 0.0123, time: 294.949598]
2023-06-12 23:23:00.473: epoch 68:	0.02655750  	0.05765384  	0.04803932  	0.09124362  	0.10237196  
2023-06-12 23:28:00.430: [iter 69 : loss : 1.1552 = 0.0124 + 1.1304 + 0.0124, time: 287.221506]
2023-06-12 23:28:07.397: epoch 69:	0.02651646  	0.05749942  	0.04793230  	0.09113314  	0.10222795  
2023-06-12 23:28:07.398: Early stopping is triggered at epoch: 69
2023-06-12 23:28:07.398: best_result@epoch 19:

2023-06-12 23:28:07.398: Loading from the saved model.
2023-06-12 23:28:15.932: 		0.03023898  	0.06720241  	0.05534680  	0.10103870  	0.11412099  
