seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-05 13:16:57.049: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-05 13:16:57.049: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=2
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=0
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using default loss
2023-06-05 13:17:00.663: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-05 13:17:05.015: 		0.00025262  	0.00046350  	0.00042333  	0.00116740  	0.00116628  
2023-06-05 13:20:30.742: [iter 1 : loss : 1.8211 = 0.6931 + 1.1280 + 0.0000, time: 197.139948]
2023-06-05 13:20:34.896: epoch 1:	0.00220727  	0.00445135  	0.00366288  	0.00804495  	0.00846565  
2023-06-05 13:20:34.896: Found a better model.
2023-06-05 13:20:34.896: Save model to file as pretrain.
2023-06-05 13:23:57.719: [iter 2 : loss : 1.8190 = 0.6930 + 1.1260 + 0.0000, time: 193.853596]
2023-06-05 13:24:01.639: epoch 2:	0.00358253  	0.00735894  	0.00608057  	0.01315444  	0.01383355  
2023-06-05 13:24:01.639: Found a better model.
2023-06-05 13:24:01.639: Save model to file as pretrain.
2023-06-05 13:27:24.671: [iter 3 : loss : 1.8186 = 0.6928 + 1.1258 + 0.0000, time: 194.152192]
2023-06-05 13:27:28.486: epoch 3:	0.00445411  	0.00940092  	0.00783527  	0.01659874  	0.01766154  
2023-06-05 13:27:28.486: Found a better model.
2023-06-05 13:27:28.486: Save model to file as pretrain.
2023-06-05 13:30:51.219: [iter 4 : loss : 1.8183 = 0.6926 + 1.1257 + 0.0000, time: 193.901544]
2023-06-05 13:30:54.611: epoch 4:	0.00532726  	0.01143414  	0.00936773  	0.01912826  	0.02046699  
2023-06-05 13:30:54.612: Found a better model.
2023-06-05 13:30:54.612: Save model to file as pretrain.
2023-06-05 13:34:16.202: [iter 5 : loss : 1.8179 = 0.6922 + 1.1257 + 0.0000, time: 193.540887]
2023-06-05 13:34:20.478: epoch 5:	0.00473831  	0.01060047  	0.00821694  	0.01541966  	0.01672770  
2023-06-05 13:37:42.318: [iter 6 : loss : 1.8163 = 0.6904 + 1.1258 + 0.0000, time: 193.256231]
2023-06-05 13:37:46.506: epoch 6:	0.00404514  	0.00908676  	0.00702917  	0.01256463  	0.01376964  
2023-06-05 13:41:08.496: [iter 7 : loss : 1.8126 = 0.6862 + 1.1264 + 0.0000, time: 193.445200]
2023-06-05 13:41:12.689: epoch 7:	0.00594145  	0.01348637  	0.01070444  	0.01876113  	0.02119673  
2023-06-05 13:41:12.689: Found a better model.
2023-06-05 13:41:12.689: Save model to file as pretrain.
2023-06-05 13:44:35.339: [iter 8 : loss : 1.7936 = 0.6627 + 1.1308 + 0.0001, time: 193.862264]
2023-06-05 13:44:39.550: epoch 8:	0.01038581  	0.02353927  	0.01944264  	0.03489838  	0.03984006  
2023-06-05 13:44:39.550: Found a better model.
2023-06-05 13:44:39.550: Save model to file as pretrain.
2023-06-05 13:48:03.156: [iter 9 : loss : 1.7297 = 0.5853 + 1.1441 + 0.0003, time: 194.713905]
2023-06-05 13:48:07.095: epoch 9:	0.01896753  	0.04157554  	0.03462382  	0.06479103  	0.07272369  
2023-06-05 13:48:07.095: Found a better model.
2023-06-05 13:48:07.095: Save model to file as pretrain.
2023-06-05 13:51:29.297: [iter 10 : loss : 1.6320 = 0.4718 + 1.1595 + 0.0006, time: 193.378170]
2023-06-05 13:51:33.358: epoch 10:	0.02687948  	0.05903574  	0.04848890  	0.08936332  	0.10022736  
2023-06-05 13:51:33.358: Found a better model.
2023-06-05 13:51:33.358: Save model to file as pretrain.
2023-06-05 13:54:55.734: [iter 11 : loss : 1.4996 = 0.3158 + 1.1825 + 0.0012, time: 193.395602]
2023-06-05 13:54:59.381: epoch 11:	0.02804139  	0.06202871  	0.05088955  	0.09354221  	0.10519212  
2023-06-05 13:54:59.381: Found a better model.
2023-06-05 13:54:59.381: Save model to file as pretrain.
2023-06-05 13:58:21.685: [iter 12 : loss : 1.3949 = 0.2018 + 1.1912 + 0.0019, time: 193.381312]
2023-06-05 13:58:25.974: epoch 12:	0.02865549  	0.06355546  	0.05221125  	0.09579043  	0.10777272  
2023-06-05 13:58:25.974: Found a better model.
2023-06-05 13:58:25.974: Save model to file as pretrain.
2023-06-05 14:01:49.473: [iter 13 : loss : 1.3343 = 0.1462 + 1.1857 + 0.0025, time: 194.707694]
2023-06-05 14:01:53.640: epoch 13:	0.02918751  	0.06486964  	0.05335985  	0.09799811  	0.11019858  
2023-06-05 14:01:53.640: Found a better model.
2023-06-05 14:01:53.640: Save model to file as pretrain.
2023-06-05 14:05:17.062: [iter 14 : loss : 1.2962 = 0.1155 + 1.1777 + 0.0030, time: 194.686755]
2023-06-05 14:05:21.024: epoch 14:	0.02955850  	0.06592107  	0.05428281  	0.10004799  	0.11246156  
2023-06-05 14:05:21.024: Found a better model.
2023-06-05 14:05:21.024: Save model to file as pretrain.
2023-06-05 14:08:44.017: [iter 15 : loss : 1.2706 = 0.0963 + 1.1707 + 0.0035, time: 194.377272]
2023-06-05 14:08:47.970: epoch 15:	0.02987580  	0.06666296  	0.05492380  	0.10095439  	0.11373974  
2023-06-05 14:08:47.970: Found a better model.
2023-06-05 14:08:47.970: Save model to file as pretrain.
2023-06-05 14:12:12.577: [iter 16 : loss : 1.2515 = 0.0824 + 1.1651 + 0.0040, time: 196.008146]
2023-06-05 14:12:16.600: epoch 16:	0.03003368  	0.06693923  	0.05517784  	0.10135604  	0.11415667  
2023-06-05 14:12:16.600: Found a better model.
2023-06-05 14:12:16.601: Save model to file as pretrain.
2023-06-05 14:15:40.511: [iter 17 : loss : 1.2370 = 0.0721 + 1.1606 + 0.0044, time: 195.133897]
2023-06-05 14:15:44.584: epoch 17:	0.03007627  	0.06722023  	0.05544754  	0.10167436  	0.11465549  
2023-06-05 14:15:44.584: Found a better model.
2023-06-05 14:15:44.584: Save model to file as pretrain.
2023-06-05 14:19:09.760: [iter 18 : loss : 1.2256 = 0.0640 + 1.1569 + 0.0048, time: 196.499753]
2023-06-05 14:19:13.718: epoch 18:	0.03022469  	0.06754647  	0.05558262  	0.10176867  	0.11470129  
2023-06-05 14:19:13.718: Found a better model.
2023-06-05 14:19:13.718: Save model to file as pretrain.
2023-06-05 14:22:35.958: [iter 19 : loss : 1.2166 = 0.0577 + 1.1538 + 0.0051, time: 193.276479]
2023-06-05 14:22:40.072: epoch 19:	0.03027205  	0.06770957  	0.05566972  	0.10193744  	0.11495554  
2023-06-05 14:22:40.072: Found a better model.
2023-06-05 14:22:40.072: Save model to file as pretrain.
2023-06-05 14:26:05.439: [iter 20 : loss : 1.2087 = 0.0520 + 1.1512 + 0.0055, time: 195.929187]
2023-06-05 14:26:09.374: epoch 20:	0.03026100  	0.06766159  	0.05558889  	0.10180844  	0.11456879  
2023-06-05 14:29:31.084: [iter 21 : loss : 1.2025 = 0.0477 + 1.1490 + 0.0058, time: 193.355650]
2023-06-05 14:29:35.442: epoch 21:	0.03027523  	0.06772901  	0.05558234  	0.10175154  	0.11446249  
2023-06-05 14:29:35.442: Found a better model.
2023-06-05 14:29:35.442: Save model to file as pretrain.
2023-06-05 14:33:00.204: [iter 22 : loss : 1.1973 = 0.0441 + 1.1472 + 0.0061, time: 195.066227]
2023-06-05 14:33:04.684: epoch 22:	0.03024365  	0.06765513  	0.05547664  	0.10160049  	0.11422040  
2023-06-05 14:36:27.984: [iter 23 : loss : 1.1929 = 0.0409 + 1.1456 + 0.0064, time: 195.060025]
2023-06-05 14:36:32.179: epoch 23:	0.03022631  	0.06750036  	0.05533681  	0.10125913  	0.11380042  
2023-06-05 14:39:54.342: [iter 24 : loss : 1.1891 = 0.0381 + 1.1442 + 0.0067, time: 193.789392]
2023-06-05 14:39:57.728: epoch 24:	0.03017735  	0.06729788  	0.05524691  	0.10115414  	0.11377740  
2023-06-05 14:43:20.968: [iter 25 : loss : 1.1858 = 0.0358 + 1.1430 + 0.0070, time: 195.044116]
2023-06-05 14:43:25.413: epoch 25:	0.03012528  	0.06717245  	0.05512974  	0.10104156  	0.11365541  
2023-06-05 14:46:46.566: [iter 26 : loss : 1.1827 = 0.0335 + 1.1420 + 0.0072, time: 192.868707]
2023-06-05 14:46:50.510: epoch 26:	0.02997690  	0.06681343  	0.05496782  	0.10097485  	0.11365785  
2023-06-05 14:50:12.348: [iter 27 : loss : 1.1804 = 0.0319 + 1.1410 + 0.0075, time: 193.544216]
2023-06-05 14:50:16.278: epoch 27:	0.02997532  	0.06659610  	0.05480793  	0.10048468  	0.11313854  
2023-06-05 14:53:38.016: [iter 28 : loss : 1.1781 = 0.0302 + 1.1401 + 0.0077, time: 193.484703]
2023-06-05 14:53:41.854: epoch 28:	0.02980956  	0.06618119  	0.05460205  	0.10033755  	0.11310330  
2023-06-05 14:57:02.883: [iter 29 : loss : 1.1761 = 0.0287 + 1.1394 + 0.0079, time: 192.733315]
2023-06-05 14:57:06.884: epoch 29:	0.02973063  	0.06582161  	0.05435752  	0.10024637  	0.11277188  
2023-06-05 15:00:27.772: [iter 30 : loss : 1.1745 = 0.0276 + 1.1387 + 0.0082, time: 192.613727]
2023-06-05 15:00:32.166: epoch 30:	0.02954750  	0.06551114  	0.05409700  	0.09992577  	0.11228400  
2023-06-05 15:03:54.299: [iter 31 : loss : 1.1727 = 0.0262 + 1.1381 + 0.0084, time: 193.890058]
2023-06-05 15:03:57.578: epoch 31:	0.02945593  	0.06526129  	0.05382386  	0.09944770  	0.11173072  
2023-06-05 15:07:18.763: [iter 32 : loss : 1.1714 = 0.0253 + 1.1376 + 0.0086, time: 192.932863]
2023-06-05 15:07:22.984: epoch 32:	0.02938646  	0.06513025  	0.05368911  	0.09913980  	0.11134868  
2023-06-05 15:10:43.492: [iter 33 : loss : 1.1701 = 0.0243 + 1.1371 + 0.0087, time: 192.284478]
2023-06-05 15:10:47.755: epoch 33:	0.02930279  	0.06477230  	0.05354074  	0.09898569  	0.11133391  
2023-06-05 15:14:07.379: [iter 34 : loss : 1.1688 = 0.0233 + 1.1366 + 0.0089, time: 191.312399]
2023-06-05 15:14:11.459: epoch 34:	0.02920492  	0.06451156  	0.05334083  	0.09886985  	0.11101629  
2023-06-05 15:17:32.370: [iter 35 : loss : 1.1680 = 0.0228 + 1.1361 + 0.0091, time: 192.618139]
2023-06-05 15:17:36.467: epoch 35:	0.02911338  	0.06423979  	0.05314687  	0.09848581  	0.11077030  
2023-06-05 15:20:55.878: [iter 36 : loss : 1.1670 = 0.0220 + 1.1358 + 0.0093, time: 191.185871]
2023-06-05 15:21:00.098: epoch 36:	0.02898708  	0.06389296  	0.05298669  	0.09844136  	0.11066856  
2023-06-05 15:24:20.196: [iter 37 : loss : 1.1660 = 0.0212 + 1.1354 + 0.0094, time: 191.791253]
2023-06-05 15:24:24.475: epoch 37:	0.02892236  	0.06368236  	0.05284785  	0.09818536  	0.11054314  
2023-06-05 15:27:44.618: [iter 38 : loss : 1.1653 = 0.0206 + 1.1351 + 0.0096, time: 191.885701]
2023-06-05 15:27:48.448: epoch 38:	0.02884815  	0.06352034  	0.05271843  	0.09804324  	0.11050125  
2023-06-05 15:31:09.439: [iter 39 : loss : 1.1645 = 0.0201 + 1.1347 + 0.0097, time: 192.638265]
2023-06-05 15:31:13.510: epoch 39:	0.02872185  	0.06318387  	0.05246373  	0.09764698  	0.10995586  
2023-06-05 15:34:32.964: [iter 40 : loss : 1.1640 = 0.0197 + 1.1344 + 0.0099, time: 191.149400]
2023-06-05 15:34:37.464: epoch 40:	0.02867923  	0.06304402  	0.05235893  	0.09736476  	0.10967228  
2023-06-05 15:37:58.287: [iter 41 : loss : 1.1634 = 0.0193 + 1.1341 + 0.0100, time: 192.520026]
2023-06-05 15:38:02.511: epoch 41:	0.02858609  	0.06284326  	0.05214288  	0.09695338  	0.10934985  
2023-06-05 15:41:22.840: [iter 42 : loss : 1.1627 = 0.0186 + 1.1339 + 0.0101, time: 191.983495]
2023-06-05 15:41:27.062: epoch 42:	0.02847558  	0.06253985  	0.05195705  	0.09693681  	0.10924818  
2023-06-05 15:44:47.200: [iter 43 : loss : 1.1623 = 0.0184 + 1.1336 + 0.0103, time: 191.814146]
2023-06-05 15:44:51.389: epoch 43:	0.02837453  	0.06216349  	0.05177383  	0.09667501  	0.10896223  
2023-06-05 15:48:10.532: [iter 44 : loss : 1.1615 = 0.0177 + 1.1334 + 0.0104, time: 190.704721]
2023-06-05 15:48:14.454: epoch 44:	0.02829875  	0.06198820  	0.05167417  	0.09658359  	0.10886680  
2023-06-05 15:51:34.814: [iter 45 : loss : 1.1611 = 0.0174 + 1.1332 + 0.0105, time: 192.087882]
2023-06-05 15:51:38.778: epoch 45:	0.02823718  	0.06180724  	0.05149176  	0.09633776  	0.10852034  
2023-06-05 15:54:59.871: [iter 46 : loss : 1.1607 = 0.0171 + 1.1330 + 0.0106, time: 192.835750]
2023-06-05 15:55:04.176: epoch 46:	0.02812036  	0.06147918  	0.05127978  	0.09624216  	0.10825129  
2023-06-05 15:58:24.863: [iter 47 : loss : 1.1603 = 0.0168 + 1.1328 + 0.0107, time: 192.383621]
2023-06-05 15:58:29.267: epoch 47:	0.02805721  	0.06132290  	0.05113666  	0.09591092  	0.10789627  
2023-06-05 16:01:49.680: [iter 48 : loss : 1.1599 = 0.0164 + 1.1327 + 0.0108, time: 192.069194]
2023-06-05 16:01:53.817: epoch 48:	0.02800827  	0.06122478  	0.05104397  	0.09595685  	0.10766705  
2023-06-05 16:05:14.967: [iter 49 : loss : 1.1594 = 0.0160 + 1.1325 + 0.0109, time: 192.808448]
2023-06-05 16:05:19.264: epoch 49:	0.02799564  	0.06117174  	0.05097299  	0.09577283  	0.10756814  
2023-06-05 16:08:40.197: [iter 50 : loss : 1.1591 = 0.0158 + 1.1323 + 0.0110, time: 192.593569]
2023-06-05 16:08:44.261: epoch 50:	0.02784568  	0.06073685  	0.05067014  	0.09527172  	0.10705767  
2023-06-05 16:12:05.098: [iter 51 : loss : 1.1588 = 0.0155 + 1.1322 + 0.0111, time: 192.373993]
2023-06-05 16:12:09.221: epoch 51:	0.02781249  	0.06073527  	0.05050988  	0.09459332  	0.10642981  
2023-06-05 16:15:30.534: [iter 52 : loss : 1.1586 = 0.0154 + 1.1320 + 0.0112, time: 192.926572]
2023-06-05 16:15:34.777: epoch 52:	0.02766411  	0.06034468  	0.05028610  	0.09429005  	0.10616769  
2023-06-05 16:18:54.064: [iter 53 : loss : 1.1584 = 0.0151 + 1.1319 + 0.0113, time: 191.004787]
2023-06-05 16:18:58.077: epoch 53:	0.02756940  	0.06012781  	0.05016903  	0.09425113  	0.10601819  
2023-06-05 16:22:20.613: [iter 54 : loss : 1.1581 = 0.0149 + 1.1318 + 0.0114, time: 194.033591]
2023-06-05 16:22:25.015: epoch 54:	0.02747153  	0.05985449  	0.05001843  	0.09412480  	0.10597636  
2023-06-05 16:25:45.290: [iter 55 : loss : 1.1578 = 0.0147 + 1.1316 + 0.0115, time: 191.883473]
2023-06-05 16:25:49.442: epoch 55:	0.02742575  	0.05970833  	0.04990427  	0.09399493  	0.10572688  
2023-06-05 16:29:09.803: [iter 56 : loss : 1.1577 = 0.0146 + 1.1315 + 0.0116, time: 192.002902]
2023-06-05 16:29:13.904: epoch 56:	0.02737365  	0.05955753  	0.04976122  	0.09380899  	0.10553344  
2023-06-05 16:32:35.327: [iter 57 : loss : 1.1573 = 0.0142 + 1.1314 + 0.0116, time: 192.996338]
2023-06-05 16:32:39.312: epoch 57:	0.02731364  	0.05934396  	0.04952767  	0.09309120  	0.10472809  
2023-06-05 16:35:59.131: [iter 58 : loss : 1.1573 = 0.0142 + 1.1313 + 0.0117, time: 191.417805]
2023-06-05 16:36:03.469: epoch 58:	0.02722524  	0.05906732  	0.04938428  	0.09301119  	0.10461514  
2023-06-05 16:39:25.367: [iter 59 : loss : 1.1567 = 0.0137 + 1.1312 + 0.0118, time: 193.352058]
2023-06-05 16:39:29.776: epoch 59:	0.02719052  	0.05894157  	0.04927529  	0.09288722  	0.10448612  
2023-06-05 16:42:52.139: [iter 60 : loss : 1.1565 = 0.0136 + 1.1311 + 0.0119, time: 193.968340]
2023-06-05 16:42:56.254: epoch 60:	0.02712420  	0.05883093  	0.04919611  	0.09287316  	0.10443024  
2023-06-05 16:46:19.094: [iter 61 : loss : 1.1566 = 0.0137 + 1.1310 + 0.0119, time: 194.432733]
2023-06-05 16:46:23.332: epoch 61:	0.02708947  	0.05868532  	0.04906614  	0.09259802  	0.10408591  
2023-06-05 16:49:45.062: [iter 62 : loss : 1.1562 = 0.0133 + 1.1309 + 0.0120, time: 193.319295]
2023-06-05 16:49:49.269: epoch 62:	0.02704369  	0.05851290  	0.04894578  	0.09218543  	0.10370828  
2023-06-05 16:53:12.041: [iter 63 : loss : 1.1562 = 0.0133 + 1.1308 + 0.0121, time: 194.299760]
2023-06-05 16:53:16.149: epoch 63:	0.02695529  	0.05833115  	0.04882220  	0.09224251  	0.10362155  
2023-06-05 16:56:39.295: [iter 64 : loss : 1.1559 = 0.0130 + 1.1308 + 0.0121, time: 194.624918]
2023-06-05 16:56:43.299: epoch 64:	0.02691898  	0.05822595  	0.04862840  	0.09174573  	0.10301869  
2023-06-05 17:00:06.686: [iter 65 : loss : 1.1559 = 0.0131 + 1.1307 + 0.0122, time: 194.980995]
2023-06-05 17:00:10.723: epoch 65:	0.02685742  	0.05808609  	0.04849619  	0.09136539  	0.10276645  
2023-06-05 17:03:34.311: [iter 66 : loss : 1.1556 = 0.0128 + 1.1306 + 0.0122, time: 195.003734]
2023-06-05 17:03:37.818: epoch 66:	0.02677532  	0.05783574  	0.04837838  	0.09132545  	0.10268975  
2023-06-05 17:07:02.029: [iter 67 : loss : 1.1557 = 0.0129 + 1.1305 + 0.0123, time: 195.827815]
2023-06-05 17:07:06.021: epoch 67:	0.02664744  	0.05757895  	0.04824353  	0.09129731  	0.10268530  
2023-06-05 17:10:30.393: [iter 68 : loss : 1.1554 = 0.0126 + 1.1304 + 0.0123, time: 195.802789]
2023-06-05 17:10:34.435: epoch 68:	0.02660483  	0.05745686  	0.04813210  	0.09101816  	0.10243852  
2023-06-05 17:13:56.493: [iter 69 : loss : 1.1551 = 0.0124 + 1.1304 + 0.0124, time: 193.485382]
2023-06-05 17:14:00.640: epoch 69:	0.02654801  	0.05729708  	0.04807181  	0.09107201  	0.10247731  
2023-06-05 17:17:24.859: [iter 70 : loss : 1.1550 = 0.0123 + 1.1303 + 0.0125, time: 195.644196]
2023-06-05 17:17:29.113: epoch 70:	0.02645645  	0.05712721  	0.04793618  	0.09086306  	0.10222043  
2023-06-05 17:20:52.617: [iter 71 : loss : 1.1551 = 0.0124 + 1.1302 + 0.0125, time: 195.099585]
2023-06-05 17:20:56.647: epoch 71:	0.02636012  	0.05687094  	0.04774326  	0.09063630  	0.10192116  
2023-06-05 17:20:56.647: Early stopping is triggered at epoch: 71
2023-06-05 17:20:56.647: best_result@epoch 21:

2023-06-05 17:20:56.647: Loading from the saved model.
2023-06-05 17:21:00.834: 		0.03027523  	0.06772901  	0.05558225  	0.10175098  	0.11446194  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
