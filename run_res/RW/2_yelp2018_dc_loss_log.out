seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-05 21:37:17.689: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-05 21:37:17.689: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=2
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=1
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using decoupled loss
2023-06-05 21:37:21.381: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-05 21:37:25.794: 		0.00029209  	0.00056424  	0.00045723  	0.00112776  	0.00112689  
2023-06-05 21:40:50.127: [iter 1 : loss : 1.8203 = 0.6931 + 1.1272 + 0.0000, time: 195.687766]
2023-06-05 21:40:54.096: epoch 1:	0.00218358  	0.00441390  	0.00369199  	0.00830716  	0.00866361  
2023-06-05 21:40:54.096: Found a better model.
2023-06-05 21:40:54.096: Save model to file as pretrain.
2023-06-05 21:44:16.419: [iter 2 : loss : 1.8182 = 0.6930 + 1.1253 + 0.0000, time: 192.787787]
2023-06-05 21:44:20.010: epoch 2:	0.00346253  	0.00681221  	0.00577281  	0.01274671  	0.01335665  
2023-06-05 21:44:20.010: Found a better model.
2023-06-05 21:44:20.010: Save model to file as pretrain.
2023-06-05 21:47:42.752: [iter 3 : loss : 1.8179 = 0.6928 + 1.1250 + 0.0000, time: 193.094330]
2023-06-05 21:47:46.657: epoch 3:	0.00449990  	0.00927556  	0.00787198  	0.01716773  	0.01809870  
2023-06-05 21:47:46.657: Found a better model.
2023-06-05 21:47:46.658: Save model to file as pretrain.
2023-06-05 21:51:11.064: [iter 4 : loss : 1.8176 = 0.6926 + 1.1249 + 0.0000, time: 194.773726]
2023-06-05 21:51:15.135: epoch 4:	0.00547569  	0.01174046  	0.00957853  	0.01923290  	0.02068612  
2023-06-05 21:51:15.135: Found a better model.
2023-06-05 21:51:15.135: Save model to file as pretrain.
2023-06-05 21:54:36.426: [iter 5 : loss : 1.8171 = 0.6922 + 1.1249 + 0.0000, time: 191.717780]
2023-06-05 21:54:40.363: epoch 5:	0.00444463  	0.00986441  	0.00761947  	0.01408655  	0.01520752  
2023-06-05 21:58:00.314: [iter 6 : loss : 1.8155 = 0.6903 + 1.1251 + 0.0000, time: 191.558456]
2023-06-05 21:58:04.683: epoch 6:	0.00392672  	0.00901444  	0.00712559  	0.01309493  	0.01447378  
2023-06-05 22:01:26.686: [iter 7 : loss : 1.8120 = 0.6863 + 1.1256 + 0.0000, time: 193.666500]
2023-06-05 22:01:30.658: epoch 7:	0.00565566  	0.01285985  	0.01020168  	0.01823528  	0.02035750  
2023-06-05 22:01:30.658: Found a better model.
2023-06-05 22:01:30.658: Save model to file as pretrain.
2023-06-05 22:04:52.130: [iter 8 : loss : 1.7935 = 0.6635 + 1.1299 + 0.0001, time: 192.136265]
2023-06-05 22:04:56.123: epoch 8:	0.01051370  	0.02388423  	0.01962592  	0.03480924  	0.03977835  
2023-06-05 22:04:56.123: Found a better model.
2023-06-05 22:04:56.123: Save model to file as pretrain.
2023-06-05 22:08:18.475: [iter 9 : loss : 1.7305 = 0.5873 + 1.1429 + 0.0003, time: 192.934658]
2023-06-05 22:08:22.791: epoch 9:	0.01874495  	0.04098367  	0.03390411  	0.06259038  	0.07073191  
2023-06-05 22:08:22.791: Found a better model.
2023-06-05 22:08:22.791: Save model to file as pretrain.
2023-06-05 22:11:46.907: [iter 10 : loss : 1.6327 = 0.4738 + 1.1583 + 0.0006, time: 194.743031]
2023-06-05 22:11:50.797: epoch 10:	0.02677375  	0.05868404  	0.04836931  	0.08970015  	0.10076945  
2023-06-05 22:11:50.797: Found a better model.
2023-06-05 22:11:50.797: Save model to file as pretrain.
2023-06-05 22:15:12.682: [iter 11 : loss : 1.5015 = 0.3195 + 1.1808 + 0.0012, time: 192.448844]
2023-06-05 22:15:16.625: epoch 11:	0.02790565  	0.06161221  	0.05062173  	0.09344660  	0.10471931  
2023-06-05 22:15:16.625: Found a better model.
2023-06-05 22:15:16.625: Save model to file as pretrain.
2023-06-05 22:18:40.662: [iter 12 : loss : 1.3960 = 0.2043 + 1.1898 + 0.0019, time: 194.704795]
2023-06-05 22:18:44.535: epoch 12:	0.02863025  	0.06340605  	0.05214671  	0.09640647  	0.10790436  
2023-06-05 22:18:44.535: Found a better model.
2023-06-05 22:18:44.535: Save model to file as pretrain.
2023-06-05 22:22:08.170: [iter 13 : loss : 1.3347 = 0.1476 + 1.1846 + 0.0025, time: 194.265184]
2023-06-05 22:22:11.812: epoch 13:	0.02928856  	0.06509339  	0.05335934  	0.09830643  	0.10982854  
2023-06-05 22:22:11.812: Found a better model.
2023-06-05 22:22:11.812: Save model to file as pretrain.
2023-06-05 22:25:35.132: [iter 14 : loss : 1.2961 = 0.1163 + 1.1767 + 0.0030, time: 194.014018]
2023-06-05 22:25:39.470: epoch 14:	0.02957431  	0.06581845  	0.05410967  	0.09974401  	0.11159988  
2023-06-05 22:25:39.470: Found a better model.
2023-06-05 22:25:39.470: Save model to file as pretrain.
2023-06-05 22:29:02.284: [iter 15 : loss : 1.2701 = 0.0968 + 1.1698 + 0.0035, time: 193.439604]
2023-06-05 22:29:06.453: epoch 15:	0.02996739  	0.06682940  	0.05489255  	0.10089023  	0.11319593  
2023-06-05 22:29:06.453: Found a better model.
2023-06-05 22:29:06.453: Save model to file as pretrain.
2023-06-05 22:32:29.512: [iter 16 : loss : 1.2507 = 0.0825 + 1.1642 + 0.0040, time: 193.646923]
2023-06-05 22:32:33.731: epoch 16:	0.03017421  	0.06735616  	0.05527708  	0.10131837  	0.11382554  
2023-06-05 22:32:33.731: Found a better model.
2023-06-05 22:32:33.731: Save model to file as pretrain.
2023-06-05 22:35:57.122: [iter 17 : loss : 1.2362 = 0.0722 + 1.1596 + 0.0044, time: 193.987807]
2023-06-05 22:36:01.104: epoch 17:	0.03028314  	0.06758349  	0.05548749  	0.10153576  	0.11416852  
2023-06-05 22:36:01.104: Found a better model.
2023-06-05 22:36:01.104: Save model to file as pretrain.
2023-06-05 22:39:24.707: [iter 18 : loss : 1.2248 = 0.0640 + 1.1560 + 0.0048, time: 194.238247]
2023-06-05 22:39:28.705: epoch 18:	0.03040471  	0.06781197  	0.05565290  	0.10177021  	0.11442497  
2023-06-05 22:39:28.706: Found a better model.
2023-06-05 22:39:28.706: Save model to file as pretrain.
2023-06-05 22:42:52.135: [iter 19 : loss : 1.2156 = 0.0575 + 1.1529 + 0.0051, time: 194.094214]
2023-06-05 22:42:55.994: epoch 19:	0.03039527  	0.06766646  	0.05565136  	0.10189832  	0.11459724  
2023-06-05 22:46:17.328: [iter 20 : loss : 1.2078 = 0.0519 + 1.1504 + 0.0055, time: 193.010043]
2023-06-05 22:46:21.369: epoch 20:	0.03038579  	0.06762056  	0.05556147  	0.10136770  	0.11421432  
2023-06-05 22:49:42.412: [iter 21 : loss : 1.2015 = 0.0475 + 1.1482 + 0.0058, time: 192.814507]
2023-06-05 22:49:46.509: epoch 21:	0.03034317  	0.06748663  	0.05550263  	0.10151376  	0.11430659  
2023-06-05 22:53:09.706: [iter 22 : loss : 1.1964 = 0.0440 + 1.1463 + 0.0061, time: 194.945459]
2023-06-05 22:53:13.669: epoch 22:	0.03032423  	0.06742398  	0.05547854  	0.10172046  	0.11458728  
2023-06-05 22:56:36.796: [iter 23 : loss : 1.1918 = 0.0407 + 1.1447 + 0.0064, time: 194.868548]
2023-06-05 22:56:41.016: epoch 23:	0.03019789  	0.06713343  	0.05534417  	0.10180055  	0.11464520  
2023-06-05 23:00:04.090: [iter 24 : loss : 1.1881 = 0.0380 + 1.1434 + 0.0067, time: 194.829774]
2023-06-05 23:00:07.876: epoch 24:	0.03016477  	0.06703451  	0.05531501  	0.10185568  	0.11482558  
2023-06-05 23:03:30.190: [iter 25 : loss : 1.1848 = 0.0356 + 1.1422 + 0.0070, time: 194.158494]
2023-06-05 23:03:34.854: epoch 25:	0.03013948  	0.06694958  	0.05511577  	0.10132231  	0.11420735  
2023-06-05 23:06:57.741: [iter 26 : loss : 1.1819 = 0.0335 + 1.1412 + 0.0072, time: 194.677454]
2023-06-05 23:07:02.305: epoch 26:	0.03005583  	0.06671337  	0.05496192  	0.10107533  	0.11404555  
2023-06-05 23:10:24.797: [iter 27 : loss : 1.1794 = 0.0317 + 1.1402 + 0.0075, time: 194.181275]
2023-06-05 23:10:28.705: epoch 27:	0.02991373  	0.06627035  	0.05465323  	0.10060577  	0.11347203  
2023-06-05 23:13:51.739: [iter 28 : loss : 1.1772 = 0.0302 + 1.1394 + 0.0077, time: 194.663956]
2023-06-05 23:13:55.603: epoch 28:	0.02984110  	0.06613164  	0.05452694  	0.10041694  	0.11322168  
2023-06-05 23:17:18.594: [iter 29 : loss : 1.1752 = 0.0287 + 1.1386 + 0.0079, time: 194.762047]
2023-06-05 23:17:22.771: epoch 29:	0.02974008  	0.06590680  	0.05432048  	0.10012442  	0.11285267  
2023-06-05 23:20:45.688: [iter 30 : loss : 1.1736 = 0.0274 + 1.1380 + 0.0082, time: 194.696872]
2023-06-05 23:20:49.223: epoch 30:	0.02966270  	0.06560573  	0.05409500  	0.09993105  	0.11242788  
2023-06-05 23:24:12.303: [iter 31 : loss : 1.1718 = 0.0261 + 1.1374 + 0.0084, time: 194.878034]
2023-06-05 23:24:16.441: epoch 31:	0.02956800  	0.06542033  	0.05395469  	0.09977634  	0.11223043  
2023-06-05 23:27:38.829: [iter 32 : loss : 1.1706 = 0.0252 + 1.1368 + 0.0086, time: 194.099467]
2023-06-05 23:27:43.051: epoch 32:	0.02945432  	0.06513615  	0.05375991  	0.09939987  	0.11187019  
2023-06-05 23:31:03.591: [iter 33 : loss : 1.1692 = 0.0241 + 1.1363 + 0.0088, time: 192.325487]
2023-06-05 23:31:07.633: epoch 33:	0.02939276  	0.06492102  	0.05361060  	0.09936463  	0.11162823  
2023-06-05 23:34:30.516: [iter 34 : loss : 1.1680 = 0.0232 + 1.1358 + 0.0089, time: 194.705969]
2023-06-05 23:34:34.497: epoch 34:	0.02924435  	0.06457875  	0.05331425  	0.09876493  	0.11104564  
2023-06-05 23:37:56.476: [iter 35 : loss : 1.1670 = 0.0225 + 1.1354 + 0.0091, time: 193.651444]
2023-06-05 23:38:00.738: epoch 35:	0.02912595  	0.06424231  	0.05312581  	0.09874423  	0.11081041  
2023-06-05 23:41:22.922: [iter 36 : loss : 1.1662 = 0.0219 + 1.1350 + 0.0093, time: 194.057849]
2023-06-05 23:41:26.989: epoch 36:	0.02903437  	0.06399591  	0.05296493  	0.09853212  	0.11059099  
2023-06-05 23:44:48.756: [iter 37 : loss : 1.1651 = 0.0210 + 1.1346 + 0.0094, time: 193.503325]
2023-06-05 23:44:53.005: epoch 37:	0.02893334  	0.06361310  	0.05279415  	0.09849840  	0.11059165  
2023-06-05 23:48:16.163: [iter 38 : loss : 1.1643 = 0.0204 + 1.1343 + 0.0096, time: 194.928187]
2023-06-05 23:48:19.951: epoch 38:	0.02894911  	0.06352276  	0.05276290  	0.09844571  	0.11061640  
2023-06-05 23:51:42.897: [iter 39 : loss : 1.1636 = 0.0199 + 1.1340 + 0.0097, time: 194.717180]
2023-06-05 23:51:46.904: epoch 39:	0.02877705  	0.06309465  	0.05243417  	0.09776546  	0.10992093  
2023-06-05 23:55:09.412: [iter 40 : loss : 1.1632 = 0.0197 + 1.1337 + 0.0099, time: 194.206666]
2023-06-05 23:55:13.737: epoch 40:	0.02866813  	0.06290093  	0.05228356  	0.09772356  	0.10965515  
2023-06-05 23:58:36.134: [iter 41 : loss : 1.1625 = 0.0191 + 1.1334 + 0.0100, time: 194.189834]
2023-06-05 23:58:39.494: epoch 41:	0.02859710  	0.06269075  	0.05205784  	0.09733126  	0.10914847  
2023-06-06 00:02:01.422: [iter 42 : loss : 1.1617 = 0.0184 + 1.1331 + 0.0101, time: 193.709267]
2023-06-06 00:02:05.503: epoch 42:	0.02846290  	0.06227288  	0.05176207  	0.09680860  	0.10856415  
2023-06-06 00:05:27.997: [iter 43 : loss : 1.1614 = 0.0182 + 1.1329 + 0.0103, time: 194.199620]
2023-06-06 00:05:32.050: epoch 43:	0.02844870  	0.06221750  	0.05166568  	0.09653346  	0.10829737  
2023-06-06 00:08:55.775: [iter 44 : loss : 1.1607 = 0.0176 + 1.1327 + 0.0104, time: 195.422485]
2023-06-06 00:09:00.165: epoch 44:	0.02836658  	0.06207116  	0.05155661  	0.09638628  	0.10810833  
2023-06-06 00:12:21.175: [iter 45 : loss : 1.1601 = 0.0172 + 1.1324 + 0.0105, time: 192.759056]
2023-06-06 00:12:25.361: epoch 45:	0.02835239  	0.06207009  	0.05147768  	0.09615740  	0.10789789  
2023-06-06 00:15:48.444: [iter 46 : loss : 1.1599 = 0.0170 + 1.1323 + 0.0106, time: 194.762341]
2023-06-06 00:15:52.584: epoch 46:	0.02818663  	0.06164215  	0.05117761  	0.09586412  	0.10735507  
2023-06-06 00:19:14.269: [iter 47 : loss : 1.1595 = 0.0167 + 1.1321 + 0.0107, time: 193.442441]
2023-06-06 00:19:18.363: epoch 47:	0.02813297  	0.06145361  	0.05104682  	0.09567668  	0.10711010  
2023-06-06 00:22:40.779: [iter 48 : loss : 1.1590 = 0.0162 + 1.1319 + 0.0108, time: 194.131990]
2023-06-06 00:22:44.901: epoch 48:	0.02803824  	0.06118320  	0.05087459  	0.09557240  	0.10686746  
2023-06-06 00:26:07.521: [iter 49 : loss : 1.1586 = 0.0159 + 1.1317 + 0.0110, time: 194.262334]
2023-06-06 00:26:11.673: epoch 49:	0.02799720  	0.06108053  	0.05077298  	0.09533789  	0.10668465  
2023-06-06 00:29:34.121: [iter 50 : loss : 1.1584 = 0.0157 + 1.1316 + 0.0111, time: 194.047716]
2023-06-06 00:29:38.186: epoch 50:	0.02791196  	0.06084894  	0.05062677  	0.09512381  	0.10645176  
2023-06-06 00:33:01.935: [iter 51 : loss : 1.1581 = 0.0155 + 1.1314 + 0.0111, time: 195.418555]
2023-06-06 00:33:06.011: epoch 51:	0.02783774  	0.06072905  	0.05048040  	0.09486036  	0.10620359  
2023-06-06 00:36:29.211: [iter 52 : loss : 1.1579 = 0.0153 + 1.1313 + 0.0112, time: 194.752360]
2023-06-06 00:36:32.522: epoch 52:	0.02779040  	0.06063262  	0.05040882  	0.09497920  	0.10609331  
2023-06-06 00:39:56.388: [iter 53 : loss : 1.1575 = 0.0150 + 1.1312 + 0.0113, time: 195.574864]
2023-06-06 00:40:00.418: epoch 53:	0.02768463  	0.06031642  	0.05015224  	0.09435457  	0.10548966  
2023-06-06 00:43:24.255: [iter 54 : loss : 1.1574 = 0.0149 + 1.1310 + 0.0114, time: 195.497456]
2023-06-06 00:43:28.593: epoch 54:	0.02752520  	0.05988460  	0.04989967  	0.09427342  	0.10523278  
2023-06-06 00:46:51.320: [iter 55 : loss : 1.1569 = 0.0145 + 1.1309 + 0.0115, time: 194.291372]
2023-06-06 00:46:55.554: epoch 55:	0.02739099  	0.05955565  	0.04978120  	0.09444650  	0.10543595  
2023-06-06 00:50:18.607: [iter 56 : loss : 1.1568 = 0.0145 + 1.1308 + 0.0116, time: 194.686351]
2023-06-06 00:50:22.709: epoch 56:	0.02733101  	0.05945516  	0.04964893  	0.09409142  	0.10513780  
2023-06-06 00:53:44.454: [iter 57 : loss : 1.1565 = 0.0142 + 1.1307 + 0.0117, time: 193.395324]
2023-06-06 00:53:48.773: epoch 57:	0.02730259  	0.05932205  	0.04951875  	0.09368189  	0.10481121  
2023-06-06 00:57:12.006: [iter 58 : loss : 1.1563 = 0.0140 + 1.1306 + 0.0117, time: 194.932407]
2023-06-06 00:57:16.098: epoch 58:	0.02723628  	0.05921333  	0.04939165  	0.09350663  	0.10451993  
2023-06-06 01:00:37.502: [iter 59 : loss : 1.1559 = 0.0137 + 1.1305 + 0.0118, time: 192.921576]
2023-06-06 01:00:41.700: epoch 59:	0.02708156  	0.05890978  	0.04920413  	0.09343399  	0.10430534  
2023-06-06 01:04:04.614: [iter 60 : loss : 1.1559 = 0.0136 + 1.1303 + 0.0119, time: 194.520566]
2023-06-06 01:04:08.800: epoch 60:	0.02711316  	0.05892446  	0.04915779  	0.09319588  	0.10405544  
2023-06-06 01:07:30.585: [iter 61 : loss : 1.1557 = 0.0135 + 1.1303 + 0.0119, time: 193.447573]
2023-06-06 01:07:34.613: epoch 61:	0.02704843  	0.05866078  	0.04896005  	0.09285489  	0.10365168  
2023-06-06 01:10:56.722: [iter 62 : loss : 1.1554 = 0.0133 + 1.1302 + 0.0120, time: 193.680452]
2023-06-06 01:11:00.755: epoch 62:	0.02694897  	0.05844409  	0.04883715  	0.09282903  	0.10359594  
2023-06-06 01:14:21.600: [iter 63 : loss : 1.1553 = 0.0132 + 1.1301 + 0.0121, time: 192.501484]
2023-06-06 01:14:24.960: epoch 63:	0.02696316  	0.05848642  	0.04882892  	0.09268904  	0.10349365  
2023-06-06 01:17:47.395: [iter 64 : loss : 1.1552 = 0.0130 + 1.1300 + 0.0121, time: 194.096798]
2023-06-06 01:17:51.500: epoch 64:	0.02689527  	0.05825293  	0.04863706  	0.09241588  	0.10308327  
2023-06-06 01:21:12.680: [iter 65 : loss : 1.1551 = 0.0130 + 1.1299 + 0.0122, time: 192.737967]
2023-06-06 01:21:16.838: epoch 65:	0.02686844  	0.05822989  	0.04862522  	0.09228635  	0.10313992  
2023-06-06 01:24:39.126: [iter 66 : loss : 1.1549 = 0.0129 + 1.1298 + 0.0122, time: 193.890616]
2023-06-06 01:24:43.167: epoch 66:	0.02682268  	0.05819211  	0.04847295  	0.09198670  	0.10272774  
2023-06-06 01:28:06.301: [iter 67 : loss : 1.1548 = 0.0128 + 1.1298 + 0.0123, time: 194.667561]
2023-06-06 01:28:10.338: epoch 67:	0.02673109  	0.05788978  	0.04836289  	0.09196734  	0.10277972  
2023-06-06 01:31:30.826: [iter 68 : loss : 1.1546 = 0.0125 + 1.1297 + 0.0124, time: 192.078965]
2023-06-06 01:31:34.895: epoch 68:	0.02669637  	0.05785862  	0.04831004  	0.09190553  	0.10266038  
2023-06-06 01:31:34.895: Early stopping is triggered at epoch: 68
2023-06-06 01:31:34.895: best_result@epoch 18:

2023-06-06 01:31:34.895: Loading from the saved model.
2023-06-06 01:31:39.249: 		0.03040471  	0.06781197  	0.05565290  	0.10177021  	0.11442497  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
