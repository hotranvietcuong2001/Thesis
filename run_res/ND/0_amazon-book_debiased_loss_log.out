seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-06 15:19:07.373: Dataset name: amazon-book
The number of users: 52643
The number of items: 91599
The number of ratings: 2984108
Average actions of users: 56.69
Average actions of items: 32.58
The sparsity of the dataset: 99.938115%
2023-06-06 15:19:07.373: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=amazon-book
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=0
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=2
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using debiased loss
2023-06-06 15:19:13.224: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-06 15:19:28.247: 		0.00013678  	0.00026263  	0.00021307  	0.00052583  	0.00052583  
2023-06-06 15:24:36.881: [iter 1 : loss : 6.9955 = 0.6931 + 6.3024 + 0.0000, time: 307.595783]
2023-06-06 15:24:48.447: epoch 1:	0.00037425  	0.00077257  	0.00070682  	0.00159080  	0.00167924  
2023-06-06 15:24:48.447: Found a better model.
2023-06-06 15:24:48.447: Save model to file as pretrain.
2023-06-06 15:29:57.651: [iter 2 : loss : 6.9842 = 0.6930 + 6.2913 + 0.0000, time: 305.691518]
2023-06-06 15:30:09.456: epoch 2:	0.00044169  	0.00080725  	0.00073451  	0.00188940  	0.00193697  
2023-06-06 15:30:09.456: Found a better model.
2023-06-06 15:30:09.456: Save model to file as pretrain.
2023-06-06 15:35:20.018: [iter 3 : loss : 6.9823 = 0.6928 + 6.2895 + 0.0000, time: 307.063338]
2023-06-06 15:35:29.648: epoch 3:	0.00045688  	0.00086221  	0.00075041  	0.00192530  	0.00199637  
2023-06-06 15:35:29.648: Found a better model.
2023-06-06 15:35:29.648: Save model to file as pretrain.
2023-06-06 15:40:38.305: [iter 4 : loss : 6.9812 = 0.6926 + 6.2886 + 0.0000, time: 305.305604]
2023-06-06 15:40:49.895: epoch 4:	0.00044739  	0.00086454  	0.00072133  	0.00173959  	0.00177297  
2023-06-06 15:40:49.895: Found a better model.
2023-06-06 15:40:49.895: Save model to file as pretrain.
2023-06-06 15:45:59.199: [iter 5 : loss : 6.9800 = 0.6923 + 6.2877 + 0.0000, time: 305.985168]
2023-06-06 15:46:07.506: epoch 5:	0.00049203  	0.00089794  	0.00080452  	0.00198233  	0.00205299  
2023-06-06 15:46:07.507: Found a better model.
2023-06-06 15:46:07.507: Save model to file as pretrain.
2023-06-06 15:51:17.440: [iter 6 : loss : 6.9797 = 0.6920 + 6.2877 + 0.0000, time: 306.499868]
2023-06-06 15:51:29.248: epoch 6:	0.00058891  	0.00110088  	0.00094596  	0.00215071  	0.00218589  
2023-06-06 15:51:29.248: Found a better model.
2023-06-06 15:51:29.248: Save model to file as pretrain.
2023-06-06 15:56:39.740: [iter 7 : loss : 6.9791 = 0.6915 + 6.2876 + 0.0000, time: 307.044106]
2023-06-06 15:56:52.678: epoch 7:	0.00057372  	0.00112176  	0.00092494  	0.00199430  	0.00202208  
2023-06-06 15:56:52.678: Found a better model.
2023-06-06 15:56:52.679: Save model to file as pretrain.
2023-06-06 16:02:06.135: [iter 8 : loss : 6.9789 = 0.6908 + 6.2880 + 0.0000, time: 310.100715]
2023-06-06 16:02:16.276: epoch 8:	0.00063831  	0.00125385  	0.00103832  	0.00236023  	0.00239247  
2023-06-06 16:02:16.276: Found a better model.
2023-06-06 16:02:16.276: Save model to file as pretrain.
2023-06-06 16:07:29.242: [iter 9 : loss : 6.9779 = 0.6898 + 6.2880 + 0.0001, time: 309.519529]
2023-06-06 16:07:41.570: epoch 9:	0.00064401  	0.00129610  	0.00108094  	0.00249240  	0.00255329  
2023-06-06 16:07:41.570: Found a better model.
2023-06-06 16:07:41.570: Save model to file as pretrain.
2023-06-06 16:12:52.754: [iter 10 : loss : 6.9770 = 0.6882 + 6.2887 + 0.0001, time: 307.719455]
2023-06-06 16:13:08.053: epoch 10:	0.00079503  	0.00163874  	0.00134778  	0.00303826  	0.00308422  
2023-06-06 16:13:08.053: Found a better model.
2023-06-06 16:13:08.053: Save model to file as pretrain.
2023-06-06 16:18:20.562: [iter 11 : loss : 6.9753 = 0.6856 + 6.2896 + 0.0001, time: 308.865117]
2023-06-06 16:18:28.910: epoch 11:	0.00109709  	0.00249872  	0.00198025  	0.00415936  	0.00434938  
2023-06-06 16:18:28.910: Found a better model.
2023-06-06 16:18:28.910: Save model to file as pretrain.
2023-06-06 16:23:37.409: [iter 12 : loss : 6.9713 = 0.6805 + 6.2906 + 0.0001, time: 305.019196]
2023-06-06 16:23:52.248: epoch 12:	0.00174112  	0.00424311  	0.00341579  	0.00714646  	0.00746836  
2023-06-06 16:23:52.248: Found a better model.
2023-06-06 16:23:52.248: Save model to file as pretrain.
2023-06-06 16:29:02.646: [iter 13 : loss : 6.9628 = 0.6688 + 6.2938 + 0.0002, time: 307.028920]
2023-06-06 16:29:14.221: epoch 13:	0.00378911  	0.00964613  	0.00772609  	0.01466686  	0.01576554  
2023-06-06 16:29:14.221: Found a better model.
2023-06-06 16:29:14.221: Save model to file as pretrain.
2023-06-06 16:34:21.789: [iter 14 : loss : 6.9395 = 0.6376 + 6.3015 + 0.0004, time: 304.157892]
2023-06-06 16:34:33.250: epoch 14:	0.00680191  	0.01666887  	0.01353224  	0.02551503  	0.02741028  
2023-06-06 16:34:33.250: Found a better model.
2023-06-06 16:34:33.250: Save model to file as pretrain.
2023-06-06 16:39:44.588: [iter 15 : loss : 6.8880 = 0.5723 + 6.3151 + 0.0007, time: 307.918047]
2023-06-06 16:39:56.149: epoch 15:	0.00964737  	0.02276943  	0.01865347  	0.03495523  	0.03807961  
2023-06-06 16:39:56.149: Found a better model.
2023-06-06 16:39:56.149: Save model to file as pretrain.
2023-06-06 16:45:04.305: [iter 16 : loss : 6.8160 = 0.4855 + 6.3294 + 0.0011, time: 304.752724]
2023-06-06 16:45:13.222: epoch 16:	0.01199220  	0.02793455  	0.02279591  	0.04198185  	0.04603521  
2023-06-06 16:45:13.222: Found a better model.
2023-06-06 16:45:13.223: Save model to file as pretrain.
2023-06-06 16:50:25.359: [iter 17 : loss : 6.7389 = 0.3976 + 6.3397 + 0.0017, time: 308.686485]
2023-06-06 16:50:37.181: epoch 17:	0.01351935  	0.03156981  	0.02562872  	0.04683659  	0.05131794  
2023-06-06 16:50:37.181: Found a better model.
2023-06-06 16:50:37.182: Save model to file as pretrain.
2023-06-06 16:55:49.464: [iter 18 : loss : 6.6666 = 0.3186 + 6.3458 + 0.0023, time: 308.685220]
2023-06-06 16:56:01.578: epoch 18:	0.01461625  	0.03433798  	0.02783907  	0.05081284  	0.05557958  
2023-06-06 16:56:01.578: Found a better model.
2023-06-06 16:56:01.578: Save model to file as pretrain.
2023-06-06 17:01:17.255: [iter 19 : loss : 6.6039 = 0.2531 + 6.3479 + 0.0029, time: 312.192563]
2023-06-06 17:01:32.344: epoch 19:	0.01533802  	0.03617147  	0.02916945  	0.05274463  	0.05761540  
2023-06-06 17:01:32.345: Found a better model.
2023-06-06 17:01:32.345: Save model to file as pretrain.
2023-06-06 17:06:39.296: [iter 20 : loss : 6.5525 = 0.2015 + 6.3474 + 0.0035, time: 303.573363]
2023-06-06 17:06:49.433: epoch 20:	0.01571887  	0.03719527  	0.02996504  	0.05412500  	0.05927974  
2023-06-06 17:06:49.433: Found a better model.
2023-06-06 17:06:49.433: Save model to file as pretrain.
2023-06-06 17:11:59.494: [iter 21 : loss : 6.5113 = 0.1615 + 6.3457 + 0.0041, time: 306.622862]
2023-06-06 17:12:14.909: epoch 21:	0.01593253  	0.03753859  	0.03023966  	0.05455159  	0.05971827  
2023-06-06 17:12:14.909: Found a better model.
2023-06-06 17:12:14.909: Save model to file as pretrain.
2023-06-06 17:17:23.242: [iter 22 : loss : 6.4799 = 0.1313 + 6.3438 + 0.0047, time: 304.919323]
2023-06-06 17:17:31.515: epoch 22:	0.01605504  	0.03782531  	0.03052219  	0.05536970  	0.06039232  
2023-06-06 17:17:31.515: Found a better model.
2023-06-06 17:17:31.515: Save model to file as pretrain.
2023-06-06 17:22:41.738: [iter 23 : loss : 6.4553 = 0.1083 + 6.3418 + 0.0053, time: 306.716644]
2023-06-06 17:22:54.038: epoch 23:	0.01610254  	0.03797877  	0.03063751  	0.05542792  	0.06057440  
2023-06-06 17:22:54.038: Found a better model.
2023-06-06 17:22:54.038: Save model to file as pretrain.
2023-06-06 17:28:01.371: [iter 24 : loss : 6.4357 = 0.0911 + 6.3388 + 0.0058, time: 304.024973]
2023-06-06 17:28:12.979: epoch 24:	0.01600469  	0.03764093  	0.03042901  	0.05538208  	0.06043066  
2023-06-06 17:33:17.761: [iter 25 : loss : 6.4199 = 0.0774 + 6.3362 + 0.0063, time: 303.766372]
2023-06-06 17:33:26.090: epoch 25:	0.01602843  	0.03770550  	0.03039421  	0.05528374  	0.06027519  
2023-06-06 17:38:30.045: [iter 26 : loss : 6.4084 = 0.0671 + 6.3345 + 0.0068, time: 302.917438]
2023-06-06 17:38:41.578: epoch 26:	0.01593535  	0.03742896  	0.03017276  	0.05510763  	0.06012135  
2023-06-06 17:43:47.551: [iter 27 : loss : 6.3991 = 0.0592 + 6.3327 + 0.0072, time: 304.937458]
2023-06-06 17:43:59.070: epoch 27:	0.01576346  	0.03687448  	0.02984613  	0.05455622  	0.05963276  
2023-06-06 17:49:06.072: [iter 28 : loss : 6.3914 = 0.0528 + 6.3309 + 0.0076, time: 306.003327]
2023-06-06 17:49:14.621: epoch 28:	0.01562954  	0.03658262  	0.02964374  	0.05443265  	0.05936974  
2023-06-06 17:54:21.120: [iter 29 : loss : 6.3849 = 0.0476 + 6.3293 + 0.0080, time: 305.494825]
2023-06-06 17:54:33.156: epoch 29:	0.01560769  	0.03636666  	0.02938505  	0.05373570  	0.05854608  
2023-06-06 17:59:39.556: [iter 30 : loss : 6.3802 = 0.0435 + 6.3284 + 0.0084, time: 305.380185]
2023-06-06 17:59:49.385: epoch 30:	0.01542155  	0.03589622  	0.02903763  	0.05319920  	0.05804580  
2023-06-06 18:04:55.395: [iter 31 : loss : 6.3759 = 0.0399 + 6.3273 + 0.0087, time: 304.967067]
2023-06-06 18:05:07.206: epoch 31:	0.01516892  	0.03518689  	0.02847416  	0.05240868  	0.05703925  
2023-06-06 18:10:15.307: [iter 32 : loss : 6.3722 = 0.0369 + 6.3262 + 0.0090, time: 307.055993]
2023-06-06 18:10:23.629: epoch 32:	0.01497613  	0.03460821  	0.02808711  	0.05188309  	0.05631501  
2023-06-06 18:15:28.930: [iter 33 : loss : 6.3693 = 0.0345 + 6.3254 + 0.0094, time: 304.280274]
2023-06-06 18:15:37.278: epoch 33:	0.01479852  	0.03418299  	0.02771159  	0.05125944  	0.05567195  
2023-06-06 18:20:44.041: [iter 34 : loss : 6.3667 = 0.0324 + 6.3246 + 0.0096, time: 305.723373]
2023-06-06 18:20:55.531: epoch 34:	0.01461144  	0.03364996  	0.02737824  	0.05106247  	0.05536998  
2023-06-06 18:26:02.135: [iter 35 : loss : 6.3646 = 0.0305 + 6.3242 + 0.0099, time: 305.613775]
2023-06-06 18:26:10.475: epoch 35:	0.01443860  	0.03315213  	0.02699765  	0.05056985  	0.05484301  
2023-06-06 18:31:15.505: [iter 36 : loss : 6.3628 = 0.0290 + 6.3236 + 0.0102, time: 304.032372]
2023-06-06 18:31:27.031: epoch 36:	0.01434077  	0.03283130  	0.02669562  	0.05001646  	0.05426445  
2023-06-06 18:36:32.486: [iter 37 : loss : 6.3611 = 0.0278 + 6.3228 + 0.0104, time: 304.455232]
2023-06-06 18:36:44.198: epoch 37:	0.01418216  	0.03231873  	0.02640989  	0.04987693  	0.05407360  
2023-06-06 18:41:51.610: [iter 38 : loss : 6.3595 = 0.0265 + 6.3223 + 0.0106, time: 306.386783]
2023-06-06 18:42:01.564: epoch 38:	0.01398652  	0.03180140  	0.02599750  	0.04927427  	0.05331734  
2023-06-06 18:47:09.039: [iter 39 : loss : 6.3576 = 0.0251 + 6.3216 + 0.0109, time: 306.443986]
2023-06-06 18:47:17.377: epoch 39:	0.01381556  	0.03130988  	0.02576524  	0.04913954  	0.05330782  
2023-06-06 18:52:21.075: [iter 40 : loss : 6.3568 = 0.0244 + 6.3213 + 0.0111, time: 302.704182]
2023-06-06 18:52:29.403: epoch 40:	0.01370255  	0.03103160  	0.02545166  	0.04850865  	0.05258981  
2023-06-06 18:57:35.062: [iter 41 : loss : 6.3559 = 0.0235 + 6.3211 + 0.0113, time: 304.649932]
2023-06-06 18:57:47.008: epoch 41:	0.01363512  	0.03078890  	0.02534948  	0.04841452  	0.05258922  
2023-06-06 19:03:18.947: [iter 42 : loss : 6.3549 = 0.0228 + 6.3206 + 0.0115, time: 330.902964]
2023-06-06 19:03:36.177: epoch 42:	0.01354204  	0.03042877  	0.02510241  	0.04825429  	0.05224394  
2023-06-06 19:10:13.021: [iter 43 : loss : 6.3542 = 0.0221 + 6.3205 + 0.0116, time: 395.716153]
2023-06-06 19:10:28.820: epoch 43:	0.01336729  	0.02994743  	0.02470118  	0.04762585  	0.05157172  
2023-06-06 19:16:52.757: [iter 44 : loss : 6.3534 = 0.0215 + 6.3201 + 0.0118, time: 382.802634]
2023-06-06 19:17:09.446: epoch 44:	0.01319064  	0.02944210  	0.02435735  	0.04717536  	0.05107637  
2023-06-06 19:23:28.939: [iter 45 : loss : 6.3522 = 0.0208 + 6.3194 + 0.0120, time: 378.350852]
2023-06-06 19:23:42.573: epoch 45:	0.01306052  	0.02900722  	0.02409752  	0.04688388  	0.05082263  
2023-06-06 19:30:02.164: [iter 46 : loss : 6.3515 = 0.0204 + 6.3190 + 0.0121, time: 378.472106]
2023-06-06 19:30:17.577: epoch 46:	0.01296271  	0.02880214  	0.02388375  	0.04644103  	0.05030853  
2023-06-06 19:36:39.040: [iter 47 : loss : 6.3508 = 0.0199 + 6.3186 + 0.0123, time: 380.285423]
2023-06-06 19:36:52.879: epoch 47:	0.01284684  	0.02842256  	0.02369410  	0.04633810  	0.05032109  
2023-06-06 19:43:16.072: [iter 48 : loss : 6.3509 = 0.0196 + 6.3189 + 0.0124, time: 382.016813]
2023-06-06 19:43:29.871: epoch 48:	0.01271767  	0.02810497  	0.02345478  	0.04607016  	0.04986391  
2023-06-06 19:49:52.147: [iter 49 : loss : 6.3504 = 0.0191 + 6.3186 + 0.0126, time: 381.159163]
2023-06-06 19:50:05.685: epoch 49:	0.01262650  	0.02783443  	0.02321280  	0.04568746  	0.04944002  
2023-06-06 19:56:26.816: [iter 50 : loss : 6.3494 = 0.0187 + 6.3180 + 0.0127, time: 379.978486]
2023-06-06 19:56:43.952: epoch 50:	0.01260086  	0.02769280  	0.02311293  	0.04548712  	0.04918073  
2023-06-06 20:03:05.279: [iter 51 : loss : 6.3493 = 0.0185 + 6.3180 + 0.0128, time: 380.183218]
2023-06-06 20:03:19.074: epoch 51:	0.01243182  	0.02732821  	0.02286545  	0.04521006  	0.04891032  
2023-06-06 20:09:38.721: [iter 52 : loss : 6.3489 = 0.0180 + 6.3179 + 0.0129, time: 378.496533]
2023-06-06 20:09:52.678: epoch 52:	0.01236248  	0.02705350  	0.02268171  	0.04494888  	0.04855317  
2023-06-06 20:16:15.049: [iter 53 : loss : 6.3485 = 0.0179 + 6.3175 + 0.0131, time: 381.196972]
2023-06-06 20:16:32.006: epoch 53:	0.01232164  	0.02691799  	0.02263401  	0.04496216  	0.04869034  
2023-06-06 20:22:51.575: [iter 54 : loss : 6.3479 = 0.0175 + 6.3173 + 0.0132, time: 378.413449]
2023-06-06 20:23:05.703: epoch 54:	0.01218490  	0.02649668  	0.02233490  	0.04443002  	0.04807049  
2023-06-06 20:29:27.433: [iter 55 : loss : 6.3479 = 0.0173 + 6.3174 + 0.0133, time: 380.564732]
2023-06-06 20:29:43.046: epoch 55:	0.01210512  	0.02626464  	0.02215974  	0.04418194  	0.04785001  
2023-06-06 20:36:06.151: [iter 56 : loss : 6.3475 = 0.0171 + 6.3170 + 0.0134, time: 381.969845]
2023-06-06 20:36:22.515: epoch 56:	0.01203390  	0.02613574  	0.02212126  	0.04409180  	0.04797686  
2023-06-06 20:42:45.332: [iter 57 : loss : 6.3474 = 0.0168 + 6.3171 + 0.0135, time: 381.692003]
2023-06-06 20:42:59.699: epoch 57:	0.01198451  	0.02596430  	0.02200407  	0.04379683  	0.04773226  
2023-06-06 20:49:22.335: [iter 58 : loss : 6.3466 = 0.0167 + 6.3164 + 0.0136, time: 381.437116]
2023-06-06 20:49:37.681: epoch 58:	0.01197785  	0.02593859  	0.02198361  	0.04380708  	0.04781210  
2023-06-06 20:55:59.266: [iter 59 : loss : 6.3463 = 0.0163 + 6.3163 + 0.0137, time: 380.433441]
2023-06-06 20:56:14.781: epoch 59:	0.01190187  	0.02574458  	0.02184207  	0.04363229  	0.04749954  
2023-06-06 21:02:36.044: [iter 60 : loss : 6.3462 = 0.0161 + 6.3163 + 0.0138, time: 380.119058]
2023-06-06 21:02:50.136: epoch 60:	0.01179077  	0.02549921  	0.02163956  	0.04332384  	0.04707962  
2023-06-06 21:09:13.891: [iter 61 : loss : 6.3460 = 0.0161 + 6.3161 + 0.0138, time: 382.603515]
2023-06-06 21:09:29.789: epoch 61:	0.01171670  	0.02525759  	0.02146472  	0.04313405  	0.04674976  
2023-06-06 21:15:51.695: [iter 62 : loss : 6.3462 = 0.0160 + 6.3163 + 0.0139, time: 380.713527]
2023-06-06 21:16:07.639: epoch 62:	0.01165402  	0.02509628  	0.02127462  	0.04253491  	0.04608339  
2023-06-06 21:22:29.993: [iter 63 : loss : 6.3454 = 0.0158 + 6.3156 + 0.0140, time: 381.227687]
2023-06-06 21:22:45.469: epoch 63:	0.01158564  	0.02485086  	0.02112452  	0.04229245  	0.04591370  
2023-06-06 21:29:07.698: [iter 64 : loss : 6.3457 = 0.0155 + 6.3160 + 0.0141, time: 381.081090]
2023-06-06 21:29:21.648: epoch 64:	0.01152581  	0.02474689  	0.02102748  	0.04211914  	0.04572208  
2023-06-06 21:35:52.303: [iter 65 : loss : 6.3451 = 0.0154 + 6.3156 + 0.0142, time: 389.500830]
2023-06-06 21:36:08.264: epoch 65:	0.01142419  	0.02441925  	0.02080430  	0.04187836  	0.04537430  
2023-06-06 21:42:40.025: [iter 66 : loss : 6.3450 = 0.0152 + 6.3156 + 0.0142, time: 390.535649]
2023-06-06 21:42:55.279: epoch 66:	0.01133966  	0.02428911  	0.02060788  	0.04144032  	0.04485668  
2023-06-06 21:48:46.694: [iter 67 : loss : 6.3444 = 0.0151 + 6.3150 + 0.0143, time: 350.288696]
2023-06-06 21:48:55.176: epoch 67:	0.01129882  	0.02417652  	0.02057748  	0.04143780  	0.04489027  
2023-06-06 21:54:00.267: [iter 68 : loss : 6.3446 = 0.0151 + 6.3152 + 0.0144, time: 304.068001]
2023-06-06 21:54:11.807: epoch 68:	0.01126749  	0.02405098  	0.02050976  	0.04118558  	0.04480662  
2023-06-06 21:59:14.431: [iter 69 : loss : 6.3444 = 0.0147 + 6.3153 + 0.0144, time: 301.579755]
2023-06-06 21:59:23.100: epoch 69:	0.01124089  	0.02392976  	0.02042931  	0.04110278  	0.04456555  
2023-06-06 22:04:30.263: [iter 70 : loss : 6.3444 = 0.0147 + 6.3152 + 0.0145, time: 306.117950]
2023-06-06 22:04:42.982: epoch 70:	0.01121335  	0.02384410  	0.02035582  	0.04096705  	0.04436140  
2023-06-06 22:09:49.145: [iter 71 : loss : 6.3439 = 0.0147 + 6.3147 + 0.0145, time: 305.135241]
2023-06-06 22:09:58.610: epoch 71:	0.01114022  	0.02372267  	0.02023397  	0.04082314  	0.04418099  
2023-06-06 22:15:08.779: [iter 72 : loss : 6.3440 = 0.0146 + 6.3148 + 0.0146, time: 309.078262]
2023-06-06 22:15:21.806: epoch 72:	0.01115446  	0.02369709  	0.02026204  	0.04093745  	0.04443908  
2023-06-06 22:20:34.144: [iter 73 : loss : 6.3439 = 0.0144 + 6.3148 + 0.0147, time: 311.291230]
2023-06-06 22:20:45.583: epoch 73:	0.01107184  	0.02350589  	0.02011810  	0.04071461  	0.04416841  
2023-06-06 22:20:45.584: Early stopping is triggered at epoch: 73
2023-06-06 22:20:45.584: best_result@epoch 23:

2023-06-06 22:20:45.584: Loading from the saved model.
2023-06-06 22:20:57.479: 		0.01610254  	0.03797877  	0.03063751  	0.05542792  	0.06057440  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
