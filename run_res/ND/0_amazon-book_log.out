seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-06 01:35:39.273: Dataset name: amazon-book
The number of users: 52643
The number of items: 91599
The number of ratings: 2984108
Average actions of users: 56.69
Average actions of items: 32.58
The sparsity of the dataset: 99.938115%
2023-06-06 01:35:39.273: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=amazon-book
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=0
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=0
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using default loss
2023-06-06 01:35:45.505: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-06 01:36:00.834: 		0.00015863  	0.00025811  	0.00023695  	0.00066210  	0.00066549  
2023-06-06 01:41:16.379: [iter 1 : loss : 7.0243 = 0.6931 + 6.3312 + 0.0000, time: 314.429629]
2023-06-06 01:41:31.025: epoch 1:	0.00036380  	0.00066310  	0.00061362  	0.00145587  	0.00151724  
2023-06-06 01:41:31.025: Found a better model.
2023-06-06 01:41:31.025: Save model to file as pretrain.
2023-06-06 01:46:41.836: [iter 2 : loss : 7.0133 = 0.6930 + 6.3203 + 0.0000, time: 307.235715]
2023-06-06 01:46:53.609: epoch 2:	0.00042649  	0.00077626  	0.00071245  	0.00185267  	0.00187778  
2023-06-06 01:46:53.609: Found a better model.
2023-06-06 01:46:53.609: Save model to file as pretrain.
2023-06-06 01:52:09.699: [iter 3 : loss : 7.0115 = 0.6928 + 6.3186 + 0.0000, time: 312.661315]
2023-06-06 01:52:22.886: epoch 3:	0.00043789  	0.00080168  	0.00069494  	0.00171273  	0.00177446  
2023-06-06 01:52:22.886: Found a better model.
2023-06-06 01:52:22.886: Save model to file as pretrain.
2023-06-06 01:57:42.194: [iter 4 : loss : 7.0103 = 0.6926 + 6.3177 + 0.0000, time: 315.890207]
2023-06-06 01:57:55.464: epoch 4:	0.00044169  	0.00084769  	0.00071370  	0.00168717  	0.00172525  
2023-06-06 01:57:55.465: Found a better model.
2023-06-06 01:57:55.465: Save model to file as pretrain.
2023-06-06 02:03:10.929: [iter 5 : loss : 7.0093 = 0.6924 + 6.3169 + 0.0000, time: 311.830944]
2023-06-06 02:03:25.277: epoch 5:	0.00046638  	0.00085698  	0.00076826  	0.00183467  	0.00193249  
2023-06-06 02:03:25.277: Found a better model.
2023-06-06 02:03:25.277: Save model to file as pretrain.
2023-06-06 02:08:45.819: [iter 6 : loss : 7.0089 = 0.6920 + 6.3169 + 0.0000, time: 317.052769]
2023-06-06 02:08:59.138: epoch 6:	0.00056992  	0.00104782  	0.00086287  	0.00190423  	0.00196342  
2023-06-06 02:08:59.138: Found a better model.
2023-06-06 02:08:59.138: Save model to file as pretrain.
2023-06-06 02:14:17.086: [iter 7 : loss : 7.0084 = 0.6915 + 6.3168 + 0.0000, time: 314.444956]
2023-06-06 02:14:30.809: epoch 7:	0.00057182  	0.00111891  	0.00094583  	0.00210761  	0.00215905  
2023-06-06 02:14:30.809: Found a better model.
2023-06-06 02:14:30.809: Save model to file as pretrain.
2023-06-06 02:19:42.349: [iter 8 : loss : 7.0080 = 0.6908 + 6.3172 + 0.0000, time: 308.133081]
2023-06-06 02:19:54.116: epoch 8:	0.00060981  	0.00122436  	0.00102917  	0.00237450  	0.00245754  
2023-06-06 02:19:54.117: Found a better model.
2023-06-06 02:19:54.117: Save model to file as pretrain.
2023-06-06 02:25:10.950: [iter 9 : loss : 7.0071 = 0.6898 + 6.3172 + 0.0001, time: 313.322981]
2023-06-06 02:25:23.401: epoch 9:	0.00070575  	0.00149960  	0.00122814  	0.00272853  	0.00282529  
2023-06-06 02:25:23.401: Found a better model.
2023-06-06 02:25:23.401: Save model to file as pretrain.
2023-06-06 02:30:36.582: [iter 10 : loss : 7.0062 = 0.6882 + 6.3179 + 0.0001, time: 309.809523]
2023-06-06 02:30:49.923: epoch 10:	0.00076939  	0.00169996  	0.00133063  	0.00273285  	0.00283403  
2023-06-06 02:30:49.923: Found a better model.
2023-06-06 02:30:49.923: Save model to file as pretrain.
2023-06-06 02:36:08.770: [iter 11 : loss : 7.0044 = 0.6854 + 6.3188 + 0.0001, time: 315.420928]
2023-06-06 02:36:21.869: epoch 11:	0.00098500  	0.00230888  	0.00179408  	0.00368102  	0.00380785  
2023-06-06 02:36:21.869: Found a better model.
2023-06-06 02:36:21.869: Save model to file as pretrain.
2023-06-06 02:41:38.365: [iter 12 : loss : 7.0001 = 0.6800 + 6.3200 + 0.0001, time: 312.777363]
2023-06-06 02:41:53.085: epoch 12:	0.00174017  	0.00435261  	0.00343244  	0.00687721  	0.00720965  
2023-06-06 02:41:53.085: Found a better model.
2023-06-06 02:41:53.085: Save model to file as pretrain.
2023-06-06 02:47:08.926: [iter 13 : loss : 6.9905 = 0.6668 + 6.3235 + 0.0002, time: 312.414068]
2023-06-06 02:47:22.400: epoch 13:	0.00422416  	0.01070932  	0.00851401  	0.01607289  	0.01723257  
2023-06-06 02:47:22.400: Found a better model.
2023-06-06 02:47:22.400: Save model to file as pretrain.
2023-06-06 02:52:38.413: [iter 14 : loss : 6.9628 = 0.6301 + 6.3323 + 0.0004, time: 312.453232]
2023-06-06 02:52:50.024: epoch 14:	0.00751232  	0.01813564  	0.01477094  	0.02810321  	0.03016881  
2023-06-06 02:52:50.024: Found a better model.
2023-06-06 02:52:50.024: Save model to file as pretrain.
2023-06-06 02:58:06.164: [iter 15 : loss : 6.9056 = 0.5582 + 6.3467 + 0.0007, time: 312.604448]
2023-06-06 02:58:18.817: epoch 15:	0.01027606  	0.02400552  	0.01979147  	0.03730578  	0.04066130  
2023-06-06 02:58:18.817: Found a better model.
2023-06-06 02:58:18.817: Save model to file as pretrain.
2023-06-06 03:03:32.043: [iter 16 : loss : 6.8304 = 0.4688 + 6.3603 + 0.0012, time: 309.802296]
2023-06-06 03:03:43.905: epoch 16:	0.01246705  	0.02900431  	0.02387454  	0.04431269  	0.04855614  
2023-06-06 03:03:43.905: Found a better model.
2023-06-06 03:03:43.905: Save model to file as pretrain.
2023-06-06 03:08:59.122: [iter 17 : loss : 6.7520 = 0.3806 + 6.3696 + 0.0018, time: 311.667054]
2023-06-06 03:09:10.897: epoch 17:	0.01406733  	0.03292774  	0.02692725  	0.04911550  	0.05389573  
2023-06-06 03:09:10.897: Found a better model.
2023-06-06 03:09:10.897: Save model to file as pretrain.
2023-06-06 03:14:27.080: [iter 18 : loss : 6.6803 = 0.3030 + 6.3749 + 0.0024, time: 312.799212]
2023-06-06 03:14:40.651: epoch 18:	0.01514144  	0.03557773  	0.02884516  	0.05187476  	0.05708541  
2023-06-06 03:14:40.651: Found a better model.
2023-06-06 03:14:40.651: Save model to file as pretrain.
2023-06-06 03:19:55.531: [iter 19 : loss : 6.6191 = 0.2398 + 6.3762 + 0.0030, time: 311.320295]
2023-06-06 03:20:07.736: epoch 19:	0.01572550  	0.03702999  	0.02987180  	0.05337130  	0.05851510  
2023-06-06 03:20:07.736: Found a better model.
2023-06-06 03:20:07.736: Save model to file as pretrain.
2023-06-06 03:25:22.075: [iter 20 : loss : 6.5696 = 0.1905 + 6.3755 + 0.0037, time: 310.915542]
2023-06-06 03:25:35.676: epoch 20:	0.01610635  	0.03806815  	0.03056571  	0.05446418  	0.05967389  
2023-06-06 03:25:35.676: Found a better model.
2023-06-06 03:25:35.676: Save model to file as pretrain.
2023-06-06 03:30:53.405: [iter 21 : loss : 6.5303 = 0.1525 + 6.3736 + 0.0043, time: 314.397753]
2023-06-06 03:31:05.835: epoch 21:	0.01635989  	0.03884845  	0.03102254  	0.05515260  	0.06050722  
2023-06-06 03:31:05.835: Found a better model.
2023-06-06 03:31:05.835: Save model to file as pretrain.
2023-06-06 03:36:18.683: [iter 22 : loss : 6.5005 = 0.1241 + 6.3715 + 0.0048, time: 309.314017]
2023-06-06 03:36:32.104: epoch 22:	0.01647290  	0.03901256  	0.03109829  	0.05535132  	0.06069576  
2023-06-06 03:36:32.104: Found a better model.
2023-06-06 03:36:32.104: Save model to file as pretrain.
2023-06-06 03:41:49.125: [iter 23 : loss : 6.4774 = 0.1026 + 6.3694 + 0.0054, time: 313.631352]
2023-06-06 03:42:02.431: epoch 23:	0.01640071  	0.03885309  	0.03110787  	0.05558145  	0.06083325  
2023-06-06 03:47:17.078: [iter 24 : loss : 6.4588 = 0.0865 + 6.3664 + 0.0059, time: 313.550329]
2023-06-06 03:47:31.840: epoch 24:	0.01628103  	0.03852946  	0.03086511  	0.05530900  	0.06043165  
2023-06-06 03:52:44.872: [iter 25 : loss : 6.4439 = 0.0736 + 6.3639 + 0.0064, time: 311.953344]
2023-06-06 03:52:58.150: epoch 25:	0.01621456  	0.03829331  	0.03071896  	0.05519053  	0.06021625  
2023-06-06 03:58:12.777: [iter 26 : loss : 6.4331 = 0.0640 + 6.3622 + 0.0069, time: 313.541341]
2023-06-06 03:58:26.106: epoch 26:	0.01607780  	0.03785014  	0.03058662  	0.05540524  	0.06065872  
2023-06-06 04:03:37.692: [iter 27 : loss : 6.4245 = 0.0568 + 6.3604 + 0.0073, time: 310.513840]
2023-06-06 04:03:48.208: epoch 27:	0.01601606  	0.03757076  	0.03040349  	0.05516634  	0.06045859  
2023-06-06 04:09:01.975: [iter 28 : loss : 6.4172 = 0.0507 + 6.3588 + 0.0077, time: 312.693073]
2023-06-06 04:09:14.091: epoch 28:	0.01592395  	0.03731597  	0.03017312  	0.05501950  	0.06004524  
2023-06-06 04:14:27.737: [iter 29 : loss : 6.4110 = 0.0458 + 6.3572 + 0.0081, time: 312.569288]
2023-06-06 04:14:42.529: epoch 29:	0.01577959  	0.03688706  	0.02992531  	0.05498728  	0.05982405  
2023-06-06 04:19:57.400: [iter 30 : loss : 6.4066 = 0.0419 + 6.3563 + 0.0084, time: 313.795665]
2023-06-06 04:20:10.733: epoch 30:	0.01564376  	0.03642858  	0.02956111  	0.05434377  	0.05907180  
2023-06-06 04:25:25.829: [iter 31 : loss : 6.4026 = 0.0386 + 6.3552 + 0.0088, time: 314.024294]
2023-06-06 04:25:37.609: epoch 31:	0.01543387  	0.03591396  	0.02917101  	0.05388886  	0.05854925  
2023-06-06 04:30:52.474: [iter 32 : loss : 6.3989 = 0.0356 + 6.3542 + 0.0091, time: 313.766778]
2023-06-06 04:31:05.798: epoch 32:	0.01535599  	0.03557115  	0.02884700  	0.05302928  	0.05781387  
2023-06-06 04:36:18.692: [iter 33 : loss : 6.3963 = 0.0335 + 6.3534 + 0.0094, time: 311.842910]
2023-06-06 04:36:31.305: epoch 33:	0.01518218  	0.03508244  	0.02849227  	0.05272720  	0.05740361  
2023-06-06 04:41:43.584: [iter 34 : loss : 6.3937 = 0.0314 + 6.3526 + 0.0097, time: 311.213911]
2023-06-06 04:41:56.741: epoch 34:	0.01496091  	0.03441268  	0.02802308  	0.05202442  	0.05652951  
2023-06-06 04:47:12.009: [iter 35 : loss : 6.3919 = 0.0297 + 6.3522 + 0.0099, time: 314.184878]
2023-06-06 04:47:23.787: epoch 35:	0.01477477  	0.03389343  	0.02762366  	0.05138706  	0.05591303  
2023-06-06 04:52:38.950: [iter 36 : loss : 6.3900 = 0.0281 + 6.3517 + 0.0102, time: 314.050097]
2023-06-06 04:52:53.749: epoch 36:	0.01462946  	0.03346443  	0.02734767  	0.05123141  	0.05584339  
2023-06-06 04:58:08.056: [iter 37 : loss : 6.3883 = 0.0269 + 6.3509 + 0.0104, time: 313.239808]
2023-06-06 04:58:21.363: epoch 37:	0.01445945  	0.03296166  	0.02694769  	0.05063138  	0.05490819  
2023-06-06 05:03:36.053: [iter 38 : loss : 6.3869 = 0.0259 + 6.3504 + 0.0107, time: 313.633867]
2023-06-06 05:03:50.671: epoch 38:	0.01428470  	0.03249876  	0.02668581  	0.05041499  	0.05467419  
2023-06-06 05:09:05.664: [iter 39 : loss : 6.3851 = 0.0245 + 6.3497 + 0.0109, time: 313.903470]
2023-06-06 05:09:18.393: epoch 39:	0.01417927  	0.03219561  	0.02652758  	0.05028544  	0.05478045  
2023-06-06 05:14:31.066: [iter 40 : loss : 6.3843 = 0.0238 + 6.3495 + 0.0111, time: 311.619134]
2023-06-06 05:14:44.028: epoch 40:	0.01402446  	0.03180286  	0.02620868  	0.04975377  	0.05429985  
2023-06-06 05:19:56.634: [iter 41 : loss : 6.3834 = 0.0228 + 6.3493 + 0.0113, time: 311.552257]
2023-06-06 05:20:10.203: epoch 41:	0.01387917  	0.03130613  	0.02601108  	0.04982363  	0.05433220  
2023-06-06 05:25:24.232: [iter 42 : loss : 6.3826 = 0.0223 + 6.3488 + 0.0115, time: 312.952124]
2023-06-06 05:25:37.783: epoch 42:	0.01376806  	0.03097815  	0.02572902  	0.04938045  	0.05370562  
2023-06-06 05:30:53.439: [iter 43 : loss : 6.3818 = 0.0215 + 6.3486 + 0.0117, time: 314.593788]
2023-06-06 05:31:06.788: epoch 43:	0.01367310  	0.03073220  	0.02551541  	0.04902656  	0.05337773  
2023-06-06 05:36:21.043: [iter 44 : loss : 6.3810 = 0.0209 + 6.3482 + 0.0118, time: 313.190539]
2023-06-06 05:36:32.761: epoch 44:	0.01352303  	0.03027358  	0.02520242  	0.04867164  	0.05285085  
2023-06-06 05:41:46.162: [iter 45 : loss : 6.3799 = 0.0203 + 6.3475 + 0.0120, time: 312.320503]
2023-06-06 05:41:59.477: epoch 45:	0.01344609  	0.02993569  	0.02499269  	0.04845681  	0.05262987  
2023-06-06 05:47:11.581: [iter 46 : loss : 6.3793 = 0.0199 + 6.3472 + 0.0121, time: 311.007618]
2023-06-06 05:47:24.956: epoch 46:	0.01326564  	0.02942454  	0.02464434  	0.04790160  	0.05199330  
2023-06-06 05:52:38.850: [iter 47 : loss : 6.3787 = 0.0195 + 6.3469 + 0.0123, time: 312.818976]
2023-06-06 05:52:50.649: epoch 47:	0.01323810  	0.02928033  	0.02450156  	0.04748552  	0.05160927  
2023-06-06 05:58:04.951: [iter 48 : loss : 6.3786 = 0.0191 + 6.3471 + 0.0124, time: 313.211123]
2023-06-06 05:58:18.386: epoch 48:	0.01313650  	0.02892396  	0.02427856  	0.04734330  	0.05136646  
2023-06-06 06:03:33.635: [iter 49 : loss : 6.3781 = 0.0187 + 6.3469 + 0.0126, time: 314.132722]
2023-06-06 06:03:45.417: epoch 49:	0.01299118  	0.02861119  	0.02404493  	0.04700240  	0.05093306  
2023-06-06 06:09:01.139: [iter 50 : loss : 6.3773 = 0.0184 + 6.3463 + 0.0127, time: 314.635565]
2023-06-06 06:09:14.437: epoch 50:	0.01294750  	0.02849368  	0.02389536  	0.04667137  	0.05048732  
2023-06-06 06:14:26.450: [iter 51 : loss : 6.3770 = 0.0179 + 6.3463 + 0.0128, time: 310.937160]
2023-06-06 06:14:38.494: epoch 51:	0.01278986  	0.02796061  	0.02357099  	0.04618835  	0.05005851  
2023-06-06 06:19:52.063: [iter 52 : loss : 6.3767 = 0.0176 + 6.3461 + 0.0129, time: 312.493865]
2023-06-06 06:20:05.479: epoch 52:	0.01272338  	0.02778845  	0.02350527  	0.04622808  	0.05009844  
2023-06-06 06:25:17.317: [iter 53 : loss : 6.3763 = 0.0176 + 6.3457 + 0.0130, time: 310.784832]
2023-06-06 06:25:29.094: epoch 53:	0.01269867  	0.02768969  	0.02339179  	0.04595836  	0.04982162  
2023-06-06 06:30:44.378: [iter 54 : loss : 6.3758 = 0.0171 + 6.3455 + 0.0131, time: 314.181328]
2023-06-06 06:30:59.055: epoch 54:	0.01258280  	0.02739699  	0.02321032  	0.04576826  	0.04962417  
2023-06-06 06:36:11.469: [iter 55 : loss : 6.3756 = 0.0168 + 6.3456 + 0.0133, time: 311.304256]
2023-06-06 06:36:24.853: epoch 55:	0.01258375  	0.02737493  	0.02315634  	0.04563758  	0.04946478  
2023-06-06 06:41:36.878: [iter 56 : loss : 6.3754 = 0.0167 + 6.3453 + 0.0134, time: 310.951640]
2023-06-06 06:41:49.626: epoch 56:	0.01256189  	0.02733824  	0.02305415  	0.04546826  	0.04924669  
2023-06-06 06:47:01.993: [iter 57 : loss : 6.3750 = 0.0163 + 6.3452 + 0.0135, time: 311.278656]
2023-06-06 06:47:16.406: epoch 57:	0.01247263  	0.02701861  	0.02289506  	0.04524661  	0.04899825  
2023-06-06 06:52:29.395: [iter 58 : loss : 6.3744 = 0.0162 + 6.3446 + 0.0135, time: 311.897286]
2023-06-06 06:52:41.357: epoch 58:	0.01240045  	0.02692929  	0.02272730  	0.04488403  	0.04855468  
2023-06-06 06:57:56.322: [iter 59 : loss : 6.3741 = 0.0159 + 6.3446 + 0.0136, time: 313.856507]
2023-06-06 06:58:11.044: epoch 59:	0.01233683  	0.02678652  	0.02260580  	0.04456129  	0.04840874  
2023-06-06 07:03:24.098: [iter 60 : loss : 6.3742 = 0.0158 + 6.3446 + 0.0137, time: 311.948676]
2023-06-06 07:03:37.635: epoch 60:	0.01221906  	0.02646361  	0.02237076  	0.04409948  	0.04785789  
2023-06-06 07:08:51.504: [iter 61 : loss : 6.3738 = 0.0156 + 6.3444 + 0.0138, time: 312.808873]
2023-06-06 07:09:04.761: epoch 61:	0.01219152  	0.02640913  	0.02228971  	0.04395415  	0.04771987  
2023-06-06 07:14:18.170: [iter 62 : loss : 6.3741 = 0.0157 + 6.3446 + 0.0139, time: 312.329676]
2023-06-06 07:14:30.163: epoch 62:	0.01218108  	0.02635335  	0.02219492  	0.04360031  	0.04733521  
2023-06-06 07:19:44.394: [iter 63 : loss : 6.3732 = 0.0154 + 6.3438 + 0.0140, time: 313.155395]
2023-06-06 07:19:58.960: epoch 63:	0.01212222  	0.02620015  	0.02210416  	0.04339148  	0.04724673  
2023-06-06 07:25:13.426: [iter 64 : loss : 6.3735 = 0.0152 + 6.3443 + 0.0140, time: 313.380096]
2023-06-06 07:25:26.714: epoch 64:	0.01205669  	0.02601025  	0.02197840  	0.04326388  	0.04700774  
2023-06-06 07:30:41.164: [iter 65 : loss : 6.3731 = 0.0150 + 6.3439 + 0.0141, time: 313.385147]
2023-06-06 07:30:54.093: epoch 65:	0.01194177  	0.02574491  	0.02177193  	0.04304265  	0.04676832  
2023-06-06 07:36:10.500: [iter 66 : loss : 6.3729 = 0.0149 + 6.3438 + 0.0142, time: 315.353400]
2023-06-06 07:36:25.533: epoch 66:	0.01182020  	0.02547183  	0.02155288  	0.04273734  	0.04643120  
2023-06-06 07:41:35.042: [iter 67 : loss : 6.3723 = 0.0147 + 6.3433 + 0.0143, time: 308.431889]
2023-06-06 07:41:48.264: epoch 67:	0.01180406  	0.02537268  	0.02151159  	0.04279995  	0.04645748  
2023-06-06 07:47:03.939: [iter 68 : loss : 6.3727 = 0.0149 + 6.3434 + 0.0143, time: 314.601265]
2023-06-06 07:47:15.775: epoch 68:	0.01175467  	0.02517992  	0.02138120  	0.04266428  	0.04631576  
2023-06-06 07:52:28.731: [iter 69 : loss : 6.3723 = 0.0144 + 6.3435 + 0.0144, time: 311.856555]
2023-06-06 07:52:42.108: epoch 69:	0.01171194  	0.02513590  	0.02128523  	0.04238267  	0.04603054  
2023-06-06 07:57:55.755: [iter 70 : loss : 6.3724 = 0.0145 + 6.3435 + 0.0144, time: 312.567120]
2023-06-06 07:58:08.885: epoch 70:	0.01169389  	0.02506143  	0.02121904  	0.04233399  	0.04593572  
2023-06-06 08:03:24.390: [iter 71 : loss : 6.3719 = 0.0143 + 6.3431 + 0.0145, time: 314.418712]
2023-06-06 08:03:35.882: epoch 71:	0.01157613  	0.02484010  	0.02109642  	0.04221036  	0.04582236  
2023-06-06 08:08:50.799: [iter 72 : loss : 6.3719 = 0.0143 + 6.3431 + 0.0146, time: 313.815792]
2023-06-06 08:09:02.580: epoch 72:	0.01158181  	0.02478478  	0.02106010  	0.04222667  	0.04578513  
2023-06-06 08:09:02.580: Early stopping is triggered at epoch: 72
2023-06-06 08:09:02.580: best_result@epoch 22:

2023-06-06 08:09:02.580: Loading from the saved model.
2023-06-06 08:09:14.604: 		0.01647290  	0.03901256  	0.03109829  	0.05535132  	0.06069576  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
