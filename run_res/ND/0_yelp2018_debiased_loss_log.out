seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-05 11:28:43.350: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-05 11:28:43.350: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=0
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=2
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using debiased loss
2023-06-05 11:28:47.092: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-05 11:28:50.931: 		0.00032525  	0.00065866  	0.00049299  	0.00109993  	0.00110557  
2023-06-05 11:30:01.804: [iter 1 : loss : 1.8063 = 0.6931 + 1.1132 + 0.0000, time: 70.324254]
2023-06-05 11:30:05.863: epoch 1:	0.00060313  	0.00127874  	0.00100926  	0.00235956  	0.00237106  
2023-06-05 11:30:05.864: Found a better model.
2023-06-05 11:30:05.864: Save model to file as pretrain.
2023-06-05 11:31:22.860: [iter 2 : loss : 1.8038 = 0.6930 + 1.1107 + 0.0000, time: 75.577537]
2023-06-05 11:31:26.813: epoch 2:	0.00079575  	0.00162103  	0.00131466  	0.00323242  	0.00325209  
2023-06-05 11:31:26.813: Found a better model.
2023-06-05 11:31:26.813: Save model to file as pretrain.
2023-06-05 11:32:52.894: [iter 3 : loss : 1.8033 = 0.6929 + 1.1103 + 0.0000, time: 85.217763]
2023-06-05 11:32:56.857: epoch 3:	0.00095364  	0.00199456  	0.00158350  	0.00371809  	0.00375816  
2023-06-05 11:32:56.858: Found a better model.
2023-06-05 11:32:56.858: Save model to file as pretrain.
2023-06-05 11:34:22.324: [iter 4 : loss : 1.8029 = 0.6928 + 1.1101 + 0.0000, time: 84.569678]
2023-06-05 11:34:26.463: epoch 4:	0.00097890  	0.00205837  	0.00163646  	0.00379247  	0.00385457  
2023-06-05 11:34:26.463: Found a better model.
2023-06-05 11:34:26.463: Save model to file as pretrain.
2023-06-05 11:35:51.313: [iter 5 : loss : 1.8027 = 0.6927 + 1.1100 + 0.0000, time: 83.987205]
2023-06-05 11:35:55.347: epoch 5:	0.00111626  	0.00230768  	0.00183361  	0.00429230  	0.00435481  
2023-06-05 11:35:55.347: Found a better model.
2023-06-05 11:35:55.347: Save model to file as pretrain.
2023-06-05 11:37:14.334: [iter 6 : loss : 1.8024 = 0.6924 + 1.1099 + 0.0000, time: 78.081469]
2023-06-05 11:37:18.476: epoch 6:	0.00116994  	0.00243530  	0.00190991  	0.00431538  	0.00434920  
2023-06-05 11:37:18.476: Found a better model.
2023-06-05 11:37:18.476: Save model to file as pretrain.
2023-06-05 11:38:43.889: [iter 7 : loss : 1.8020 = 0.6921 + 1.1099 + 0.0000, time: 84.494368]
2023-06-05 11:38:47.723: epoch 7:	0.00126152  	0.00259345  	0.00216431  	0.00512189  	0.00521982  
2023-06-05 11:38:47.724: Found a better model.
2023-06-05 11:38:47.724: Save model to file as pretrain.
2023-06-05 11:40:13.631: [iter 8 : loss : 1.8015 = 0.6915 + 1.1100 + 0.0000, time: 84.990851]
2023-06-05 11:40:18.024: epoch 8:	0.00162939  	0.00353770  	0.00287791  	0.00669160  	0.00685425  
2023-06-05 11:40:18.024: Found a better model.
2023-06-05 11:40:18.024: Save model to file as pretrain.
2023-06-05 11:41:29.574: [iter 9 : loss : 1.8007 = 0.6903 + 1.1103 + 0.0000, time: 70.687171]
2023-06-05 11:41:33.474: epoch 9:	0.00334569  	0.00768447  	0.00628707  	0.01397833  	0.01452720  
2023-06-05 11:41:33.474: Found a better model.
2023-06-05 11:41:33.475: Save model to file as pretrain.
2023-06-05 11:42:44.460: [iter 10 : loss : 1.7973 = 0.6859 + 1.1114 + 0.0001, time: 70.128288]
2023-06-05 11:42:48.538: epoch 10:	0.00886553  	0.01932224  	0.01609963  	0.03223922  	0.03536946  
2023-06-05 11:42:48.538: Found a better model.
2023-06-05 11:42:48.538: Save model to file as pretrain.
2023-06-05 11:43:59.417: [iter 11 : loss : 1.7745 = 0.6551 + 1.1192 + 0.0002, time: 70.025753]
2023-06-05 11:44:03.425: epoch 11:	0.01580543  	0.03434183  	0.02874710  	0.05609205  	0.06223388  
2023-06-05 11:44:03.425: Found a better model.
2023-06-05 11:44:03.425: Save model to file as pretrain.
2023-06-05 11:45:13.111: [iter 12 : loss : 1.6939 = 0.5507 + 1.1428 + 0.0004, time: 68.827396]
2023-06-05 11:45:17.142: epoch 12:	0.02128322  	0.04598457  	0.03795073  	0.07248083  	0.08021645  
2023-06-05 11:45:17.142: Found a better model.
2023-06-05 11:45:17.143: Save model to file as pretrain.
2023-06-05 11:46:29.734: [iter 13 : loss : 1.5757 = 0.4044 + 1.1703 + 0.0010, time: 71.697079]
2023-06-05 11:46:33.949: epoch 13:	0.02377582  	0.05159745  	0.04219064  	0.07889453  	0.08770146  
2023-06-05 11:46:33.949: Found a better model.
2023-06-05 11:46:33.949: Save model to file as pretrain.
2023-06-05 11:47:59.818: [iter 14 : loss : 1.4639 = 0.2737 + 1.1887 + 0.0016, time: 84.959117]
2023-06-05 11:48:04.184: epoch 14:	0.02489978  	0.05411961  	0.04445307  	0.08335026  	0.09248107  
2023-06-05 11:48:04.184: Found a better model.
2023-06-05 11:48:04.184: Save model to file as pretrain.
2023-06-05 11:49:27.951: [iter 15 : loss : 1.3870 = 0.1955 + 1.1892 + 0.0022, time: 82.892746]
2023-06-05 11:49:32.178: epoch 15:	0.02565910  	0.05623558  	0.04618863  	0.08645010  	0.09618346  
2023-06-05 11:49:32.178: Found a better model.
2023-06-05 11:49:32.178: Save model to file as pretrain.
2023-06-05 11:50:44.873: [iter 16 : loss : 1.3378 = 0.1527 + 1.1823 + 0.0027, time: 71.792591]
2023-06-05 11:50:49.102: epoch 16:	0.02622902  	0.05778763  	0.04748071  	0.08873712  	0.09887330  
2023-06-05 11:50:49.102: Found a better model.
2023-06-05 11:50:49.102: Save model to file as pretrain.
2023-06-05 11:52:02.051: [iter 17 : loss : 1.3035 = 0.1254 + 1.1748 + 0.0032, time: 72.103837]
2023-06-05 11:52:06.355: epoch 17:	0.02677840  	0.05931740  	0.04860470  	0.09062843  	0.10094541  
2023-06-05 11:52:06.355: Found a better model.
2023-06-05 11:52:06.355: Save model to file as pretrain.
2023-06-05 11:53:24.391: [iter 18 : loss : 1.2783 = 0.1064 + 1.1682 + 0.0037, time: 77.197512]
2023-06-05 11:53:28.365: epoch 18:	0.02712256  	0.06007385  	0.04927888  	0.09186878  	0.10237464  
2023-06-05 11:53:28.366: Found a better model.
2023-06-05 11:53:28.366: Save model to file as pretrain.
2023-06-05 11:54:39.464: [iter 19 : loss : 1.2590 = 0.0924 + 1.1625 + 0.0041, time: 70.242828]
2023-06-05 11:54:43.173: epoch 19:	0.02743199  	0.06087551  	0.04989553  	0.09276763  	0.10351114  
2023-06-05 11:54:43.174: Found a better model.
2023-06-05 11:54:43.174: Save model to file as pretrain.
2023-06-05 11:56:09.483: [iter 20 : loss : 1.2441 = 0.0819 + 1.1577 + 0.0045, time: 85.469547]
2023-06-05 11:56:13.743: epoch 20:	0.02758985  	0.06124907  	0.05028298  	0.09356626  	0.10451021  
2023-06-05 11:56:13.743: Found a better model.
2023-06-05 11:56:13.743: Save model to file as pretrain.
2023-06-05 11:57:39.295: [iter 21 : loss : 1.2316 = 0.0731 + 1.1537 + 0.0049, time: 84.691937]
2023-06-05 11:57:42.616: epoch 21:	0.02776984  	0.06154332  	0.05076084  	0.09475678  	0.10571931  
2023-06-05 11:57:42.616: Found a better model.
2023-06-05 11:57:42.616: Save model to file as pretrain.
2023-06-05 11:58:54.145: [iter 22 : loss : 1.2216 = 0.0661 + 1.1503 + 0.0052, time: 70.610879]
2023-06-05 11:58:58.397: epoch 22:	0.02801927  	0.06219573  	0.05111090  	0.09472485  	0.10589010  
2023-06-05 11:58:58.398: Found a better model.
2023-06-05 11:58:58.398: Save model to file as pretrain.
2023-06-05 12:00:16.825: [iter 23 : loss : 1.2132 = 0.0602 + 1.1474 + 0.0056, time: 77.582896]
2023-06-05 12:00:21.177: epoch 23:	0.02807297  	0.06236386  	0.05125586  	0.09498902  	0.10613775  
2023-06-05 12:00:21.178: Found a better model.
2023-06-05 12:00:21.178: Save model to file as pretrain.
2023-06-05 12:01:31.141: [iter 24 : loss : 1.2058 = 0.0551 + 1.1448 + 0.0059, time: 69.053045]
2023-06-05 12:01:35.399: epoch 24:	0.02810136  	0.06237172  	0.05138182  	0.09527562  	0.10660543  
2023-06-05 12:01:35.399: Found a better model.
2023-06-05 12:01:35.399: Save model to file as pretrain.
2023-06-05 12:02:46.304: [iter 25 : loss : 1.2001 = 0.0510 + 1.1429 + 0.0062, time: 70.072521]
2023-06-05 12:02:50.301: epoch 25:	0.02817716  	0.06250513  	0.05144849  	0.09526443  	0.10653929  
2023-06-05 12:02:50.301: Found a better model.
2023-06-05 12:02:50.301: Save model to file as pretrain.
2023-06-05 12:04:16.535: [iter 26 : loss : 1.1944 = 0.0472 + 1.1407 + 0.0065, time: 85.415330]
2023-06-05 12:04:20.572: epoch 26:	0.02814873  	0.06245681  	0.05137071  	0.09511388  	0.10661575  
2023-06-05 12:05:46.715: [iter 27 : loss : 1.1904 = 0.0444 + 1.1393 + 0.0068, time: 85.621963]
2023-06-05 12:05:50.948: epoch 27:	0.02806348  	0.06223311  	0.05128324  	0.09543299  	0.10675552  
2023-06-05 12:07:16.033: [iter 28 : loss : 1.1860 = 0.0411 + 1.1378 + 0.0070, time: 84.563767]
2023-06-05 12:07:20.196: epoch 28:	0.02804141  	0.06213826  	0.05123905  	0.09534276  	0.10685140  
2023-06-05 12:08:43.471: [iter 29 : loss : 1.1828 = 0.0389 + 1.1366 + 0.0073, time: 82.762865]
2023-06-05 12:08:47.561: epoch 29:	0.02797036  	0.06182718  	0.05111968  	0.09545540  	0.10687610  
2023-06-05 12:09:56.842: [iter 30 : loss : 1.1797 = 0.0369 + 1.1353 + 0.0075, time: 68.769358]
2023-06-05 12:10:01.132: epoch 30:	0.02788669  	0.06176340  	0.05088597  	0.09473033  	0.10608910  
2023-06-05 12:11:09.537: [iter 31 : loss : 1.1775 = 0.0352 + 1.1345 + 0.0078, time: 67.868348]
2023-06-05 12:11:13.718: epoch 31:	0.02783936  	0.06166815  	0.05085665  	0.09495872  	0.10609105  
2023-06-05 12:12:37.288: [iter 32 : loss : 1.1747 = 0.0333 + 1.1334 + 0.0080, time: 83.047382]
2023-06-05 12:12:41.284: epoch 32:	0.02774462  	0.06127861  	0.05073342  	0.09510385  	0.10622605  
2023-06-05 12:14:01.751: [iter 33 : loss : 1.1730 = 0.0321 + 1.1326 + 0.0082, time: 79.945369]
2023-06-05 12:14:05.652: epoch 33:	0.02776199  	0.06120723  	0.05072358  	0.09501332  	0.10644026  
2023-06-05 12:15:14.754: [iter 34 : loss : 1.1707 = 0.0305 + 1.1318 + 0.0085, time: 68.590843]
2023-06-05 12:15:18.664: epoch 34:	0.02773832  	0.06116916  	0.05060772  	0.09478002  	0.10603487  
2023-06-05 12:16:28.816: [iter 35 : loss : 1.1691 = 0.0293 + 1.1311 + 0.0087, time: 69.624586]
2023-06-05 12:16:32.921: epoch 35:	0.02763727  	0.06093252  	0.05031545  	0.09416633  	0.10525937  
2023-06-05 12:17:58.855: [iter 36 : loss : 1.1676 = 0.0282 + 1.1306 + 0.0089, time: 85.398385]
2023-06-05 12:18:02.887: epoch 36:	0.02763411  	0.06079645  	0.05027900  	0.09430756  	0.10530696  
2023-06-05 12:19:12.107: [iter 37 : loss : 1.1663 = 0.0272 + 1.1300 + 0.0090, time: 68.697867]
2023-06-05 12:19:16.210: epoch 37:	0.02761994  	0.06074060  	0.05021304  	0.09402131  	0.10527062  
2023-06-05 12:20:40.640: [iter 38 : loss : 1.1650 = 0.0262 + 1.1295 + 0.0092, time: 83.904796]
2023-06-05 12:20:44.975: epoch 38:	0.02745417  	0.06026282  	0.04997331  	0.09412011  	0.10514818  
2023-06-05 12:22:08.540: [iter 39 : loss : 1.1638 = 0.0254 + 1.1290 + 0.0094, time: 83.048628]
2023-06-05 12:22:12.782: epoch 39:	0.02738155  	0.06008241  	0.04982961  	0.09383202  	0.10488992  
2023-06-05 12:23:23.466: [iter 40 : loss : 1.1630 = 0.0246 + 1.1288 + 0.0096, time: 70.154818]
2023-06-05 12:23:27.558: epoch 40:	0.02728842  	0.05989913  	0.04974145  	0.09382524  	0.10503136  
2023-06-05 12:24:53.195: [iter 41 : loss : 1.1616 = 0.0237 + 1.1282 + 0.0097, time: 85.098131]
2023-06-05 12:24:56.659: epoch 41:	0.02723158  	0.05950396  	0.04957482  	0.09386443  	0.10501177  
2023-06-05 12:26:05.873: [iter 42 : loss : 1.1608 = 0.0231 + 1.1278 + 0.0099, time: 68.699077]
2023-06-05 12:26:09.846: epoch 42:	0.02714792  	0.05924435  	0.04941766  	0.09361482  	0.10477857  
2023-06-05 12:27:20.310: [iter 43 : loss : 1.1601 = 0.0225 + 1.1276 + 0.0100, time: 69.942108]
2023-06-05 12:27:24.471: epoch 43:	0.02704688  	0.05896034  	0.04935532  	0.09394306  	0.10511437  
2023-06-05 12:28:33.539: [iter 44 : loss : 1.1592 = 0.0219 + 1.1271 + 0.0102, time: 68.525075]
2023-06-05 12:28:37.797: epoch 44:	0.02699321  	0.05876492  	0.04915247  	0.09349987  	0.10449218  
2023-06-05 12:29:48.689: [iter 45 : loss : 1.1584 = 0.0213 + 1.1268 + 0.0103, time: 70.350535]
2023-06-05 12:29:52.863: epoch 45:	0.02687479  	0.05857091  	0.04889820  	0.09303563  	0.10410301  
2023-06-05 12:31:02.110: [iter 46 : loss : 1.1578 = 0.0209 + 1.1264 + 0.0105, time: 68.731848]
2023-06-05 12:31:05.444: epoch 46:	0.02679742  	0.05833555  	0.04868039  	0.09259958  	0.10334733  
2023-06-05 12:32:14.864: [iter 47 : loss : 1.1572 = 0.0205 + 1.1261 + 0.0106, time: 68.910269]
2023-06-05 12:32:18.868: epoch 47:	0.02674377  	0.05809120  	0.04859260  	0.09259306  	0.10322946  
2023-06-05 12:33:28.787: [iter 48 : loss : 1.1569 = 0.0201 + 1.1261 + 0.0107, time: 69.399740]
2023-06-05 12:33:32.809: epoch 48:	0.02662381  	0.05774092  	0.04836087  	0.09210487  	0.10279131  
2023-06-05 12:34:43.649: [iter 49 : loss : 1.1565 = 0.0198 + 1.1259 + 0.0109, time: 70.329709]
2023-06-05 12:34:48.012: epoch 49:	0.02656066  	0.05757864  	0.04823420  	0.09192590  	0.10264545  
2023-06-05 12:35:57.454: [iter 50 : loss : 1.1558 = 0.0193 + 1.1255 + 0.0110, time: 68.892495]
2023-06-05 12:36:01.521: epoch 50:	0.02652909  	0.05752925  	0.04814460  	0.09171461  	0.10244405  
2023-06-05 12:37:12.838: [iter 51 : loss : 1.1554 = 0.0190 + 1.1253 + 0.0111, time: 70.777497]
2023-06-05 12:37:17.299: epoch 51:	0.02647069  	0.05725620  	0.04795428  	0.09120855  	0.10192776  
2023-06-05 12:38:28.294: [iter 52 : loss : 1.1550 = 0.0186 + 1.1252 + 0.0112, time: 70.450442]
2023-06-05 12:38:32.398: epoch 52:	0.02635700  	0.05691293  	0.04776092  	0.09112139  	0.10175299  
2023-06-05 12:39:42.797: [iter 53 : loss : 1.1548 = 0.0185 + 1.1250 + 0.0113, time: 69.854241]
2023-06-05 12:39:46.758: epoch 53:	0.02638385  	0.05693678  	0.04767879  	0.09080254  	0.10144417  
2023-06-05 12:40:57.454: [iter 54 : loss : 1.1540 = 0.0179 + 1.1246 + 0.0114, time: 70.181227]
2023-06-05 12:41:01.812: epoch 54:	0.02621810  	0.05655312  	0.04749069  	0.09066540  	0.10144485  
2023-06-05 12:42:11.880: [iter 55 : loss : 1.1537 = 0.0176 + 1.1246 + 0.0115, time: 69.545545]
2023-06-05 12:42:16.076: epoch 55:	0.02617231  	0.05643257  	0.04742600  	0.09069873  	0.10143689  
2023-06-05 12:43:26.858: [iter 56 : loss : 1.1532 = 0.0171 + 1.1244 + 0.0116, time: 70.262691]
2023-06-05 12:43:31.124: epoch 56:	0.02615966  	0.05637213  	0.04735802  	0.09064604  	0.10137282  
2023-06-05 12:44:42.501: [iter 57 : loss : 1.1528 = 0.0169 + 1.1242 + 0.0117, time: 70.859504]
2023-06-05 12:44:46.581: epoch 57:	0.02607283  	0.05617645  	0.04719164  	0.09034514  	0.10096826  
2023-06-05 12:45:57.172: [iter 58 : loss : 1.1528 = 0.0168 + 1.1242 + 0.0118, time: 70.079659]
2023-06-05 12:46:01.438: epoch 58:	0.02603965  	0.05607966  	0.04713254  	0.09041513  	0.10116624  
2023-06-05 12:47:11.369: [iter 59 : loss : 1.1526 = 0.0166 + 1.1241 + 0.0119, time: 69.372631]
2023-06-05 12:47:15.687: epoch 59:	0.02591809  	0.05582677  	0.04689100  	0.09014975  	0.10075069  
2023-06-05 12:48:26.895: [iter 60 : loss : 1.1522 = 0.0163 + 1.1238 + 0.0120, time: 70.696963]
2023-06-05 12:48:30.820: epoch 60:	0.02587073  	0.05567807  	0.04685088  	0.09024797  	0.10078722  
2023-06-05 12:49:41.108: [iter 61 : loss : 1.1522 = 0.0162 + 1.1239 + 0.0121, time: 69.772902]
2023-06-05 12:49:45.375: epoch 61:	0.02579973  	0.05544281  	0.04670202  	0.08996402  	0.10045857  
2023-06-05 12:50:54.719: [iter 62 : loss : 1.1518 = 0.0161 + 1.1235 + 0.0122, time: 68.826313]
2023-06-05 12:50:58.808: epoch 62:	0.02567973  	0.05515929  	0.04645836  	0.08940498  	0.09972000  
2023-06-05 12:52:08.830: [iter 63 : loss : 1.1515 = 0.0157 + 1.1235 + 0.0122, time: 69.486362]
2023-06-05 12:52:12.503: epoch 63:	0.02564972  	0.05509511  	0.04635984  	0.08912145  	0.09942742  
2023-06-05 12:53:22.327: [iter 64 : loss : 1.1509 = 0.0154 + 1.1232 + 0.0123, time: 69.321036]
2023-06-05 12:53:26.338: epoch 64:	0.02566550  	0.05506047  	0.04635590  	0.08917809  	0.09948483  
2023-06-05 12:54:37.469: [iter 65 : loss : 1.1512 = 0.0154 + 1.1234 + 0.0124, time: 70.620433]
2023-06-05 12:54:41.450: epoch 65:	0.02564181  	0.05491449  	0.04627455  	0.08921756  	0.09933338  
2023-06-05 12:55:49.358: [iter 66 : loss : 1.1510 = 0.0153 + 1.1232 + 0.0125, time: 67.380072]
2023-06-05 12:55:53.335: epoch 66:	0.02553135  	0.05462888  	0.04604018  	0.08865727  	0.09892794  
2023-06-05 12:57:03.704: [iter 67 : loss : 1.1504 = 0.0147 + 1.1232 + 0.0125, time: 69.846316]
2023-06-05 12:57:07.556: epoch 67:	0.02544608  	0.05432213  	0.04593771  	0.08876339  	0.09890506  
2023-06-05 12:58:16.676: [iter 68 : loss : 1.1505 = 0.0149 + 1.1229 + 0.0126, time: 68.603839]
2023-06-05 12:58:20.040: epoch 68:	0.02537822  	0.05409070  	0.04582432  	0.08874734  	0.09899552  
2023-06-05 12:59:30.717: [iter 69 : loss : 1.1502 = 0.0148 + 1.1228 + 0.0127, time: 70.170184]
2023-06-05 12:59:34.892: epoch 69:	0.02543349  	0.05418928  	0.04579242  	0.08851430  	0.09871300  
2023-06-05 13:00:45.096: [iter 70 : loss : 1.1502 = 0.0146 + 1.1228 + 0.0127, time: 69.695891]
2023-06-05 13:00:49.187: epoch 70:	0.02541453  	0.05409648  	0.04574764  	0.08858568  	0.09866706  
2023-06-05 13:01:57.796: [iter 71 : loss : 1.1497 = 0.0142 + 1.1227 + 0.0128, time: 68.066642]
2023-06-05 13:02:01.919: epoch 71:	0.02536084  	0.05394236  	0.04564101  	0.08832791  	0.09845942  
2023-06-05 13:03:12.545: [iter 72 : loss : 1.1498 = 0.0144 + 1.1226 + 0.0128, time: 70.090522]
2023-06-05 13:03:16.587: epoch 72:	0.02533087  	0.05383189  	0.04557363  	0.08829165  	0.09841111  
2023-06-05 13:04:25.875: [iter 73 : loss : 1.1497 = 0.0141 + 1.1227 + 0.0129, time: 68.746754]
2023-06-05 13:04:29.588: epoch 73:	0.02518247  	0.05344662  	0.04531077  	0.08809702  	0.09791884  
2023-06-05 13:05:40.229: [iter 74 : loss : 1.1495 = 0.0141 + 1.1225 + 0.0130, time: 70.100895]
2023-06-05 13:05:44.182: epoch 74:	0.02513510  	0.05332372  	0.04534110  	0.08824276  	0.09836616  
2023-06-05 13:06:55.131: [iter 75 : loss : 1.1493 = 0.0138 + 1.1224 + 0.0130, time: 70.405703]
2023-06-05 13:06:59.521: epoch 75:	0.02505302  	0.05317200  	0.04519273  	0.08818213  	0.09806354  
2023-06-05 13:06:59.521: Early stopping is triggered at epoch: 75
2023-06-05 13:06:59.522: best_result@epoch 25:

2023-06-05 13:06:59.522: Loading from the saved model.
2023-06-05 13:07:03.648: 		0.02817716  	0.06250513  	0.05144849  	0.09526443  	0.10653929  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
