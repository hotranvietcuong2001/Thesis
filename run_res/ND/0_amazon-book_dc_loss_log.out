seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-06 08:48:06.971: Dataset name: amazon-book
The number of users: 52643
The number of items: 91599
The number of ratings: 2984108
Average actions of users: 56.69
Average actions of items: 32.58
The sparsity of the dataset: 99.938115%
2023-06-06 08:48:06.971: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=amazon-book
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=0
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=1
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using decoupled loss
2023-06-06 08:48:13.229: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-06 08:48:28.103: 		0.00015293  	0.00026681  	0.00023563  	0.00062383  	0.00062383  
2023-06-06 08:53:44.655: [iter 1 : loss : 7.0222 = 0.6931 + 6.3292 + 0.0000, time: 315.408761]
2023-06-06 08:53:57.847: epoch 1:	0.00041604  	0.00081476  	0.00072933  	0.00164830  	0.00170354  
2023-06-06 08:53:57.847: Found a better model.
2023-06-06 08:53:57.847: Save model to file as pretrain.
2023-06-06 08:59:11.213: [iter 2 : loss : 7.0115 = 0.6930 + 6.3185 + 0.0000, time: 309.879577]
2023-06-06 08:59:21.190: epoch 2:	0.00042744  	0.00080545  	0.00076058  	0.00193614  	0.00200672  
2023-06-06 09:04:35.985: [iter 3 : loss : 7.0096 = 0.6928 + 6.3168 + 0.0000, time: 313.739085]
2023-06-06 09:04:49.298: epoch 3:	0.00045973  	0.00083255  	0.00071378  	0.00173656  	0.00180052  
2023-06-06 09:04:49.298: Found a better model.
2023-06-06 09:04:49.298: Save model to file as pretrain.
2023-06-06 09:10:02.988: [iter 4 : loss : 7.0085 = 0.6926 + 6.3159 + 0.0000, time: 310.170475]
2023-06-06 09:10:14.731: epoch 4:	0.00047113  	0.00085022  	0.00075525  	0.00191911  	0.00196551  
2023-06-06 09:10:14.731: Found a better model.
2023-06-06 09:10:14.731: Save model to file as pretrain.
2023-06-06 09:15:31.390: [iter 5 : loss : 7.0074 = 0.6923 + 6.3151 + 0.0000, time: 313.014412]
2023-06-06 09:15:44.481: epoch 5:	0.00054807  	0.00099448  	0.00083760  	0.00189118  	0.00198335  
2023-06-06 09:15:44.481: Found a better model.
2023-06-06 09:15:44.481: Save model to file as pretrain.
2023-06-06 09:20:55.743: [iter 6 : loss : 7.0070 = 0.6919 + 6.3151 + 0.0000, time: 307.712264]
2023-06-06 09:21:07.542: epoch 6:	0.00058701  	0.00112047  	0.00094301  	0.00219261  	0.00224688  
2023-06-06 09:21:07.542: Found a better model.
2023-06-06 09:21:07.543: Save model to file as pretrain.
2023-06-06 09:26:24.908: [iter 7 : loss : 7.0063 = 0.6913 + 6.3150 + 0.0000, time: 313.810833]
2023-06-06 09:26:35.435: epoch 7:	0.00061361  	0.00117841  	0.00101051  	0.00232708  	0.00237876  
2023-06-06 09:26:35.435: Found a better model.
2023-06-06 09:26:35.435: Save model to file as pretrain.
2023-06-06 09:31:51.602: [iter 8 : loss : 7.0058 = 0.6905 + 6.3152 + 0.0000, time: 312.750132]
2023-06-06 09:32:04.903: epoch 8:	0.00069150  	0.00142027  	0.00119772  	0.00268325  	0.00275132  
2023-06-06 09:32:04.903: Found a better model.
2023-06-06 09:32:04.903: Save model to file as pretrain.
2023-06-06 09:37:18.516: [iter 9 : loss : 7.0046 = 0.6894 + 6.3152 + 0.0001, time: 310.234374]
2023-06-06 09:37:31.766: epoch 9:	0.00069625  	0.00145627  	0.00129565  	0.00309204  	0.00319312  
2023-06-06 09:37:31.766: Found a better model.
2023-06-06 09:37:31.766: Save model to file as pretrain.
2023-06-06 09:42:44.625: [iter 10 : loss : 7.0035 = 0.6875 + 6.3159 + 0.0001, time: 309.345765]
2023-06-06 09:42:57.891: epoch 10:	0.00083398  	0.00184081  	0.00152139  	0.00348572  	0.00361378  
2023-06-06 09:42:57.892: Found a better model.
2023-06-06 09:42:57.892: Save model to file as pretrain.
2023-06-06 09:48:13.580: [iter 11 : loss : 7.0013 = 0.6842 + 6.3170 + 0.0001, time: 312.277144]
2023-06-06 09:48:24.924: epoch 11:	0.00126521  	0.00301434  	0.00241948  	0.00507075  	0.00530211  
2023-06-06 09:48:24.925: Found a better model.
2023-06-06 09:48:24.925: Save model to file as pretrain.
2023-06-06 09:53:37.667: [iter 12 : loss : 6.9962 = 0.6774 + 6.3186 + 0.0002, time: 309.133058]
2023-06-06 09:53:50.327: epoch 12:	0.00240225  	0.00597175  	0.00471737  	0.00933901  	0.00993755  
2023-06-06 09:53:50.328: Found a better model.
2023-06-06 09:53:50.328: Save model to file as pretrain.
2023-06-06 09:59:04.030: [iter 13 : loss : 6.9835 = 0.6602 + 6.3230 + 0.0003, time: 310.195864]
2023-06-06 09:59:17.254: epoch 13:	0.00472573  	0.01173591  	0.00927457  	0.01771516  	0.01903159  
2023-06-06 09:59:17.254: Found a better model.
2023-06-06 09:59:17.254: Save model to file as pretrain.
2023-06-06 10:04:31.927: [iter 14 : loss : 6.9489 = 0.6150 + 6.3334 + 0.0005, time: 311.240109]
2023-06-06 10:04:45.436: epoch 14:	0.00819612  	0.01984307  	0.01612854  	0.03040497  	0.03290750  
2023-06-06 10:04:45.436: Found a better model.
2023-06-06 10:04:45.436: Save model to file as pretrain.
2023-06-06 10:09:59.458: [iter 15 : loss : 6.8833 = 0.5340 + 6.3485 + 0.0008, time: 310.518603]
2023-06-06 10:10:12.944: epoch 15:	0.01100545  	0.02579468  	0.02119156  	0.03937405  	0.04281203  
2023-06-06 10:10:12.945: Found a better model.
2023-06-06 10:10:12.945: Save model to file as pretrain.
2023-06-06 10:15:23.018: [iter 16 : loss : 6.8031 = 0.4398 + 6.3619 + 0.0014, time: 306.644917]
2023-06-06 10:15:34.931: epoch 16:	0.01309766  	0.03069199  	0.02513968  	0.04626507  	0.05068286  
2023-06-06 10:15:34.932: Found a better model.
2023-06-06 10:15:34.932: Save model to file as pretrain.
2023-06-06 10:20:51.831: [iter 17 : loss : 6.7244 = 0.3525 + 6.3700 + 0.0020, time: 313.408716]
2023-06-06 10:21:04.558: epoch 17:	0.01450135  	0.03410434  	0.02758374  	0.04972360  	0.05465055  
2023-06-06 10:21:04.558: Found a better model.
2023-06-06 10:21:04.559: Save model to file as pretrain.
2023-06-06 10:26:19.936: [iter 18 : loss : 6.6554 = 0.2789 + 6.3739 + 0.0026, time: 311.835230]
2023-06-06 10:26:33.311: epoch 18:	0.01528962  	0.03601633  	0.02906424  	0.05204330  	0.05714579  
2023-06-06 10:26:33.311: Found a better model.
2023-06-06 10:26:33.311: Save model to file as pretrain.
2023-06-06 10:31:49.875: [iter 19 : loss : 6.5981 = 0.2205 + 6.3744 + 0.0032, time: 313.238348]
2023-06-06 10:32:04.625: epoch 19:	0.01570175  	0.03709074  	0.02976993  	0.05318301  	0.05833187  
2023-06-06 10:32:04.625: Found a better model.
2023-06-06 10:32:04.625: Save model to file as pretrain.
2023-06-06 10:37:19.202: [iter 20 : loss : 6.5525 = 0.1755 + 6.3731 + 0.0038, time: 311.101827]
2023-06-06 10:37:32.449: epoch 20:	0.01605029  	0.03806903  	0.03038233  	0.05404758  	0.05926958  
2023-06-06 10:37:32.449: Found a better model.
2023-06-06 10:37:32.449: Save model to file as pretrain.
2023-06-06 10:42:47.725: [iter 21 : loss : 6.5162 = 0.1410 + 6.3708 + 0.0044, time: 311.866104]
2023-06-06 10:43:02.382: epoch 21:	0.01624403  	0.03844455  	0.03070480  	0.05480384  	0.06001474  
2023-06-06 10:43:02.382: Found a better model.
2023-06-06 10:43:02.382: Save model to file as pretrain.
2023-06-06 10:48:12.642: [iter 22 : loss : 6.4890 = 0.1154 + 6.3686 + 0.0050, time: 306.777303]
2023-06-06 10:48:25.287: epoch 22:	0.01639693  	0.03883962  	0.03095998  	0.05516281  	0.06032418  
2023-06-06 10:48:25.287: Found a better model.
2023-06-06 10:48:25.287: Save model to file as pretrain.
2023-06-06 10:53:34.857: [iter 23 : loss : 6.4681 = 0.0960 + 6.3666 + 0.0056, time: 306.141255]
2023-06-06 10:53:49.594: epoch 23:	0.01634279  	0.03866355  	0.03093601  	0.05507123  	0.06040150  
2023-06-06 10:58:58.685: [iter 24 : loss : 6.4511 = 0.0814 + 6.3636 + 0.0061, time: 308.007617]
2023-06-06 10:59:13.536: epoch 24:	0.01618134  	0.03817324  	0.03062937  	0.05499829  	0.06021317  
2023-06-06 11:04:19.759: [iter 25 : loss : 6.4374 = 0.0698 + 6.3611 + 0.0066, time: 305.151928]
2023-06-06 11:04:33.082: epoch 25:	0.01620128  	0.03824249  	0.03072809  	0.05519259  	0.06034594  
2023-06-06 11:09:39.546: [iter 26 : loss : 6.4277 = 0.0611 + 6.3596 + 0.0070, time: 305.365027]
2023-06-06 11:09:51.369: epoch 26:	0.01607496  	0.03789124  	0.03046725  	0.05500733  	0.06002526  
2023-06-06 11:14:55.310: [iter 27 : loss : 6.4197 = 0.0544 + 6.3579 + 0.0074, time: 302.916577]
2023-06-06 11:15:08.663: epoch 27:	0.01594959  	0.03759039  	0.03025397  	0.05470405  	0.05971364  
2023-06-06 11:20:16.286: [iter 28 : loss : 6.4131 = 0.0489 + 6.3563 + 0.0078, time: 306.517871]
2023-06-06 11:20:28.671: epoch 28:	0.01590021  	0.03737804  	0.03010511  	0.05454725  	0.05965568  
2023-06-06 11:25:34.753: [iter 29 : loss : 6.4073 = 0.0443 + 6.3547 + 0.0082, time: 305.015338]
2023-06-06 11:25:48.230: epoch 29:	0.01577389  	0.03685350  	0.02970144  	0.05398650  	0.05888890  
2023-06-06 11:30:56.186: [iter 30 : loss : 6.4032 = 0.0406 + 6.3540 + 0.0086, time: 306.831040]
2023-06-06 11:31:09.020: epoch 30:	0.01558677  	0.03629998  	0.02929861  	0.05331173  	0.05828774  
2023-06-06 11:36:14.695: [iter 31 : loss : 6.3994 = 0.0376 + 6.3529 + 0.0089, time: 304.576954]
2023-06-06 11:36:26.757: epoch 31:	0.01540918  	0.03574090  	0.02883583  	0.05250974  	0.05734827  
2023-06-06 11:41:36.977: [iter 32 : loss : 6.3959 = 0.0348 + 6.3519 + 0.0092, time: 309.125161]
2023-06-06 11:41:48.552: epoch 32:	0.01525914  	0.03540243  	0.02848660  	0.05184695  	0.05660328  
2023-06-06 11:46:57.704: [iter 33 : loss : 6.3935 = 0.0328 + 6.3512 + 0.0095, time: 308.060788]
2023-06-06 11:47:10.052: epoch 33:	0.01509388  	0.03491154  	0.02808771  	0.05121967  	0.05581186  
2023-06-06 11:52:20.837: [iter 34 : loss : 6.3910 = 0.0307 + 6.3505 + 0.0098, time: 309.663570]
2023-06-06 11:52:33.626: epoch 34:	0.01486687  	0.03434049  	0.02764097  	0.05067845  	0.05511937  
2023-06-06 11:57:44.194: [iter 35 : loss : 6.3893 = 0.0292 + 6.3500 + 0.0100, time: 309.479077]
2023-06-06 11:57:56.276: epoch 35:	0.01466270  	0.03368809  	0.02724221  	0.05021170  	0.05474169  
2023-06-06 12:03:08.598: [iter 36 : loss : 6.3876 = 0.0277 + 6.3495 + 0.0103, time: 311.224577]
2023-06-06 12:03:20.451: epoch 36:	0.01452880  	0.03330835  	0.02693975  	0.04988158  	0.05431551  
2023-06-06 12:08:29.914: [iter 37 : loss : 6.3860 = 0.0266 + 6.3488 + 0.0105, time: 308.380405]
2023-06-06 12:08:42.247: epoch 37:	0.01434929  	0.03278668  	0.02661528  	0.04941628  	0.05392240  
2023-06-06 12:13:52.262: [iter 38 : loss : 6.3846 = 0.0255 + 6.3483 + 0.0108, time: 308.914261]
2023-06-06 12:14:05.778: epoch 38:	0.01415746  	0.03228116  	0.02631514  	0.04925093  	0.05348770  
2023-06-06 12:19:15.665: [iter 39 : loss : 6.3829 = 0.0242 + 6.3477 + 0.0110, time: 308.802734]
2023-06-06 12:19:27.785: epoch 39:	0.01407577  	0.03202232  	0.02612181  	0.04902209  	0.05321738  
2023-06-06 12:24:36.656: [iter 40 : loss : 6.3821 = 0.0235 + 6.3474 + 0.0112, time: 307.804639]
2023-06-06 12:24:48.366: epoch 40:	0.01391813  	0.03152345  	0.02571572  	0.04836352  	0.05240325  
2023-06-06 12:30:01.621: [iter 41 : loss : 6.3813 = 0.0227 + 6.3472 + 0.0114, time: 312.193056]
2023-06-06 12:30:15.424: epoch 41:	0.01381174  	0.03119003  	0.02555606  	0.04823645  	0.05228219  
2023-06-06 12:35:24.343: [iter 42 : loss : 6.3804 = 0.0221 + 6.3467 + 0.0116, time: 307.847349]
2023-06-06 12:35:36.475: epoch 42:	0.01370346  	0.03087394  	0.02532745  	0.04807586  	0.05211239  
2023-06-06 12:40:47.950: [iter 43 : loss : 6.3796 = 0.0213 + 6.3466 + 0.0117, time: 310.371145]
2023-06-06 12:40:58.263: epoch 43:	0.01344895  	0.03013754  	0.02487882  	0.04756014  	0.05150827  
2023-06-06 12:46:06.917: [iter 44 : loss : 6.3790 = 0.0209 + 6.3462 + 0.0119, time: 307.568005]
2023-06-06 12:46:20.241: epoch 44:	0.01337390  	0.02983417  	0.02468484  	0.04730893  	0.05128743  
2023-06-06 12:51:25.947: [iter 45 : loss : 6.3778 = 0.0202 + 6.3456 + 0.0121, time: 304.613968]
2023-06-06 12:51:37.502: epoch 45:	0.01327513  	0.02951309  	0.02444481  	0.04682913  	0.05077175  
2023-06-06 12:56:48.955: [iter 46 : loss : 6.3773 = 0.0198 + 6.3452 + 0.0122, time: 310.359139]
2023-06-06 12:57:02.243: epoch 46:	0.01319727  	0.02933415  	0.02428493  	0.04657748  	0.05046408  
2023-06-06 13:02:12.491: [iter 47 : loss : 6.3768 = 0.0195 + 6.3449 + 0.0124, time: 309.139556]
2023-06-06 13:02:25.579: epoch 47:	0.01310040  	0.02906309  	0.02400405  	0.04587261  	0.04971606  
2023-06-06 13:07:37.665: [iter 48 : loss : 6.3768 = 0.0191 + 6.3451 + 0.0125, time: 310.980658]
2023-06-06 13:07:51.222: epoch 48:	0.01295699  	0.02861156  	0.02375234  	0.04572574  	0.04946853  
2023-06-06 13:13:01.137: [iter 49 : loss : 6.3761 = 0.0186 + 6.3449 + 0.0126, time: 308.836937]
2023-06-06 13:13:13.201: epoch 49:	0.01285157  	0.02848307  	0.02355611  	0.04524545  	0.04906095  
2023-06-06 13:18:26.545: [iter 50 : loss : 6.3753 = 0.0182 + 6.3443 + 0.0128, time: 312.272304]
2023-06-06 13:18:40.173: epoch 50:	0.01277844  	0.02819038  	0.02340435  	0.04512048  	0.04888237  
2023-06-06 13:23:51.165: [iter 51 : loss : 6.3751 = 0.0179 + 6.3443 + 0.0129, time: 309.877562]
2023-06-06 13:24:04.776: epoch 51:	0.01256666  	0.02756406  	0.02302314  	0.04467835  	0.04828445  
2023-06-06 13:29:21.268: [iter 52 : loss : 6.3748 = 0.0176 + 6.3442 + 0.0130, time: 315.416770]
2023-06-06 13:29:34.109: epoch 52:	0.01253057  	0.02747788  	0.02295456  	0.04469230  	0.04839640  
2023-06-06 13:34:45.733: [iter 53 : loss : 6.3744 = 0.0175 + 6.3438 + 0.0131, time: 310.501790]
2023-06-06 13:34:57.682: epoch 53:	0.01243180  	0.02719063  	0.02282425  	0.04473224  	0.04847667  
2023-06-06 13:40:10.755: [iter 54 : loss : 6.3738 = 0.0170 + 6.3436 + 0.0132, time: 311.999726]
2023-06-06 13:40:24.370: epoch 54:	0.01234822  	0.02701075  	0.02269129  	0.04451802  	0.04827915  
2023-06-06 13:45:39.093: [iter 55 : loss : 6.3740 = 0.0169 + 6.3437 + 0.0133, time: 313.630054]
2023-06-06 13:45:51.184: epoch 55:	0.01235866  	0.02698314  	0.02267183  	0.04449773  	0.04815638  
2023-06-06 13:51:02.057: [iter 56 : loss : 6.3734 = 0.0166 + 6.3434 + 0.0134, time: 309.786309]
2023-06-06 13:51:14.469: epoch 56:	0.01229219  	0.02672202  	0.02245981  	0.04407068  	0.04761362  
2023-06-06 13:56:20.867: [iter 57 : loss : 6.3732 = 0.0164 + 6.3434 + 0.0135, time: 305.350398]
2023-06-06 13:56:33.801: epoch 57:	0.01227890  	0.02671226  	0.02245580  	0.04401319  	0.04761191  
2023-06-06 14:01:44.115: [iter 58 : loss : 6.3726 = 0.0163 + 6.3427 + 0.0136, time: 309.245932]
2023-06-06 14:01:54.407: epoch 58:	0.01218299  	0.02647110  	0.02234006  	0.04389367  	0.04759328  
2023-06-06 14:07:03.144: [iter 59 : loss : 6.3722 = 0.0159 + 6.3427 + 0.0137, time: 307.692120]
2023-06-06 14:07:14.660: epoch 59:	0.01211080  	0.02635488  	0.02213397  	0.04353966  	0.04712046  
2023-06-06 14:12:22.489: [iter 60 : loss : 6.3722 = 0.0158 + 6.3427 + 0.0138, time: 306.787337]
2023-06-06 14:12:34.078: epoch 60:	0.01205381  	0.02612686  	0.02199514  	0.04321276  	0.04677644  
2023-06-06 14:17:42.322: [iter 61 : loss : 6.3719 = 0.0156 + 6.3424 + 0.0139, time: 307.242928]
2023-06-06 14:17:53.814: epoch 61:	0.01195410  	0.02580950  	0.02176503  	0.04281631  	0.04638076  
2023-06-06 14:22:58.658: [iter 62 : loss : 6.3722 = 0.0156 + 6.3427 + 0.0139, time: 303.815595]
2023-06-06 14:23:10.222: epoch 62:	0.01187337  	0.02563095  	0.02160752  	0.04251502  	0.04612989  
2023-06-06 14:28:14.655: [iter 63 : loss : 6.3714 = 0.0154 + 6.3420 + 0.0140, time: 303.392812]
2023-06-06 14:28:23.025: epoch 63:	0.01184679  	0.02555388  	0.02153116  	0.04244354  	0.04597376  
2023-06-06 14:33:32.222: [iter 64 : loss : 6.3716 = 0.0152 + 6.3424 + 0.0141, time: 308.172202]
2023-06-06 14:33:45.046: epoch 64:	0.01179551  	0.02542974  	0.02139461  	0.04225685  	0.04560301  
2023-06-06 14:38:53.083: [iter 65 : loss : 6.3712 = 0.0150 + 6.3420 + 0.0142, time: 306.952784]
2023-06-06 14:39:01.496: epoch 65:	0.01172902  	0.02528409  	0.02120554  	0.04188965  	0.04521773  
2023-06-06 14:44:09.358: [iter 66 : loss : 6.3711 = 0.0149 + 6.3420 + 0.0142, time: 306.834389]
2023-06-06 14:44:20.938: epoch 66:	0.01164639  	0.02506631  	0.02105840  	0.04164273  	0.04495389  
2023-06-06 14:49:25.507: [iter 67 : loss : 6.3704 = 0.0147 + 6.3414 + 0.0143, time: 303.572756]
2023-06-06 14:49:33.874: epoch 67:	0.01161695  	0.02496001  	0.02100893  	0.04171257  	0.04500993  
2023-06-06 14:54:41.457: [iter 68 : loss : 6.3707 = 0.0148 + 6.3415 + 0.0144, time: 306.553380]
2023-06-06 14:54:49.912: epoch 68:	0.01159606  	0.02484089  	0.02095664  	0.04154141  	0.04494422  
2023-06-06 14:59:57.884: [iter 69 : loss : 6.3705 = 0.0144 + 6.3417 + 0.0144, time: 306.947335]
2023-06-06 15:00:13.299: epoch 69:	0.01159035  	0.02481850  	0.02097616  	0.04171689  	0.04501208  
2023-06-06 15:05:19.226: [iter 70 : loss : 6.3705 = 0.0144 + 6.3416 + 0.0145, time: 304.893563]
2023-06-06 15:05:28.430: epoch 70:	0.01155995  	0.02469994  	0.02090359  	0.04154855  	0.04487553  
2023-06-06 15:10:36.479: [iter 71 : loss : 6.3701 = 0.0143 + 6.3412 + 0.0145, time: 307.037750]
2023-06-06 15:10:48.948: epoch 71:	0.01149254  	0.02447851  	0.02075297  	0.04132749  	0.04457784  
2023-06-06 15:15:55.563: [iter 72 : loss : 6.3700 = 0.0142 + 6.3412 + 0.0146, time: 305.587150]
2023-06-06 15:16:06.256: epoch 72:	0.01146215  	0.02444480  	0.02071583  	0.04134240  	0.04455790  
2023-06-06 15:16:06.256: Early stopping is triggered at epoch: 72
2023-06-06 15:16:06.256: best_result@epoch 22:

2023-06-06 15:16:06.256: Loading from the saved model.
2023-06-06 15:16:15.879: 		0.01639693  	0.03883962  	0.03095998  	0.05516281  	0.06032418  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
