seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-05 08:00:55.239: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-05 08:00:55.239: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=0
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=0
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using default loss
2023-06-05 08:00:58.972: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-05 08:01:02.988: 		0.00032999  	0.00062295  	0.00048575  	0.00111102  	0.00111102  
2023-06-05 08:02:16.971: [iter 1 : loss : 1.8196 = 0.6931 + 1.1264 + 0.0000, time: 73.430315]
2023-06-05 08:02:21.296: epoch 1:	0.00062682  	0.00126847  	0.00105051  	0.00259172  	0.00259158  
2023-06-05 08:02:21.296: Found a better model.
2023-06-05 08:02:21.297: Save model to file as pretrain.
2023-06-05 08:03:44.781: [iter 2 : loss : 1.8173 = 0.6931 + 1.1242 + 0.0000, time: 82.561010]
2023-06-05 08:03:48.599: epoch 2:	0.00076102  	0.00159923  	0.00126835  	0.00306241  	0.00310912  
2023-06-05 08:03:48.599: Found a better model.
2023-06-05 08:03:48.599: Save model to file as pretrain.
2023-06-05 08:04:59.905: [iter 3 : loss : 1.8169 = 0.6930 + 1.1239 + 0.0000, time: 70.382610]
2023-06-05 08:05:03.978: epoch 3:	0.00084470  	0.00174424  	0.00140145  	0.00328038  	0.00334044  
2023-06-05 08:05:03.978: Found a better model.
2023-06-05 08:05:03.979: Save model to file as pretrain.
2023-06-05 08:06:14.967: [iter 4 : loss : 1.8166 = 0.6929 + 1.1238 + 0.0000, time: 70.062190]
2023-06-05 08:06:19.130: epoch 4:	0.00088733  	0.00186356  	0.00149187  	0.00346124  	0.00348921  
2023-06-05 08:06:19.130: Found a better model.
2023-06-05 08:06:19.130: Save model to file as pretrain.
2023-06-05 08:07:45.285: [iter 5 : loss : 1.8165 = 0.6928 + 1.1238 + 0.0000, time: 85.307194]
2023-06-05 08:07:49.367: epoch 5:	0.00109100  	0.00232247  	0.00183367  	0.00418652  	0.00426076  
2023-06-05 08:07:49.367: Found a better model.
2023-06-05 08:07:49.367: Save model to file as pretrain.
2023-06-05 08:09:13.924: [iter 6 : loss : 1.8163 = 0.6926 + 1.1237 + 0.0000, time: 83.669718]
2023-06-05 08:09:18.095: epoch 6:	0.00112416  	0.00223110  	0.00180521  	0.00418622  	0.00419156  
2023-06-05 08:10:28.812: [iter 7 : loss : 1.8161 = 0.6923 + 1.1237 + 0.0000, time: 70.178901]
2023-06-05 08:10:32.595: epoch 7:	0.00128994  	0.00268868  	0.00215931  	0.00504670  	0.00511986  
2023-06-05 08:10:32.595: Found a better model.
2023-06-05 08:10:32.595: Save model to file as pretrain.
2023-06-05 08:11:43.868: [iter 8 : loss : 1.8157 = 0.6919 + 1.1238 + 0.0000, time: 70.419871]
2023-06-05 08:11:48.140: epoch 8:	0.00162781  	0.00355991  	0.00282358  	0.00654092  	0.00663107  
2023-06-05 08:11:48.140: Found a better model.
2023-06-05 08:11:48.140: Save model to file as pretrain.
2023-06-05 08:13:14.304: [iter 9 : loss : 1.8151 = 0.6910 + 1.1241 + 0.0000, time: 85.293118]
2023-06-05 08:13:18.517: epoch 9:	0.00312938  	0.00733971  	0.00585753  	0.01281535  	0.01323792  
2023-06-05 08:13:18.517: Found a better model.
2023-06-05 08:13:18.517: Save model to file as pretrain.
2023-06-05 08:14:30.978: [iter 10 : loss : 1.8129 = 0.6881 + 1.1247 + 0.0000, time: 71.635536]
2023-06-05 08:14:35.017: epoch 10:	0.00845825  	0.01955472  	0.01595785  	0.03205858  	0.03450393  
2023-06-05 08:14:35.017: Found a better model.
2023-06-05 08:14:35.017: Save model to file as pretrain.
2023-06-05 08:15:45.905: [iter 11 : loss : 1.7966 = 0.6665 + 1.1300 + 0.0001, time: 70.038229]
2023-06-05 08:15:50.165: epoch 11:	0.01507133  	0.03260398  	0.02708684  	0.05244278  	0.05818056  
2023-06-05 08:15:50.166: Found a better model.
2023-06-05 08:15:50.166: Save model to file as pretrain.
2023-06-05 08:16:59.342: [iter 12 : loss : 1.7182 = 0.5651 + 1.1528 + 0.0004, time: 68.329717]
2023-06-05 08:17:03.230: epoch 12:	0.02068806  	0.04463632  	0.03687461  	0.07052524  	0.07842264  
2023-06-05 08:17:03.230: Found a better model.
2023-06-05 08:17:03.230: Save model to file as pretrain.
2023-06-05 08:18:15.606: [iter 13 : loss : 1.5913 = 0.4074 + 1.1830 + 0.0009, time: 71.517268]
2023-06-05 08:18:19.703: epoch 13:	0.02341750  	0.05075564  	0.04181171  	0.07896516  	0.08791033  
2023-06-05 08:18:19.703: Found a better model.
2023-06-05 08:18:19.703: Save model to file as pretrain.
2023-06-05 08:19:45.072: [iter 14 : loss : 1.4733 = 0.2694 + 1.2024 + 0.0016, time: 84.537637]
2023-06-05 08:19:49.308: epoch 14:	0.02477352  	0.05391928  	0.04437560  	0.08326764  	0.09284641  
2023-06-05 08:19:49.308: Found a better model.
2023-06-05 08:19:49.308: Save model to file as pretrain.
2023-06-05 08:21:15.032: [iter 15 : loss : 1.3945 = 0.1899 + 1.2024 + 0.0022, time: 84.879870]
2023-06-05 08:21:19.200: epoch 15:	0.02565125  	0.05630501  	0.04629613  	0.08681173  	0.09681433  
2023-06-05 08:21:19.200: Found a better model.
2023-06-05 08:21:19.200: Save model to file as pretrain.
2023-06-05 08:22:32.300: [iter 16 : loss : 1.3455 = 0.1478 + 1.1949 + 0.0027, time: 72.167778]
2023-06-05 08:22:36.184: epoch 16:	0.02625269  	0.05787523  	0.04762655  	0.08908112  	0.09956835  
2023-06-05 08:22:36.184: Found a better model.
2023-06-05 08:22:36.184: Save model to file as pretrain.
2023-06-05 08:23:48.836: [iter 17 : loss : 1.3117 = 0.1214 + 1.1871 + 0.0032, time: 71.816185]
2023-06-05 08:23:52.860: epoch 17:	0.02679261  	0.05915950  	0.04866225  	0.09057502  	0.10156875  
2023-06-05 08:23:52.860: Found a better model.
2023-06-05 08:23:52.860: Save model to file as pretrain.
2023-06-05 08:25:18.653: [iter 18 : loss : 1.2871 = 0.1030 + 1.1804 + 0.0036, time: 84.908806]
2023-06-05 08:25:22.594: epoch 18:	0.02711941  	0.05991450  	0.04942478  	0.09218241  	0.10334837  
2023-06-05 08:25:22.594: Found a better model.
2023-06-05 08:25:22.594: Save model to file as pretrain.
2023-06-05 08:26:33.730: [iter 19 : loss : 1.2684 = 0.0896 + 1.1747 + 0.0041, time: 70.272752]
2023-06-05 08:26:38.041: epoch 19:	0.02744301  	0.06083371  	0.05011621  	0.09325071  	0.10469793  
2023-06-05 08:26:38.041: Found a better model.
2023-06-05 08:26:38.041: Save model to file as pretrain.
2023-06-05 08:27:57.681: [iter 20 : loss : 1.2539 = 0.0795 + 1.1699 + 0.0044, time: 78.787746]
2023-06-05 08:28:02.013: epoch 20:	0.02760562  	0.06116970  	0.05050860  	0.09397464  	0.10571084  
2023-06-05 08:28:02.013: Found a better model.
2023-06-05 08:28:02.013: Save model to file as pretrain.
2023-06-05 08:29:27.801: [iter 21 : loss : 1.2419 = 0.0711 + 1.1661 + 0.0048, time: 84.924044]
2023-06-05 08:29:31.707: epoch 21:	0.02777928  	0.06159849  	0.05087874  	0.09455938  	0.10646614  
2023-06-05 08:29:31.708: Found a better model.
2023-06-05 08:29:31.708: Save model to file as pretrain.
2023-06-05 08:30:42.672: [iter 22 : loss : 1.2321 = 0.0643 + 1.1627 + 0.0052, time: 70.072573]
2023-06-05 08:30:46.914: epoch 22:	0.02785194  	0.06165923  	0.05098668  	0.09454001  	0.10654904  
2023-06-05 08:30:46.914: Found a better model.
2023-06-05 08:30:46.914: Save model to file as pretrain.
2023-06-05 08:32:05.168: [iter 23 : loss : 1.2241 = 0.0587 + 1.1599 + 0.0055, time: 77.411884]
2023-06-05 08:32:09.288: epoch 23:	0.02792772  	0.06183299  	0.05110279  	0.09479716  	0.10654679  
2023-06-05 08:32:09.288: Found a better model.
2023-06-05 08:32:09.288: Save model to file as pretrain.
2023-06-05 08:33:27.185: [iter 24 : loss : 1.2171 = 0.0539 + 1.1574 + 0.0058, time: 77.048557]
2023-06-05 08:33:31.438: epoch 24:	0.02796404  	0.06197952  	0.05118116  	0.09492449  	0.10658368  
2023-06-05 08:33:31.438: Found a better model.
2023-06-05 08:33:31.438: Save model to file as pretrain.
2023-06-05 08:34:41.300: [iter 25 : loss : 1.2114 = 0.0499 + 1.1554 + 0.0061, time: 69.038129]
2023-06-05 08:34:44.771: epoch 25:	0.02803979  	0.06213379  	0.05131776  	0.09497996  	0.10691455  
2023-06-05 08:34:44.772: Found a better model.
2023-06-05 08:34:44.772: Save model to file as pretrain.
2023-06-05 08:36:09.961: [iter 26 : loss : 1.2060 = 0.0463 + 1.1534 + 0.0064, time: 84.334812]
2023-06-05 08:36:14.085: epoch 26:	0.02803509  	0.06211335  	0.05131594  	0.09533914  	0.10725600  
2023-06-05 08:37:39.116: [iter 27 : loss : 1.2022 = 0.0436 + 1.1519 + 0.0067, time: 84.498601]
2023-06-05 08:37:42.494: epoch 27:	0.02798615  	0.06183395  	0.05120803  	0.09514334  	0.10690301  
2023-06-05 08:39:01.685: [iter 28 : loss : 1.1979 = 0.0405 + 1.1505 + 0.0069, time: 78.659250]
2023-06-05 08:39:05.658: epoch 28:	0.02801140  	0.06186785  	0.05123367  	0.09528742  	0.10683931  
2023-06-05 08:40:30.472: [iter 29 : loss : 1.1949 = 0.0384 + 1.1494 + 0.0072, time: 84.294954]
2023-06-05 08:40:34.548: epoch 29:	0.02797668  	0.06178907  	0.05110081  	0.09513546  	0.10655273  
2023-06-05 08:41:44.853: [iter 30 : loss : 1.1920 = 0.0365 + 1.1481 + 0.0074, time: 69.761150]
2023-06-05 08:41:48.623: epoch 30:	0.02792615  	0.06163930  	0.05097878  	0.09474188  	0.10624268  
2023-06-05 08:43:00.045: [iter 31 : loss : 1.1897 = 0.0347 + 1.1473 + 0.0077, time: 70.902214]
2023-06-05 08:43:04.357: epoch 31:	0.02794510  	0.06171990  	0.05102137  	0.09492202  	0.10643768  
2023-06-05 08:44:14.180: [iter 32 : loss : 1.1870 = 0.0329 + 1.1462 + 0.0079, time: 69.285879]
2023-06-05 08:44:18.277: epoch 32:	0.02790879  	0.06157255  	0.05091894  	0.09480961  	0.10618693  
2023-06-05 08:45:28.033: [iter 33 : loss : 1.1852 = 0.0316 + 1.1454 + 0.0081, time: 69.228335]
2023-06-05 08:45:32.160: epoch 33:	0.02787093  	0.06144648  	0.05078891  	0.09450711  	0.10583251  
2023-06-05 08:46:51.291: [iter 34 : loss : 1.1830 = 0.0301 + 1.1447 + 0.0083, time: 78.596413]
2023-06-05 08:46:54.495: epoch 34:	0.02780777  	0.06129147  	0.05066279  	0.09446182  	0.10567810  
2023-06-05 08:48:17.030: [iter 35 : loss : 1.1814 = 0.0288 + 1.1440 + 0.0085, time: 82.014693]
2023-06-05 08:48:21.044: epoch 35:	0.02770674  	0.06087894  	0.05042204  	0.09413249  	0.10529190  
2023-06-05 08:49:39.922: [iter 36 : loss : 1.1800 = 0.0279 + 1.1434 + 0.0087, time: 78.328965]
2023-06-05 08:49:43.847: epoch 36:	0.02772094  	0.06092367  	0.05048356  	0.09425617  	0.10560274  
2023-06-05 08:50:54.310: [iter 37 : loss : 1.1787 = 0.0269 + 1.1429 + 0.0089, time: 69.920320]
2023-06-05 08:50:58.373: epoch 37:	0.02767200  	0.06079840  	0.05038494  	0.09410822  	0.10538424  
2023-06-05 08:52:08.717: [iter 38 : loss : 1.1774 = 0.0259 + 1.1424 + 0.0091, time: 69.814375]
2023-06-05 08:52:12.811: epoch 38:	0.02764200  	0.06069305  	0.05030974  	0.09403466  	0.10529201  
2023-06-05 08:53:21.570: [iter 39 : loss : 1.1761 = 0.0250 + 1.1419 + 0.0093, time: 68.248828]
2023-06-05 08:53:25.738: epoch 39:	0.02759467  	0.06043191  	0.05016149  	0.09381144  	0.10498276  
2023-06-05 08:54:51.010: [iter 40 : loss : 1.1753 = 0.0242 + 1.1417 + 0.0094, time: 84.730427]
2023-06-05 08:54:55.081: epoch 40:	0.02756942  	0.06048086  	0.05014538  	0.09378676  	0.10480080  
2023-06-05 08:56:19.849: [iter 41 : loss : 1.1742 = 0.0236 + 1.1411 + 0.0096, time: 84.240950]
2023-06-05 08:56:24.033: epoch 41:	0.02757889  	0.06038063  	0.05003072  	0.09355603  	0.10464253  
2023-06-05 08:57:35.700: [iter 42 : loss : 1.1732 = 0.0228 + 1.1407 + 0.0097, time: 71.137926]
2023-06-05 08:57:40.106: epoch 42:	0.02745892  	0.06008775  	0.04988514  	0.09359350  	0.10458733  
2023-06-05 08:58:54.835: [iter 43 : loss : 1.1728 = 0.0224 + 1.1405 + 0.0099, time: 74.209701]
2023-06-05 08:58:58.384: epoch 43:	0.02747468  	0.05999510  	0.04985020  	0.09338580  	0.10443132  
2023-06-05 09:00:23.269: [iter 44 : loss : 1.1718 = 0.0216 + 1.1401 + 0.0100, time: 84.366797]
2023-06-05 09:00:27.408: epoch 44:	0.02730577  	0.05959576  	0.04963527  	0.09326926  	0.10442500  
2023-06-05 09:01:38.093: [iter 45 : loss : 1.1710 = 0.0212 + 1.1397 + 0.0102, time: 70.154236]
2023-06-05 09:01:42.294: epoch 45:	0.02722844  	0.05921556  	0.04941649  	0.09311902  	0.10417776  
2023-06-05 09:02:52.300: [iter 46 : loss : 1.1705 = 0.0208 + 1.1394 + 0.0103, time: 69.465296]
2023-06-05 09:02:56.666: epoch 46:	0.02713057  	0.05903206  	0.04928892  	0.09278550  	0.10399134  
2023-06-05 09:04:06.300: [iter 47 : loss : 1.1697 = 0.0202 + 1.1391 + 0.0105, time: 69.117300]
2023-06-05 09:04:10.597: epoch 47:	0.02712424  	0.05903559  	0.04925618  	0.09282687  	0.10413893  
2023-06-05 09:05:34.783: [iter 48 : loss : 1.1694 = 0.0198 + 1.1390 + 0.0106, time: 83.664194]
2023-06-05 09:05:39.055: epoch 48:	0.02709901  	0.05895486  	0.04909031  	0.09228770  	0.10335522  
2023-06-05 09:06:56.806: [iter 49 : loss : 1.1690 = 0.0195 + 1.1388 + 0.0107, time: 77.228834]
2023-06-05 09:07:00.898: epoch 49:	0.02699164  	0.05866428  	0.04898081  	0.09243152  	0.10356662  
2023-06-05 09:08:11.298: [iter 50 : loss : 1.1685 = 0.0192 + 1.1384 + 0.0108, time: 69.846913]
2023-06-05 09:08:14.602: epoch 50:	0.02691271  	0.05835589  	0.04881533  	0.09208360  	0.10333923  
2023-06-05 09:09:23.590: [iter 51 : loss : 1.1681 = 0.0189 + 1.1383 + 0.0109, time: 68.457051]
2023-06-05 09:09:27.409: epoch 51:	0.02681639  	0.05814756  	0.04861587  	0.09182558  	0.10276335  
2023-06-05 09:10:37.042: [iter 52 : loss : 1.1678 = 0.0185 + 1.1382 + 0.0110, time: 69.093859]
2023-06-05 09:10:41.476: epoch 52:	0.02680535  	0.05798670  	0.04854355  	0.09178537  	0.10274654  
2023-06-05 09:11:59.686: [iter 53 : loss : 1.1672 = 0.0181 + 1.1380 + 0.0112, time: 77.670472]
2023-06-05 09:12:04.032: epoch 53:	0.02674065  	0.05777624  	0.04839242  	0.09152474  	0.10269486  
2023-06-05 09:13:22.569: [iter 54 : loss : 1.1667 = 0.0178 + 1.1376 + 0.0113, time: 78.003103]
2023-06-05 09:13:26.539: epoch 54:	0.02672484  	0.05768579  	0.04828481  	0.09138088  	0.10233364  
2023-06-05 09:14:37.146: [iter 55 : loss : 1.1664 = 0.0174 + 1.1376 + 0.0114, time: 70.078487]
2023-06-05 09:14:41.164: epoch 55:	0.02669014  	0.05758341  	0.04816823  	0.09101297  	0.10215349  
2023-06-05 09:15:51.031: [iter 56 : loss : 1.1660 = 0.0171 + 1.1374 + 0.0115, time: 69.350693]
2023-06-05 09:15:55.147: epoch 56:	0.02665542  	0.05747861  	0.04810920  	0.09100343  	0.10202155  
2023-06-05 09:17:06.265: [iter 57 : loss : 1.1656 = 0.0169 + 1.1372 + 0.0116, time: 70.571650]
2023-06-05 09:17:10.279: epoch 57:	0.02661279  	0.05743028  	0.04796128  	0.09062660  	0.10150544  
2023-06-05 09:18:18.882: [iter 58 : loss : 1.1654 = 0.0166 + 1.1372 + 0.0116, time: 68.073999]
2023-06-05 09:18:23.130: epoch 58:	0.02650860  	0.05719055  	0.04786986  	0.09065454  	0.10163338  
2023-06-05 09:19:48.543: [iter 59 : loss : 1.1654 = 0.0166 + 1.1371 + 0.0117, time: 84.873724]
2023-06-05 09:19:52.731: epoch 59:	0.02647385  	0.05695730  	0.04777034  	0.09069956  	0.10165131  
2023-06-05 09:21:11.308: [iter 60 : loss : 1.1649 = 0.0162 + 1.1369 + 0.0118, time: 78.061190]
2023-06-05 09:21:15.198: epoch 60:	0.02637597  	0.05677055  	0.04770643  	0.09093940  	0.10177261  
2023-06-05 09:22:32.439: [iter 61 : loss : 1.1648 = 0.0160 + 1.1369 + 0.0119, time: 76.698488]
2023-06-05 09:22:36.420: epoch 61:	0.02633650  	0.05656776  	0.04757231  	0.09073445  	0.10144812  
2023-06-05 09:24:01.686: [iter 62 : loss : 1.1644 = 0.0159 + 1.1365 + 0.0120, time: 84.716632]
2023-06-05 09:24:05.843: epoch 62:	0.02626863  	0.05636816  	0.04742576  	0.09042456  	0.10117773  
2023-06-05 09:25:17.313: [iter 63 : loss : 1.1642 = 0.0157 + 1.1365 + 0.0121, time: 70.927731]
2023-06-05 09:25:21.365: epoch 63:	0.02623392  	0.05635631  	0.04740575  	0.09028835  	0.10119997  
2023-06-05 09:26:32.323: [iter 64 : loss : 1.1637 = 0.0153 + 1.1363 + 0.0121, time: 70.435465]
2023-06-05 09:26:35.486: epoch 64:	0.02612970  	0.05611292  	0.04715359  	0.08993522  	0.10066031  
2023-06-05 09:27:43.914: [iter 65 : loss : 1.1637 = 0.0152 + 1.1364 + 0.0122, time: 67.938809]
2023-06-05 09:27:48.017: epoch 65:	0.02603971  	0.05584779  	0.04695188  	0.08957727  	0.10010917  
2023-06-05 09:28:58.757: [iter 66 : loss : 1.1637 = 0.0151 + 1.1362 + 0.0123, time: 70.197136]
2023-06-05 09:29:02.862: epoch 66:	0.02599866  	0.05568356  	0.04687342  	0.08964691  	0.10012288  
2023-06-05 09:30:12.785: [iter 67 : loss : 1.1631 = 0.0146 + 1.1362 + 0.0124, time: 69.358200]
2023-06-05 09:30:16.970: epoch 67:	0.02589921  	0.05545818  	0.04673143  	0.08933441  	0.09987953  
2023-06-05 09:31:26.413: [iter 68 : loss : 1.1633 = 0.0149 + 1.1360 + 0.0124, time: 68.921082]
2023-06-05 09:31:30.515: epoch 68:	0.02593076  	0.05543596  	0.04673256  	0.08946183  	0.10013301  
2023-06-05 09:32:39.656: [iter 69 : loss : 1.1629 = 0.0146 + 1.1359 + 0.0125, time: 68.606087]
2023-06-05 09:32:43.083: epoch 69:	0.02590236  	0.05538678  	0.04660666  	0.08900716  	0.09956689  
2023-06-05 09:33:52.439: [iter 70 : loss : 1.1629 = 0.0144 + 1.1359 + 0.0125, time: 68.824870]
2023-06-05 09:33:56.505: epoch 70:	0.02586133  	0.05521623  	0.04651412  	0.08908404  	0.09952109  
2023-06-05 09:35:09.563: [iter 71 : loss : 1.1625 = 0.0141 + 1.1357 + 0.0126, time: 72.524873]
2023-06-05 09:35:13.565: epoch 71:	0.02586607  	0.05527648  	0.04644545  	0.08881809  	0.09913331  
2023-06-05 09:36:23.616: [iter 72 : loss : 1.1624 = 0.0142 + 1.1356 + 0.0127, time: 69.519296]
2023-06-05 09:36:27.646: epoch 72:	0.02578554  	0.05495384  	0.04632633  	0.08885118  	0.09924122  
2023-06-05 09:37:37.893: [iter 73 : loss : 1.1625 = 0.0141 + 1.1357 + 0.0127, time: 69.717132]
2023-06-05 09:37:41.939: epoch 73:	0.02571292  	0.05466454  	0.04618349  	0.08856555  	0.09912917  
2023-06-05 09:38:52.735: [iter 74 : loss : 1.1623 = 0.0140 + 1.1355 + 0.0128, time: 70.272484]
2023-06-05 09:38:56.300: epoch 74:	0.02571450  	0.05474380  	0.04613283  	0.08848464  	0.09882494  
2023-06-05 09:40:06.808: [iter 75 : loss : 1.1621 = 0.0138 + 1.1355 + 0.0128, time: 69.970935]
2023-06-05 09:40:10.795: epoch 75:	0.02576190  	0.05475079  	0.04611055  	0.08827443  	0.09873343  
2023-06-05 09:40:10.795: Early stopping is triggered at epoch: 75
2023-06-05 09:40:10.795: best_result@epoch 25:

2023-06-05 09:40:10.795: Loading from the saved model.
2023-06-05 09:40:15.124: 		0.02803979  	0.06213379  	0.05132230  	0.09498522  	0.10693034  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
