seed= 2021
split and save data...
2023-06-05 18:38:29.773: amazon-book_given_u0_i0
2023-06-05 18:38:29.773: Dataset name: amazon-book
The number of users: 52643
The number of items: 91599
The number of ratings: 2984108
Average actions of users: 56.69
Average actions of items: 32.58
The sparsity of the dataset: 99.938115%
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-05 18:38:38.168: Dataset name: amazon-book
The number of users: 52643
The number of items: 91599
The number of ratings: 2984108
Average actions of users: 56.69
Average actions of items: 32.58
The sparsity of the dataset: 99.938115%
2023-06-05 18:38:38.168: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=0
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=amazon-book
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=2
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=0
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using default loss
2023-06-05 18:38:47.245: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-05 18:39:06.965: 		0.00012633  	0.00024481  	0.00019827  	0.00047917  	0.00047917  
/content/model/general_recommender/SGL.py:147: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
2023-06-05 18:52:17.469: [iter 1 : loss : 7.0284 = 0.6930 + 6.3353 + 0.0000, time: 766.755463]
2023-06-05 18:52:32.816: epoch 1:	0.00258747  	0.00534926  	0.00449140  	0.00880199  	0.00945396  
2023-06-05 18:52:32.816: Found a better model.
2023-06-05 18:52:32.816: Save model to file as pretrain.
2023-06-05 19:04:50.466: [iter 2 : loss : 7.0187 = 0.6928 + 6.3259 + 0.0000, time: 713.710364]
2023-06-05 19:05:03.457: epoch 2:	0.00324576  	0.00696181  	0.00575911  	0.01101329  	0.01177167  
2023-06-05 19:05:03.457: Found a better model.
2023-06-05 19:05:03.457: Save model to file as pretrain.
2023-06-05 19:17:19.154: [iter 3 : loss : 7.0173 = 0.6925 + 6.3248 + 0.0000, time: 712.163659]
2023-06-05 19:17:32.065: epoch 3:	0.00373401  	0.00832220  	0.00688528  	0.01296328  	0.01408954  
2023-06-05 19:17:32.065: Found a better model.
2023-06-05 19:17:32.065: Save model to file as pretrain.
2023-06-05 19:29:56.598: [iter 4 : loss : 7.0161 = 0.6920 + 6.3241 + 0.0000, time: 721.401551]
2023-06-05 19:30:09.514: epoch 4:	0.00397909  	0.00907739  	0.00730604  	0.01343494  	0.01455775  
2023-06-05 19:30:09.514: Found a better model.
2023-06-05 19:30:09.514: Save model to file as pretrain.
2023-06-05 19:42:33.205: [iter 5 : loss : 7.0149 = 0.6911 + 6.3239 + 0.0000, time: 720.609056]
2023-06-05 19:42:46.202: epoch 5:	0.00252382  	0.00605521  	0.00462040  	0.00727845  	0.00799956  
2023-06-05 19:55:00.965: [iter 6 : loss : 7.0128 = 0.6889 + 6.3239 + 0.0000, time: 712.182513]
2023-06-05 19:55:14.186: epoch 6:	0.00154543  	0.00368901  	0.00292480  	0.00410270  	0.00461352  
2023-06-05 20:07:42.267: [iter 7 : loss : 7.0076 = 0.6829 + 6.3246 + 0.0000, time: 725.477800]
2023-06-05 20:07:55.099: epoch 7:	0.00217806  	0.00520365  	0.00413983  	0.00587046  	0.00668433  
2023-06-05 20:20:10.802: [iter 8 : loss : 6.9904 = 0.6629 + 6.3274 + 0.0001, time: 712.599122]
2023-06-05 20:20:24.044: epoch 8:	0.00448819  	0.01103603  	0.00884621  	0.01357439  	0.01518488  
2023-06-05 20:20:24.044: Found a better model.
2023-06-05 20:20:24.044: Save model to file as pretrain.
2023-06-05 20:32:59.322: [iter 9 : loss : 6.9399 = 0.6058 + 6.3337 + 0.0003, time: 732.027558]
2023-06-05 20:33:12.246: epoch 9:	0.00892564  	0.02138754  	0.01771173  	0.03051835  	0.03363788  
2023-06-05 20:33:12.246: Found a better model.
2023-06-05 20:33:12.246: Save model to file as pretrain.
2023-06-05 20:45:42.935: [iter 10 : loss : 6.8426 = 0.4985 + 6.3434 + 0.0007, time: 726.394130]
2023-06-05 20:45:55.829: epoch 10:	0.01514066  	0.03582222  	0.02963400  	0.05225347  	0.05788022  
2023-06-05 20:45:55.830: Found a better model.
2023-06-05 20:45:55.830: Save model to file as pretrain.
2023-06-05 20:58:47.034: [iter 11 : loss : 6.7197 = 0.3652 + 6.3532 + 0.0013, time: 747.212125]
2023-06-05 20:59:00.144: epoch 11:	0.01863750  	0.04377219  	0.03577016  	0.06265027  	0.06934518  
2023-06-05 20:59:00.144: Found a better model.
2023-06-05 20:59:00.144: Save model to file as pretrain.
2023-06-05 21:11:25.356: [iter 12 : loss : 6.6177 = 0.2580 + 6.3576 + 0.0021, time: 721.213557]
2023-06-05 21:11:38.355: epoch 12:	0.01957684  	0.04628094  	0.03748669  	0.06533013  	0.07229538  
2023-06-05 21:11:38.355: Found a better model.
2023-06-05 21:11:38.356: Save model to file as pretrain.
2023-06-05 21:24:10.587: [iter 13 : loss : 6.5457 = 0.1858 + 6.3571 + 0.0028, time: 728.386872]
2023-06-05 21:24:23.425: epoch 13:	0.01982954  	0.04730819  	0.03793852  	0.06582544  	0.07259989  
2023-06-05 21:24:23.425: Found a better model.
2023-06-05 21:24:23.426: Save model to file as pretrain.
2023-06-05 21:36:43.447: [iter 14 : loss : 6.4968 = 0.1389 + 6.3544 + 0.0035, time: 716.263398]
2023-06-05 21:36:56.604: epoch 14:	0.01998723  	0.04786469  	0.03822137  	0.06604811  	0.07297079  
2023-06-05 21:36:56.604: Found a better model.
2023-06-05 21:36:56.604: Save model to file as pretrain.
2023-06-05 21:49:16.152: [iter 15 : loss : 6.4629 = 0.1077 + 6.3511 + 0.0042, time: 716.074761]
2023-06-05 21:49:28.998: epoch 15:	0.02003948  	0.04810178  	0.03829550  	0.06614926  	0.07291793  
2023-06-05 21:49:28.998: Found a better model.
2023-06-05 21:49:28.998: Save model to file as pretrain.
2023-06-05 22:01:45.305: [iter 16 : loss : 6.4381 = 0.0857 + 6.3476 + 0.0048, time: 712.645084]
2023-06-05 22:01:58.482: epoch 16:	0.01998911  	0.04808855  	0.03821176  	0.06592918  	0.07262680  
2023-06-05 22:14:26.651: [iter 17 : loss : 6.4208 = 0.0706 + 6.3449 + 0.0053, time: 725.261144]
2023-06-05 22:14:39.543: epoch 17:	0.01992922  	0.04799104  	0.03805979  	0.06567167  	0.07222648  
2023-06-05 22:27:26.594: [iter 18 : loss : 6.4077 = 0.0593 + 6.3425 + 0.0059, time: 743.877129]
2023-06-05 22:27:39.411: epoch 18:	0.01977241  	0.04774364  	0.03782994  	0.06532227  	0.07186521  
2023-06-05 22:40:14.527: [iter 19 : loss : 6.3976 = 0.0509 + 6.3404 + 0.0064, time: 732.116046]
2023-06-05 22:40:27.199: epoch 19:	0.01965080  	0.04749914  	0.03769953  	0.06530826  	0.07186171  
2023-06-05 22:53:02.824: [iter 20 : loss : 6.3898 = 0.0442 + 6.3387 + 0.0068, time: 732.293620]
2023-06-05 22:53:15.631: epoch 20:	0.01944459  	0.04698419  	0.03734328  	0.06492693  	0.07139232  
2023-06-05 23:05:41.369: [iter 21 : loss : 6.3837 = 0.0390 + 6.3374 + 0.0072, time: 722.218041]
2023-06-05 23:05:54.191: epoch 21:	0.01927838  	0.04653936  	0.03704929  	0.06461231  	0.07113324  
2023-06-05 23:18:13.770: [iter 22 : loss : 6.3788 = 0.0350 + 6.3361 + 0.0076, time: 715.994295]
2023-06-05 23:18:26.205: epoch 22:	0.01912833  	0.04612727  	0.03674928  	0.06430103  	0.07072898  
2023-06-05 23:30:59.419: [iter 23 : loss : 6.3748 = 0.0317 + 6.3351 + 0.0080, time: 729.621829]
2023-06-05 23:31:12.249: epoch 23:	0.01892794  	0.04563902  	0.03631450  	0.06360381  	0.06991263  
2023-06-05 23:43:37.069: [iter 24 : loss : 6.3716 = 0.0290 + 6.3343 + 0.0083, time: 721.291959]
2023-06-05 23:43:50.778: epoch 24:	0.01879020  	0.04530943  	0.03605086  	0.06320038  	0.06945977  
2023-06-05 23:56:22.247: [iter 25 : loss : 6.3686 = 0.0266 + 6.3334 + 0.0087, time: 727.444186]
2023-06-05 23:56:35.156: epoch 25:	0.01859077  	0.04469511  	0.03564543  	0.06270201  	0.06888299  
2023-06-06 00:09:01.202: [iter 26 : loss : 6.3665 = 0.0247 + 6.3329 + 0.0090, time: 721.934584]
2023-06-06 00:09:13.972: epoch 26:	0.01843122  	0.04434829  	0.03533897  	0.06223183  	0.06834375  
2023-06-06 00:21:53.911: [iter 27 : loss : 6.3646 = 0.0231 + 6.3322 + 0.0092, time: 736.713450]
2023-06-06 00:22:06.767: epoch 27:	0.01825171  	0.04379208  	0.03498368  	0.06194272  	0.06784869  
2023-06-06 00:34:31.868: [iter 28 : loss : 6.3630 = 0.0218 + 6.3317 + 0.0095, time: 721.714194]
2023-06-06 00:34:44.897: epoch 28:	0.01807697  	0.04338099  	0.03466319  	0.06151168  	0.06733822  
2023-06-06 00:47:01.612: [iter 29 : loss : 6.3616 = 0.0206 + 6.3312 + 0.0098, time: 713.213729]
2023-06-06 00:47:14.212: epoch 29:	0.01796870  	0.04308498  	0.03441814  	0.06109812  	0.06687981  
2023-06-06 00:59:46.523: [iter 30 : loss : 6.3603 = 0.0195 + 6.3308 + 0.0100, time: 728.722880]
2023-06-06 00:59:59.415: epoch 30:	0.01786421  	0.04284848  	0.03415610  	0.06060708  	0.06634451  
2023-06-06 01:12:16.802: [iter 31 : loss : 6.3592 = 0.0186 + 6.3304 + 0.0102, time: 713.791597]
2023-06-06 01:12:29.932: epoch 31:	0.01765907  	0.04232990  	0.03384443  	0.06031485  	0.06598664  
2023-06-06 01:24:57.368: [iter 32 : loss : 6.3582 = 0.0177 + 6.3301 + 0.0104, time: 723.611309]
2023-06-06 01:25:10.331: epoch 32:	0.01761253  	0.04226699  	0.03369771  	0.05993870  	0.06562627  
2023-06-06 01:37:36.575: [iter 33 : loss : 6.3575 = 0.0170 + 6.3299 + 0.0106, time: 722.529055]
2023-06-06 01:37:49.108: epoch 33:	0.01742923  	0.04177012  	0.03335116  	0.05956166  	0.06518961  