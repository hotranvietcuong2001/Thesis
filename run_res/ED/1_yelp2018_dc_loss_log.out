seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-01 10:58:02.786: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-01 10:58:02.786: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=0.5
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=1
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=1
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using decoupled loss
2023-06-01 10:58:06.289: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-01 10:58:09.208: 		0.00028736  	0.00051617  	0.00041172  	0.00101716  	0.00101612  
2023-06-01 10:59:42.608: [iter 1 : loss : 1.8204 = 0.6931 + 1.1273 + 0.0000, time: 90.887551]
2023-06-01 10:59:45.434: epoch 1:	0.00169886  	0.00342905  	0.00279727  	0.00612482  	0.00629961  
2023-06-01 10:59:45.434: Found a better model.
2023-06-01 10:59:45.434: Save model to file as pretrain.
2023-06-01 11:01:18.450: [iter 2 : loss : 1.8186 = 0.6930 + 1.1256 + 0.0000, time: 90.072461]
2023-06-01 11:01:21.267: epoch 2:	0.00272990  	0.00574819  	0.00464964  	0.00989042  	0.01035283  
2023-06-01 11:01:21.267: Found a better model.
2023-06-01 11:01:21.267: Save model to file as pretrain.
2023-06-01 11:02:54.377: [iter 3 : loss : 1.8183 = 0.6929 + 1.1254 + 0.0000, time: 90.194352]
2023-06-01 11:02:57.226: epoch 3:	0.00361885  	0.00749950  	0.00606739  	0.01297920  	0.01358172  
2023-06-01 11:02:57.226: Found a better model.
2023-06-01 11:02:57.226: Save model to file as pretrain.
2023-06-01 11:04:30.249: [iter 4 : loss : 1.8181 = 0.6927 + 1.1254 + 0.0000, time: 90.134446]
2023-06-01 11:04:33.055: epoch 4:	0.00450148  	0.00963716  	0.00776629  	0.01620765  	0.01695139  
2023-06-01 11:04:33.055: Found a better model.
2023-06-01 11:04:33.055: Save model to file as pretrain.
2023-06-01 11:06:05.585: [iter 5 : loss : 1.8179 = 0.6925 + 1.1254 + 0.0000, time: 89.661009]
2023-06-01 11:06:08.418: epoch 5:	0.00526253  	0.01135787  	0.00912890  	0.01803631  	0.01945001  
2023-06-01 11:06:08.418: Found a better model.
2023-06-01 11:06:08.418: Save model to file as pretrain.
2023-06-01 11:07:42.118: [iter 6 : loss : 1.8173 = 0.6918 + 1.1255 + 0.0000, time: 90.839002]
2023-06-01 11:07:44.955: epoch 6:	0.00368515  	0.00847831  	0.00640467  	0.01148532  	0.01249496  
2023-06-01 11:09:17.368: [iter 7 : loss : 1.8148 = 0.6889 + 1.1259 + 0.0000, time: 90.002534]
2023-06-01 11:09:21.521: epoch 7:	0.00427882  	0.00986404  	0.00770200  	0.01366372  	0.01538381  
2023-06-01 11:10:54.560: [iter 8 : loss : 1.8064 = 0.6783 + 1.1281 + 0.0000, time: 90.531456]
2023-06-01 11:10:57.426: epoch 8:	0.00808090  	0.01848673  	0.01517872  	0.02687703  	0.03060980  
2023-06-01 11:10:57.426: Found a better model.
2023-06-01 11:10:57.426: Save model to file as pretrain.
2023-06-01 11:12:31.548: [iter 9 : loss : 1.7663 = 0.6287 + 1.1374 + 0.0002, time: 91.276155]
2023-06-01 11:12:34.934: epoch 9:	0.01327015  	0.02991040  	0.02431742  	0.04341865  	0.04909530  
2023-06-01 11:12:34.935: Found a better model.
2023-06-01 11:12:34.935: Save model to file as pretrain.
2023-06-01 11:14:08.461: [iter 10 : loss : 1.6838 = 0.5313 + 1.1520 + 0.0004, time: 90.650411]
2023-06-01 11:14:11.337: epoch 10:	0.02344443  	0.05119102  	0.04209113  	0.07781736  	0.08717286  
2023-06-01 11:14:11.337: Found a better model.
2023-06-01 11:14:11.337: Save model to file as pretrain.
2023-06-01 11:15:45.519: [iter 11 : loss : 1.5694 = 0.3967 + 1.1718 + 0.0009, time: 91.321448]
2023-06-01 11:15:48.336: epoch 11:	0.02682262  	0.05926681  	0.04847492  	0.08949462  	0.10016613  
2023-06-01 11:15:48.336: Found a better model.
2023-06-01 11:15:48.336: Save model to file as pretrain.
2023-06-01 11:17:21.314: [iter 12 : loss : 1.4443 = 0.2520 + 1.1908 + 0.0016, time: 90.092522]
2023-06-01 11:17:24.202: epoch 12:	0.02783456  	0.06165102  	0.05051894  	0.09312928  	0.10434243  
2023-06-01 11:17:24.202: Found a better model.
2023-06-01 11:17:24.202: Save model to file as pretrain.
2023-06-01 11:18:57.184: [iter 13 : loss : 1.3642 = 0.1715 + 1.1904 + 0.0022, time: 90.092667]
2023-06-01 11:19:00.041: epoch 13:	0.02862231  	0.06354481  	0.05206025  	0.09537948  	0.10707344  
2023-06-01 11:19:00.041: Found a better model.
2023-06-01 11:19:00.041: Save model to file as pretrain.
2023-06-01 11:20:34.325: [iter 14 : loss : 1.3171 = 0.1316 + 1.1827 + 0.0028, time: 91.469423]
2023-06-01 11:20:38.083: epoch 14:	0.02909118  	0.06471898  	0.05312149  	0.09769812  	0.10940883  
2023-06-01 11:20:38.083: Found a better model.
2023-06-01 11:20:38.083: Save model to file as pretrain.
2023-06-01 11:22:13.045: [iter 15 : loss : 1.2856 = 0.1072 + 1.1751 + 0.0033, time: 91.820706]
2023-06-01 11:22:16.563: epoch 15:	0.02940851  	0.06550875  	0.05376342  	0.09866142  	0.11054792  
2023-06-01 11:22:16.563: Found a better model.
2023-06-01 11:22:16.563: Save model to file as pretrain.
2023-06-01 11:23:51.175: [iter 16 : loss : 1.2630 = 0.0904 + 1.1689 + 0.0038, time: 91.482764]
2023-06-01 11:23:54.764: epoch 16:	0.02958690  	0.06586712  	0.05420385  	0.09953307  	0.11164721  
2023-06-01 11:23:54.764: Found a better model.
2023-06-01 11:23:54.764: Save model to file as pretrain.
2023-06-01 11:25:28.714: [iter 17 : loss : 1.2466 = 0.0786 + 1.1638 + 0.0042, time: 90.758787]
2023-06-01 11:25:32.053: epoch 17:	0.02976373  	0.06625918  	0.05454993  	0.10009710  	0.11232726  
2023-06-01 11:25:32.053: Found a better model.
2023-06-01 11:25:32.053: Save model to file as pretrain.
2023-06-01 11:27:06.313: [iter 18 : loss : 1.2333 = 0.0690 + 1.1597 + 0.0046, time: 91.123679]
2023-06-01 11:27:09.650: epoch 18:	0.02996423  	0.06672318  	0.05481718  	0.10039459  	0.11266497  
2023-06-01 11:27:09.650: Found a better model.
2023-06-01 11:27:09.650: Save model to file as pretrain.
2023-06-01 11:28:43.106: [iter 19 : loss : 1.2230 = 0.0619 + 1.1562 + 0.0050, time: 90.487149]
2023-06-01 11:28:46.853: epoch 19:	0.02992791  	0.06662264  	0.05490682  	0.10079078  	0.11320906  
2023-06-01 11:30:20.071: [iter 20 : loss : 1.2145 = 0.0559 + 1.1533 + 0.0053, time: 90.752227]
2023-06-01 11:30:23.697: epoch 20:	0.02994687  	0.06670264  	0.05500564  	0.10100093  	0.11362556  
2023-06-01 11:31:56.671: [iter 21 : loss : 1.2075 = 0.0509 + 1.1509 + 0.0057, time: 90.396727]
2023-06-01 11:31:59.578: epoch 21:	0.02997213  	0.06676180  	0.05511372  	0.10140065  	0.11405790  
2023-06-01 11:31:59.578: Found a better model.
2023-06-01 11:31:59.578: Save model to file as pretrain.
2023-06-01 11:33:33.065: [iter 22 : loss : 1.2014 = 0.0465 + 1.1489 + 0.0060, time: 90.627010]
2023-06-01 11:33:35.935: epoch 22:	0.02988372  	0.06653698  	0.05499236  	0.10148480  	0.11402741  
2023-06-01 11:35:09.110: [iter 23 : loss : 1.1964 = 0.0430 + 1.1471 + 0.0063, time: 90.784751]
2023-06-01 11:35:12.692: epoch 23:	0.02982214  	0.06643146  	0.05488946  	0.10140352  	0.11390428  
2023-06-01 11:36:45.837: [iter 24 : loss : 1.1926 = 0.0404 + 1.1456 + 0.0066, time: 90.659614]
2023-06-01 11:36:48.991: epoch 24:	0.02986320  	0.06648355  	0.05486377  	0.10132322  	0.11375310  
2023-06-01 11:38:21.884: [iter 25 : loss : 1.1888 = 0.0377 + 1.1443 + 0.0068, time: 90.291294]
2023-06-01 11:38:25.570: epoch 25:	0.02975586  	0.06614837  	0.05469982  	0.10142668  	0.11378529  
2023-06-01 11:39:56.692: [iter 26 : loss : 1.1854 = 0.0352 + 1.1431 + 0.0071, time: 88.355283]
2023-06-01 11:40:00.839: epoch 26:	0.02965955  	0.06587669  	0.05456772  	0.10119475  	0.11366116  
2023-06-01 11:41:34.854: [iter 27 : loss : 1.1826 = 0.0332 + 1.1421 + 0.0073, time: 91.230212]
2023-06-01 11:41:37.834: epoch 27:	0.02963743  	0.06577975  	0.05449772  	0.10117915  	0.11366417  
2023-06-01 11:43:11.037: [iter 28 : loss : 1.1804 = 0.0316 + 1.1412 + 0.0076, time: 90.786534]
2023-06-01 11:43:13.969: epoch 28:	0.02959320  	0.06566089  	0.05440155  	0.10111605  	0.11354004  
2023-06-01 11:44:46.570: [iter 29 : loss : 1.1783 = 0.0301 + 1.1404 + 0.0078, time: 90.181123]
2023-06-01 11:44:49.524: epoch 29:	0.02945430  	0.06526705  	0.05416296  	0.10098703  	0.11324045  
2023-06-01 11:46:23.316: [iter 30 : loss : 1.1763 = 0.0287 + 1.1396 + 0.0080, time: 91.382055]
2023-06-01 11:46:26.219: epoch 30:	0.02938171  	0.06505437  	0.05395590  	0.10062096  	0.11277173  
2023-06-01 11:47:59.386: [iter 31 : loss : 1.1746 = 0.0274 + 1.1389 + 0.0082, time: 90.756147]
2023-06-01 11:48:02.307: epoch 31:	0.02931225  	0.06491917  	0.05384284  	0.10050411  	0.11274423  
2023-06-01 11:49:35.437: [iter 32 : loss : 1.1730 = 0.0263 + 1.1383 + 0.0084, time: 90.702690]
2023-06-01 11:49:38.435: epoch 32:	0.02921594  	0.06464960  	0.05367399  	0.10026076  	0.11268938  
2023-06-01 11:51:11.770: [iter 33 : loss : 1.1718 = 0.0254 + 1.1378 + 0.0086, time: 90.909227]
2023-06-01 11:51:14.715: epoch 33:	0.02914489  	0.06447104  	0.05349538  	0.09996001  	0.11231135  
2023-06-01 11:52:47.948: [iter 34 : loss : 1.1707 = 0.0246 + 1.1373 + 0.0088, time: 90.798890]
2023-06-01 11:52:50.877: epoch 34:	0.02907229  	0.06425618  	0.05335273  	0.09983345  	0.11229605  
2023-06-01 11:54:24.219: [iter 35 : loss : 1.1691 = 0.0233 + 1.1368 + 0.0090, time: 90.922065]
2023-06-01 11:54:27.139: epoch 35:	0.02895232  	0.06388147  	0.05311894  	0.09937228  	0.11184011  
2023-06-01 11:56:00.419: [iter 36 : loss : 1.1681 = 0.0226 + 1.1364 + 0.0092, time: 90.860616]
2023-06-01 11:56:03.305: epoch 36:	0.02881020  	0.06352757  	0.05284677  	0.09904618  	0.11133807  
2023-06-01 11:57:36.510: [iter 37 : loss : 1.1673 = 0.0219 + 1.1360 + 0.0093, time: 90.753282]
2023-06-01 11:57:39.442: epoch 37:	0.02879287  	0.06336769  	0.05280442  	0.09897465  	0.11144246  
2023-06-01 11:59:13.185: [iter 38 : loss : 1.1664 = 0.0212 + 1.1357 + 0.0095, time: 91.325537]
2023-06-01 11:59:16.095: epoch 38:	0.02863816  	0.06306703  	0.05250917  	0.09844763  	0.11072169  
2023-06-01 12:00:49.240: [iter 39 : loss : 1.1655 = 0.0206 + 1.1353 + 0.0096, time: 90.713227]
2023-06-01 12:00:52.186: epoch 39:	0.02860660  	0.06288776  	0.05231339  	0.09797119  	0.11003372  
2023-06-01 12:02:25.609: [iter 40 : loss : 1.1647 = 0.0200 + 1.1350 + 0.0098, time: 90.991950]
2023-06-01 12:02:28.542: epoch 40:	0.02836980  	0.06232922  	0.05204586  	0.09794999  	0.11009372  
2023-06-01 12:04:01.190: [iter 41 : loss : 1.1643 = 0.0197 + 1.1347 + 0.0099, time: 90.197000]
2023-06-01 12:04:04.068: epoch 41:	0.02826876  	0.06205679  	0.05179381  	0.09759235  	0.10940409  
2023-06-01 12:05:36.821: [iter 42 : loss : 1.1637 = 0.0192 + 1.1345 + 0.0101, time: 90.319082]
2023-06-01 12:05:39.740: epoch 42:	0.02818825  	0.06184688  	0.05159237  	0.09715608  	0.10888928  
2023-06-01 12:07:12.434: [iter 43 : loss : 1.1630 = 0.0186 + 1.1341 + 0.0102, time: 90.264524]
2023-06-01 12:07:15.347: epoch 43:	0.02811248  	0.06165805  	0.05145585  	0.09714440  	0.10871692  
2023-06-01 12:08:48.148: [iter 44 : loss : 1.1626 = 0.0184 + 1.1339 + 0.0103, time: 90.366823]
2023-06-01 12:08:51.060: epoch 44:	0.02797672  	0.06134395  	0.05118481  	0.09661113  	0.10823473  
2023-06-01 12:10:23.051: [iter 45 : loss : 1.1619 = 0.0179 + 1.1336 + 0.0104, time: 89.553909]
2023-06-01 12:10:25.993: epoch 45:	0.02794515  	0.06125697  	0.05108965  	0.09636411  	0.10806043  
2023-06-01 12:11:59.241: [iter 46 : loss : 1.1616 = 0.0176 + 1.1335 + 0.0105, time: 90.799453]
2023-06-01 12:12:02.140: epoch 46:	0.02788199  	0.06106889  	0.05093486  	0.09612984  	0.10769856  
2023-06-01 12:13:35.309: [iter 47 : loss : 1.1611 = 0.0172 + 1.1333 + 0.0107, time: 90.734605]
2023-06-01 12:13:38.214: epoch 47:	0.02793725  	0.06112801  	0.05089496  	0.09580717  	0.10738926  
2023-06-01 12:15:11.437: [iter 48 : loss : 1.1607 = 0.0169 + 1.1331 + 0.0108, time: 90.783128]
2023-06-01 12:15:14.403: epoch 48:	0.02783466  	0.06087917  	0.05074133  	0.09571888  	0.10727715  
2023-06-01 12:16:47.354: [iter 49 : loss : 1.1602 = 0.0164 + 1.1329 + 0.0109, time: 90.508136]
2023-06-01 12:16:50.298: epoch 49:	0.02777466  	0.06068060  	0.05060379  	0.09547763  	0.10713140  
2023-06-01 12:18:23.568: [iter 50 : loss : 1.1600 = 0.0162 + 1.1327 + 0.0110, time: 90.824261]
2023-06-01 12:18:26.482: epoch 50:	0.02764360  	0.06037666  	0.05039469  	0.09528967  	0.10681497  
2023-06-01 12:19:59.263: [iter 51 : loss : 1.1595 = 0.0159 + 1.1326 + 0.0111, time: 90.321007]
2023-06-01 12:20:02.179: epoch 51:	0.02760415  	0.06030994  	0.05033116  	0.09516057  	0.10683568  
2023-06-01 12:21:35.562: [iter 52 : loss : 1.1591 = 0.0155 + 1.1324 + 0.0112, time: 90.911827]
2023-06-01 12:21:38.510: epoch 52:	0.02751576  	0.06004609  	0.05015127  	0.09492783  	0.10650975  
2023-06-01 12:23:12.169: [iter 53 : loss : 1.1590 = 0.0154 + 1.1323 + 0.0113, time: 91.184640]
2023-06-01 12:23:15.604: epoch 53:	0.02746996  	0.05987883  	0.04995591  	0.09438124  	0.10596357  
2023-06-01 12:24:48.312: [iter 54 : loss : 1.1588 = 0.0153 + 1.1322 + 0.0114, time: 90.002732]
2023-06-01 12:24:51.214: epoch 54:	0.02740838  	0.05973738  	0.04987078  	0.09438550  	0.10591730  
2023-06-01 12:26:24.513: [iter 55 : loss : 1.1584 = 0.0149 + 1.1320 + 0.0114, time: 90.864549]
2023-06-01 12:26:27.450: epoch 55:	0.02729787  	0.05952407  	0.04969453  	0.09408322  	0.10549089  
2023-06-01 12:28:00.801: [iter 56 : loss : 1.1581 = 0.0147 + 1.1319 + 0.0115, time: 90.861568]
2023-06-01 12:28:03.675: epoch 56:	0.02730261  	0.05963872  	0.04964346  	0.09380235  	0.10519598  
2023-06-01 12:29:37.222: [iter 57 : loss : 1.1580 = 0.0146 + 1.1318 + 0.0116, time: 91.088668]
2023-06-01 12:29:40.808: epoch 57:	0.02722525  	0.05935392  	0.04950200  	0.09394626  	0.10517468  
2023-06-01 12:31:15.315: [iter 58 : loss : 1.1575 = 0.0142 + 1.1317 + 0.0117, time: 91.847218]
2023-06-01 12:31:18.281: epoch 58:	0.02714001  	0.05912205  	0.04933375  	0.09346458  	0.10475269  
2023-06-01 12:32:51.553: [iter 59 : loss : 1.1575 = 0.0142 + 1.1316 + 0.0117, time: 90.803134]
2023-06-01 12:32:54.476: epoch 59:	0.02704529  	0.05898166  	0.04918473  	0.09326157  	0.10431101  
2023-06-01 12:34:28.032: [iter 60 : loss : 1.1573 = 0.0140 + 1.1315 + 0.0118, time: 91.102638]
2023-06-01 12:34:30.962: epoch 60:	0.02697897  	0.05873750  	0.04902076  	0.09308151  	0.10412408  
2023-06-01 12:36:05.017: [iter 61 : loss : 1.1569 = 0.0137 + 1.1314 + 0.0119, time: 91.592165]
2023-06-01 12:36:08.044: epoch 61:	0.02693634  	0.05852730  	0.04886022  	0.09294596  	0.10381439  
2023-06-01 12:37:41.122: [iter 62 : loss : 1.1570 = 0.0138 + 1.1312 + 0.0120, time: 90.607797]
2023-06-01 12:37:44.238: epoch 62:	0.02688267  	0.05845828  	0.04877812  	0.09301484  	0.10375547  
2023-06-01 12:39:17.518: [iter 63 : loss : 1.1567 = 0.0135 + 1.1312 + 0.0120, time: 90.795835]
2023-06-01 12:39:20.414: epoch 63:	0.02680846  	0.05827120  	0.04862367  	0.09260917  	0.10337986  
2023-06-01 12:40:53.852: [iter 64 : loss : 1.1565 = 0.0133 + 1.1311 + 0.0121, time: 90.964010]
2023-06-01 12:40:57.255: epoch 64:	0.02678637  	0.05812115  	0.04856388  	0.09264608  	0.10338808  
2023-06-01 12:42:29.590: [iter 65 : loss : 1.1565 = 0.0133 + 1.1310 + 0.0122, time: 89.544185]
2023-06-01 12:42:34.064: epoch 65:	0.02675795  	0.05801789  	0.04846357  	0.09256478  	0.10315789  
2023-06-01 12:44:14.674: [iter 66 : loss : 1.1561 = 0.0130 + 1.1309 + 0.0122, time: 97.799243]
2023-06-01 12:44:18.997: epoch 66:	0.02667114  	0.05782530  	0.04833259  	0.09241252  	0.10287374  
2023-06-01 12:45:56.590: [iter 67 : loss : 1.1561 = 0.0129 + 1.1309 + 0.0123, time: 94.738051]
2023-06-01 12:46:00.803: epoch 67:	0.02662851  	0.05775028  	0.04825784  	0.09210455  	0.10279880  
2023-06-01 12:47:45.721: [iter 68 : loss : 1.1561 = 0.0130 + 1.1308 + 0.0123, time: 102.113299]
2023-06-01 12:47:49.514: epoch 68:	0.02661272  	0.05773409  	0.04818868  	0.09200639  	0.10265295  
2023-06-01 12:49:22.932: [iter 69 : loss : 1.1558 = 0.0127 + 1.1307 + 0.0124, time: 90.626400]
2023-06-01 12:49:25.814: epoch 69:	0.02654641  	0.05762960  	0.04804743  	0.09169978  	0.10238038  
2023-06-01 12:50:59.436: [iter 70 : loss : 1.1555 = 0.0125 + 1.1306 + 0.0124, time: 91.142089]
2023-06-01 12:51:02.633: epoch 70:	0.02647380  	0.05743938  	0.04795288  	0.09182468  	0.10228115  
2023-06-01 12:52:35.850: [iter 71 : loss : 1.1555 = 0.0124 + 1.1306 + 0.0125, time: 90.403071]
2023-06-01 12:52:39.186: epoch 71:	0.02637593  	0.05712194  	0.04778708  	0.09164567  	0.10206384  
2023-06-01 12:52:39.186: Early stopping is triggered at epoch: 71
2023-06-01 12:52:39.186: best_result@epoch 21:

2023-06-01 12:52:39.186: Loading from the saved model.
2023-06-01 12:52:42.677: 		0.02997213  	0.06676180  	0.05511372  	0.10140065  	0.11405790  
/home/Thesis/model/general_recommender/SGL.py:138: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-01 12:58:14.026: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-01 12:58:14.027: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=0.5
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=1
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.3
ssl_mode=both_side
ssl_loss_type=1
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=1
save_flag=1

2023-06-01 12:58:17.514: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
0.00014     	0.00059     	0.00161     	0.00300     	0.00461     	0.00593     	0.00715     	0.00972     	0.01197     	0.02208     
2023-06-01 12:58:25.119: 		0.02997213  	0.06676180  	0.05511372  	0.10140065  	0.11405790  
2023-06-01 12:59:26.958: [iter 1 : loss : 0.0458 = 0.0398 + 0.0000 + 0.0060, time: 58.963364]
0.00007     	0.00036     	0.00105     	0.00211     	0.00354     	0.00475     	0.00622     	0.00926     	0.01217     	0.02616     
2023-06-01 12:59:34.862: epoch 1:	0.02940850  	0.06565202  	0.05392530  	0.09922723  	0.11123278  
2023-06-01 13:00:36.218: [iter 2 : loss : 0.0407 = 0.0345 + 0.0000 + 0.0063, time: 58.501671]
0.00006     	0.00032     	0.00096     	0.00195     	0.00341     	0.00453     	0.00603     	0.00912     	0.01210     	0.02665     
2023-06-01 13:00:43.788: epoch 2:	0.02912118  	0.06509589  	0.05351140  	0.09868892  	0.11079916  
2023-06-01 13:01:45.171: [iter 3 : loss : 0.0388 = 0.0324 + 0.0000 + 0.0064, time: 58.500504]
0.00005     	0.00030     	0.00096     	0.00187     	0.00331     	0.00446     	0.00594     	0.00914     	0.01222     	0.02682     
2023-06-01 13:01:52.756: epoch 3:	0.02909122  	0.06501421  	0.05343059  	0.09863732  	0.11064262  
2023-06-01 13:02:55.598: [iter 4 : loss : 0.0375 = 0.0310 + 0.0000 + 0.0065, time: 60.033025]
0.00004     	0.00028     	0.00094     	0.00184     	0.00332     	0.00447     	0.00607     	0.00923     	0.01222     	0.02667     
2023-06-01 13:03:02.252: epoch 4:	0.02908017  	0.06502858  	0.05339019  	0.09858921  	0.11052980  
2023-06-01 13:04:03.718: [iter 5 : loss : 0.0364 = 0.0298 + 0.0000 + 0.0066, time: 58.557060]
0.00005     	0.00030     	0.00093     	0.00190     	0.00336     	0.00455     	0.00613     	0.00928     	0.01223     	0.02656     
2023-06-01 13:04:11.172: epoch 5:	0.02916854  	0.06524725  	0.05347079  	0.09842800  	0.11059085  
2023-06-01 13:05:12.504: [iter 6 : loss : 0.0351 = 0.0284 + 0.0000 + 0.0067, time: 58.447429]
0.00005     	0.00032     	0.00098     	0.00193     	0.00341     	0.00454     	0.00616     	0.00919     	0.01209     	0.02636     
2023-06-01 13:05:18.862: epoch 6:	0.02908647  	0.06499391  	0.05342660  	0.09872760  	0.11074954  
2023-06-01 13:06:19.421: [iter 7 : loss : 0.0341 = 0.0274 + 0.0000 + 0.0067, time: 57.725896]
0.00005     	0.00033     	0.00098     	0.00200     	0.00339     	0.00462     	0.00623     	0.00926     	0.01213     	0.02620     
2023-06-01 13:06:27.082: epoch 7:	0.02912911  	0.06515293  	0.05351140  	0.09886758  	0.11087780  
2023-06-01 13:07:28.518: [iter 8 : loss : 0.0331 = 0.0262 + 0.0000 + 0.0068, time: 58.607961]
0.00006     	0.00033     	0.00096     	0.00202     	0.00338     	0.00468     	0.00633     	0.00929     	0.01220     	0.02597     
2023-06-01 13:07:36.068: epoch 8:	0.02913699  	0.06516296  	0.05357176  	0.09924554  	0.11125193  
2023-06-01 13:08:37.490: [iter 9 : loss : 0.0318 = 0.0249 + 0.0000 + 0.0069, time: 58.515792]
0.00005     	0.00033     	0.00097     	0.00202     	0.00346     	0.00474     	0.00632     	0.00939     	0.01219     	0.02579     
2023-06-01 13:08:43.664: epoch 9:	0.02916065  	0.06522367  	0.05359945  	0.09912824  	0.11111779  
2023-06-01 13:09:44.983: [iter 10 : loss : 0.0312 = 0.0242 + 0.0000 + 0.0069, time: 58.509274]
0.00005     	0.00031     	0.00100     	0.00200     	0.00345     	0.00468     	0.00639     	0.00946     	0.01224     	0.02559     
2023-06-01 13:09:52.570: epoch 10:	0.02913541  	0.06512723  	0.05350517  	0.09885835  	0.11077743  
2023-06-01 13:10:53.667: [iter 11 : loss : 0.0301 = 0.0231 + 0.0000 + 0.0070, time: 58.230748]
0.00005     	0.00031     	0.00104     	0.00205     	0.00351     	0.00481     	0.00648     	0.00938     	0.01218     	0.02543     
2023-06-01 13:11:01.203: epoch 11:	0.02914961  	0.06520399  	0.05357768  	0.09894381  	0.11093178  
2023-06-01 13:12:02.164: [iter 12 : loss : 0.0296 = 0.0225 + 0.0000 + 0.0071, time: 58.122855]
0.00004     	0.00030     	0.00106     	0.00205     	0.00355     	0.00477     	0.00652     	0.00942     	0.01218     	0.02532     
2023-06-01 13:12:08.651: epoch 12:	0.02914488  	0.06516416  	0.05357156  	0.09898006  	0.11096635  
2023-06-01 13:13:09.044: [iter 13 : loss : 0.0291 = 0.0220 + 0.0000 + 0.0071, time: 57.546108]
0.00004     	0.00032     	0.00103     	0.00213     	0.00356     	0.00482     	0.00654     	0.00939     	0.01209     	0.02528     
2023-06-01 13:13:16.987: epoch 13:	0.02915907  	0.06514277  	0.05355893  	0.09888913  	0.11093068  
2023-06-01 13:14:17.822: [iter 14 : loss : 0.0288 = 0.0216 + 0.0000 + 0.0072, time: 58.017284]
0.00003     	0.00031     	0.00104     	0.00215     	0.00352     	0.00487     	0.00659     	0.00939     	0.01209     	0.02518     
2023-06-01 13:14:25.389: epoch 14:	0.02911013  	0.06512918  	0.05353719  	0.09888472  	0.11096913  
2023-06-01 13:15:26.301: [iter 15 : loss : 0.0281 = 0.0209 + 0.0000 + 0.0072, time: 58.145278]
0.00005     	0.00033     	0.00105     	0.00216     	0.00354     	0.00487     	0.00667     	0.00944     	0.01206     	0.02500     
2023-06-01 13:15:32.529: epoch 15:	0.02913381  	0.06512559  	0.05351655  	0.09907648  	0.11085832  
2023-06-01 13:16:32.854: [iter 16 : loss : 0.0274 = 0.0201 + 0.0000 + 0.0073, time: 57.582805]
0.00005     	0.00034     	0.00107     	0.00220     	0.00360     	0.00497     	0.00666     	0.00944     	0.01212     	0.02490     
2023-06-01 13:16:40.444: epoch 16:	0.02919223  	0.06530732  	0.05359763  	0.09912070  	0.11077844  
2023-06-01 13:17:41.004: [iter 17 : loss : 0.0273 = 0.0200 + 0.0000 + 0.0073, time: 57.751830]
0.00005     	0.00032     	0.00109     	0.00227     	0.00361     	0.00499     	0.00657     	0.00941     	0.01207     	0.02477     
2023-06-01 13:17:48.471: epoch 17:	0.02914958  	0.06512942  	0.05352120  	0.09900960  	0.11082701  
2023-06-01 13:18:49.815: [iter 18 : loss : 0.0267 = 0.0193 + 0.0000 + 0.0074, time: 58.600293]
0.00006     	0.00033     	0.00108     	0.00229     	0.00363     	0.00500     	0.00660     	0.00938     	0.01211     	0.02465     
2023-06-01 13:18:56.192: epoch 18:	0.02914171  	0.06510638  	0.05350241  	0.09878727  	0.11069161  
2023-06-01 13:19:56.563: [iter 19 : loss : 0.0267 = 0.0192 + 0.0000 + 0.0075, time: 57.625875]
0.00006     	0.00034     	0.00117     	0.00231     	0.00367     	0.00503     	0.00663     	0.00932     	0.01211     	0.02455     
2023-06-01 13:20:03.800: epoch 19:	0.02914961  	0.06515859  	0.05350767  	0.09882062  	0.11065887  
2023-06-01 13:21:04.346: [iter 20 : loss : 0.0263 = 0.0188 + 0.0000 + 0.0075, time: 57.762697]
0.00004     	0.00034     	0.00117     	0.00238     	0.00369     	0.00506     	0.00665     	0.00941     	0.01205     	0.02437     
2023-06-01 13:21:12.168: epoch 20:	0.02913540  	0.06512561  	0.05347991  	0.09874508  	0.11061464  
2023-06-01 13:22:12.821: [iter 21 : loss : 0.0259 = 0.0184 + 0.0000 + 0.0076, time: 57.879938]
0.00005     	0.00035     	0.00118     	0.00237     	0.00374     	0.00509     	0.00665     	0.00947     	0.01207     	0.02434     
2023-06-01 13:22:19.349: epoch 21:	0.02918590  	0.06525678  	0.05353078  	0.09861237  	0.11058368  
2023-06-01 13:23:20.028: [iter 22 : loss : 0.0275 = 0.0199 + 0.0000 + 0.0076, time: 57.970874]
0.00006     	0.00033     	0.00117     	0.00235     	0.00377     	0.00510     	0.00665     	0.00944     	0.01203     	0.02436     
2023-06-01 13:23:27.099: epoch 22:	0.02915275  	0.06521394  	0.05348298  	0.09853008  	0.11056165  
2023-06-01 13:24:27.785: [iter 23 : loss : 0.0272 = 0.0196 + 0.0000 + 0.0076, time: 57.900735]
0.00005     	0.00035     	0.00120     	0.00237     	0.00370     	0.00511     	0.00663     	0.00939     	0.01208     	0.02421     
2023-06-01 13:24:35.396: epoch 23:	0.02908802  	0.06506547  	0.05342402  	0.09854283  	0.11045437  
2023-06-01 13:25:37.228: [iter 24 : loss : 0.0274 = 0.0197 + 0.0000 + 0.0077, time: 59.032737]
0.00006     	0.00034     	0.00119     	0.00242     	0.00379     	0.00513     	0.00661     	0.00940     	0.01204     	0.02399     
2023-06-01 13:25:43.460: epoch 24:	0.02902960  	0.06493104  	0.05343790  	0.09898764  	0.11089844  
2023-06-01 13:26:43.446: [iter 25 : loss : 0.0271 = 0.0194 + 0.0000 + 0.0077, time: 57.204604]
0.00006     	0.00036     	0.00120     	0.00239     	0.00376     	0.00517     	0.00669     	0.00939     	0.01203     	0.02386     
2023-06-01 13:26:51.058: epoch 25:	0.02902803  	0.06487028  	0.05338576  	0.09861305  	0.11081905  
2023-06-01 13:27:52.404: [iter 26 : loss : 0.0266 = 0.0188 + 0.0000 + 0.0078, time: 58.563595]
0.00007     	0.00034     	0.00121     	0.00242     	0.00379     	0.00517     	0.00672     	0.00944     	0.01196     	0.02378     
2023-06-01 13:28:00.089: epoch 26:	0.02903590  	0.06485526  	0.05330366  	0.09852777  	0.11043783  
2023-06-01 13:29:01.414: [iter 27 : loss : 0.0264 = 0.0186 + 0.0000 + 0.0078, time: 58.512733]
0.00007     	0.00034     	0.00120     	0.00248     	0.00385     	0.00519     	0.00666     	0.00944     	0.01198     	0.02371     
2023-06-01 13:29:07.555: epoch 27:	0.02905170  	0.06487780  	0.05339047  	0.09895249  	0.11088538  
2023-06-01 13:30:09.212: [iter 28 : loss : 0.0264 = 0.0186 + 0.0000 + 0.0078, time: 58.946950]
0.00007     	0.00036     	0.00122     	0.00246     	0.00381     	0.00519     	0.00666     	0.00939     	0.01201     	0.02369     
2023-06-01 13:30:16.674: epoch 28:	0.02902171  	0.06481285  	0.05331749  	0.09866443  	0.11071959  
2023-06-01 13:31:17.464: [iter 29 : loss : 0.0262 = 0.0184 + 0.0000 + 0.0079, time: 58.029893]
0.00007     	0.00035     	0.00128     	0.00249     	0.00384     	0.00516     	0.00667     	0.00926     	0.01183     	0.02367     
2023-06-01 13:31:25.162: epoch 29:	0.02896961  	0.06457178  	0.05321084  	0.09856988  	0.11051016  
2023-06-01 13:32:27.427: [iter 30 : loss : 0.0260 = 0.0181 + 0.0000 + 0.0079, time: 59.525694]
0.00007     	0.00034     	0.00126     	0.00247     	0.00385     	0.00520     	0.00674     	0.00936     	0.01182     	0.02350     
2023-06-01 13:32:33.053: epoch 30:	0.02890964  	0.06455514  	0.05321615  	0.09878050  	0.11073424  
2023-06-01 13:33:32.078: [iter 31 : loss : 0.0257 = 0.0178 + 0.0000 + 0.0079, time: 56.359170]
0.00007     	0.00035     	0.00124     	0.00250     	0.00384     	0.00520     	0.00673     	0.00937     	0.01185     	0.02345     
2023-06-01 13:33:40.063: epoch 31:	0.02892699  	0.06456402  	0.05315290  	0.09854185  	0.11040127  
2023-06-01 13:34:41.004: [iter 32 : loss : 0.0256 = 0.0177 + 0.0000 + 0.0079, time: 58.192952]
0.00007     	0.00037     	0.00123     	0.00246     	0.00382     	0.00526     	0.00667     	0.00938     	0.01183     	0.02340     
2023-06-01 13:34:48.162: epoch 32:	0.02887332  	0.06443807  	0.05313564  	0.09878512  	0.11063515  
2023-06-01 13:35:47.793: [iter 33 : loss : 0.0256 = 0.0176 + 0.0000 + 0.0080, time: 56.838072]
0.00007     	0.00037     	0.00125     	0.00247     	0.00385     	0.00527     	0.00667     	0.00942     	0.01183     	0.02328     
2023-06-01 13:35:53.708: epoch 33:	0.02885752  	0.06442776  	0.05309428  	0.09879504  	0.11047600  
2023-06-01 13:36:54.480: [iter 34 : loss : 0.0256 = 0.0176 + 0.0000 + 0.0080, time: 58.070405]
0.00009     	0.00039     	0.00125     	0.00248     	0.00386     	0.00524     	0.00665     	0.00943     	0.01178     	0.02326     
2023-06-01 13:37:02.192: epoch 34:	0.02885123  	0.06439596  	0.05306331  	0.09850259  	0.11028321  
2023-06-01 13:38:02.529: [iter 35 : loss : 0.0250 = 0.0169 + 0.0000 + 0.0080, time: 57.561528]
0.00009     	0.00038     	0.00127     	0.00252     	0.00390     	0.00528     	0.00667     	0.00942     	0.01177     	0.02314     
2023-06-01 13:38:09.734: epoch 35:	0.02888913  	0.06439928  	0.05306884  	0.09845553  	0.11028793  
2023-06-01 13:39:10.567: [iter 36 : loss : 0.0248 = 0.0167 + 0.0000 + 0.0081, time: 58.063597]
0.00010     	0.00039     	0.00128     	0.00256     	0.00392     	0.00527     	0.00670     	0.00939     	0.01168     	0.02309     
2023-06-01 13:39:16.365: epoch 36:	0.02886858  	0.06432782  	0.05302615  	0.09851797  	0.11026872  
2023-06-01 13:40:17.225: [iter 37 : loss : 0.0249 = 0.0168 + 0.0000 + 0.0081, time: 58.157954]
/home/Thesis/model/general_recommender/SGL.py:138: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
Traceback (most recent call last):
  File "main.py", line 57, in <module>
    model.train_model()
  File "/home/Thesis/model/general_recommender/SGL.py", line 582, in train_model
    buf, flag = self.evaluate()
  File "/home/Thesis/model/general_recommender/SGL.py", line 623, in evaluate
    current_result, buf = self.evaluator.evaluate(self)
  File "/home/Thesis/evaluator/proxy_evaluator.py", line 108, in evaluate
    return self.evaluator.evaluate(model)
  File "/home/Thesis/evaluator/backend/cpp/uni_evaluator.py", line 183, in evaluate
    result = self.eval_score_matrix(ranking_score, test_items, self.metrics,
KeyboardInterrupt
