seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-02 10:19:53.159: Dataset name: amazon-book
The number of users: 52643
The number of items: 91599
The number of ratings: 2984108
Average actions of users: 56.69
Average actions of items: 32.58
The sparsity of the dataset: 99.938115%
2023-06-02 10:19:53.159: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=0.5
data.input.path=dataset
data.input.dataset=amazon-book
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=1
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=2
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using debiased loss
2023-06-02 10:19:59.711: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-02 10:20:14.573: 		0.00014058  	0.00022644  	0.00020147  	0.00050479  	0.00050479  
2023-06-02 10:25:59.767: [iter 1 : loss : 7.0022 = 0.6931 + 6.3092 + 0.0000, time: 339.203187]
2023-06-02 10:26:11.449: epoch 1:	0.00240698  	0.00510759  	0.00426388  	0.00798297  	0.00858530  
2023-06-02 10:26:11.449: Found a better model.
2023-06-02 10:26:11.449: Save model to file as pretrain.
2023-06-02 10:31:51.090: [iter 2 : loss : 6.9921 = 0.6929 + 6.2992 + 0.0000, time: 333.223424]
2023-06-02 10:32:02.825: epoch 2:	0.00301873  	0.00642822  	0.00541957  	0.01040285  	0.01134778  
2023-06-02 10:32:02.825: Found a better model.
2023-06-02 10:32:02.825: Save model to file as pretrain.
2023-06-02 10:37:45.437: [iter 3 : loss : 6.9907 = 0.6926 + 6.2981 + 0.0000, time: 336.179726]
2023-06-02 10:37:58.711: epoch 3:	0.00308807  	0.00644831  	0.00545289  	0.01035617  	0.01127869  
2023-06-02 10:37:58.712: Found a better model.
2023-06-02 10:37:58.712: Save model to file as pretrain.
2023-06-02 10:43:29.951: [iter 4 : loss : 6.9898 = 0.6923 + 6.2975 + 0.0000, time: 324.784677]
2023-06-02 10:43:41.992: epoch 4:	0.00315077  	0.00686309  	0.00551728  	0.01013899  	0.01087853  
2023-06-02 10:43:41.992: Found a better model.
2023-06-02 10:43:41.992: Save model to file as pretrain.
2023-06-02 10:49:23.531: [iter 5 : loss : 6.9889 = 0.6917 + 6.2972 + 0.0000, time: 335.168128]
2023-06-02 10:49:35.836: epoch 5:	0.00256847  	0.00582471  	0.00470983  	0.00818679  	0.00893567  
2023-06-02 10:55:16.290: [iter 6 : loss : 6.9877 = 0.6906 + 6.2971 + 0.0000, time: 334.710769]
2023-06-02 10:55:28.006: epoch 6:	0.00145899  	0.00340657  	0.00267907  	0.00401676  	0.00444014  
2023-06-02 11:01:10.373: [iter 7 : loss : 6.9856 = 0.6880 + 6.2975 + 0.0000, time: 336.553384]
2023-06-02 11:01:23.488: epoch 7:	0.00133741  	0.00313806  	0.00250903  	0.00334372  	0.00378763  
2023-06-02 11:07:05.236: [iter 8 : loss : 6.9802 = 0.6813 + 6.2988 + 0.0001, time: 336.025080]
2023-06-02 11:07:18.477: epoch 8:	0.00229109  	0.00541095  	0.00426674  	0.00614298  	0.00686574  
2023-06-02 11:12:59.419: [iter 9 : loss : 6.9625 = 0.6593 + 6.3031 + 0.0001, time: 335.182782]
2023-06-02 11:13:11.472: epoch 9:	0.00423647  	0.01023263  	0.00807166  	0.01222246  	0.01342522  
2023-06-02 11:13:11.472: Found a better model.
2023-06-02 11:13:11.472: Save model to file as pretrain.
2023-06-02 11:18:53.651: [iter 10 : loss : 6.9141 = 0.6030 + 6.3108 + 0.0003, time: 335.791645]
2023-06-02 11:19:06.660: epoch 10:	0.00855426  	0.02031670  	0.01647994  	0.02769561  	0.03053576  
2023-06-02 11:19:06.660: Found a better model.
2023-06-02 11:19:06.660: Save model to file as pretrain.
2023-06-02 11:24:47.501: [iter 11 : loss : 6.8185 = 0.4962 + 6.3216 + 0.0007, time: 334.444270]
2023-06-02 11:24:58.877: epoch 11:	0.01458694  	0.03431316  	0.02813682  	0.04952412  	0.05440215  
2023-06-02 11:24:58.878: Found a better model.
2023-06-02 11:24:58.878: Save model to file as pretrain.
2023-06-02 11:30:40.538: [iter 12 : loss : 6.6995 = 0.3667 + 6.3314 + 0.0014, time: 335.312748]
2023-06-02 11:30:53.930: epoch 12:	0.01800785  	0.04218687  	0.03435953  	0.06034524  	0.06641586  
2023-06-02 11:30:53.930: Found a better model.
2023-06-02 11:30:53.930: Save model to file as pretrain.
2023-06-02 11:36:36.190: [iter 13 : loss : 6.5998 = 0.2625 + 6.3353 + 0.0021, time: 335.802214]
2023-06-02 11:36:49.421: epoch 13:	0.01931370  	0.04556699  	0.03666550  	0.06388044  	0.07042416  
2023-06-02 11:36:49.421: Found a better model.
2023-06-02 11:36:49.421: Save model to file as pretrain.
2023-06-02 11:42:31.988: [iter 14 : loss : 6.5284 = 0.1907 + 6.3349 + 0.0028, time: 336.212746]
2023-06-02 11:42:43.840: epoch 14:	0.01973168  	0.04703422  	0.03749284  	0.06502391  	0.07151587  
2023-06-02 11:42:43.840: Found a better model.
2023-06-02 11:42:43.840: Save model to file as pretrain.
2023-06-02 11:48:25.307: [iter 15 : loss : 6.4784 = 0.1429 + 6.3320 + 0.0035, time: 335.167203]
2023-06-02 11:48:38.570: epoch 15:	0.01982476  	0.04754457  	0.03777734  	0.06548057  	0.07203823  
2023-06-02 11:48:38.570: Found a better model.
2023-06-02 11:48:38.570: Save model to file as pretrain.
2023-06-02 11:54:19.623: [iter 16 : loss : 6.4436 = 0.1109 + 6.3285 + 0.0041, time: 334.657813]
2023-06-02 11:54:31.103: epoch 16:	0.01975538  	0.04754454  	0.03769789  	0.06530402  	0.07181790  
2023-06-02 12:00:14.571: [iter 17 : loss : 6.4187 = 0.0887 + 6.3253 + 0.0048, time: 337.857373]
2023-06-02 12:00:27.997: epoch 17:	0.01967272  	0.04757546  	0.03764345  	0.06528802  	0.07171309  
2023-06-02 12:00:27.998: Found a better model.
2023-06-02 12:00:27.998: Save model to file as pretrain.
2023-06-02 12:06:10.385: [iter 18 : loss : 6.4003 = 0.0726 + 6.3223 + 0.0053, time: 336.027820]
2023-06-02 12:06:22.425: epoch 18:	0.01957862  	0.04742187  	0.03746421  	0.06501884  	0.07141077  
2023-06-02 12:12:04.549: [iter 19 : loss : 6.3867 = 0.0611 + 6.3198 + 0.0059, time: 336.457096]
2023-06-02 12:12:16.579: epoch 19:	0.01954059  	0.04745416  	0.03741604  	0.06493852  	0.07126871  
2023-06-02 12:17:57.991: [iter 20 : loss : 6.3762 = 0.0522 + 6.3177 + 0.0063, time: 335.753916]
2023-06-02 12:18:11.280: epoch 20:	0.01943702  	0.04722901  	0.03725867  	0.06465498  	0.07103232  
2023-06-02 12:23:51.308: [iter 21 : loss : 6.3683 = 0.0456 + 6.3159 + 0.0068, time: 334.373069]
2023-06-02 12:24:03.194: epoch 21:	0.01926226  	0.04687697  	0.03687141  	0.06403311  	0.07012140  
2023-06-02 12:29:44.328: [iter 22 : loss : 6.3617 = 0.0402 + 6.3143 + 0.0072, time: 335.454640]
2023-06-02 12:29:58.776: epoch 22:	0.01912075  	0.04644040  	0.03653933  	0.06348521  	0.06957978  
2023-06-02 12:35:40.015: [iter 23 : loss : 6.3568 = 0.0360 + 6.3132 + 0.0076, time: 335.527574]
2023-06-02 12:35:50.687: epoch 23:	0.01896024  	0.04607645  	0.03628161  	0.06305445  	0.06920815  
2023-06-02 12:41:31.884: [iter 24 : loss : 6.3526 = 0.0325 + 6.3121 + 0.0080, time: 335.445114]
2023-06-02 12:41:46.705: epoch 24:	0.01875890  	0.04554193  	0.03586429  	0.06242529  	0.06842240  
2023-06-02 12:47:28.831: [iter 25 : loss : 6.3493 = 0.0298 + 6.3111 + 0.0083, time: 336.425223]
2023-06-02 12:47:40.415: epoch 25:	0.01856135  	0.04499767  	0.03547778  	0.06196981  	0.06781511  
2023-06-02 12:53:18.593: [iter 26 : loss : 6.3463 = 0.0273 + 6.3103 + 0.0087, time: 332.379408]
2023-06-02 12:53:30.659: epoch 26:	0.01840559  	0.04457969  	0.03516076  	0.06143490  	0.06723156  
2023-06-02 12:59:12.111: [iter 27 : loss : 6.3440 = 0.0254 + 6.3097 + 0.0090, time: 335.713144]
2023-06-02 12:59:24.372: epoch 27:	0.01826123  	0.04419587  	0.03487666  	0.06107622  	0.06681694  
2023-06-02 13:05:02.598: [iter 28 : loss : 6.3421 = 0.0238 + 6.3091 + 0.0092, time: 332.490844]
2023-06-02 13:05:14.410: epoch 28:	0.01809313  	0.04383830  	0.03464924  	0.06097222  	0.06665666  
2023-06-02 13:10:55.931: [iter 29 : loss : 6.3402 = 0.0222 + 6.3085 + 0.0095, time: 335.770508]
2023-06-02 13:11:07.652: epoch 29:	0.01790792  	0.04330540  	0.03425935  	0.06041361  	0.06595503  
2023-06-02 13:16:48.978: [iter 30 : loss : 6.3387 = 0.0210 + 6.3079 + 0.0098, time: 335.612194]
2023-06-02 13:17:00.907: epoch 30:	0.01778352  	0.04288260  	0.03397059  	0.05993519  	0.06556785  
2023-06-02 13:22:44.257: [iter 31 : loss : 6.3376 = 0.0200 + 6.3076 + 0.0100, time: 337.541189]
2023-06-02 13:22:56.203: epoch 31:	0.01762396  	0.04244336  	0.03369667  	0.05958595  	0.06523083  
2023-06-02 13:28:35.366: [iter 32 : loss : 6.3363 = 0.0190 + 6.3071 + 0.0102, time: 333.377812]
2023-06-02 13:28:48.441: epoch 32:	0.01754037  	0.04227515  	0.03356137  	0.05934079  	0.06508445  
2023-06-02 13:34:30.615: [iter 33 : loss : 6.3355 = 0.0182 + 6.3068 + 0.0104, time: 336.352836]
2023-06-02 13:34:42.800: epoch 33:	0.01737987  	0.04182445  	0.03326787  	0.05906952  	0.06473978  
2023-06-02 13:40:24.672: [iter 34 : loss : 6.3346 = 0.0174 + 6.3066 + 0.0106, time: 336.108698]
2023-06-02 13:40:37.482: epoch 34:	0.01733522  	0.04162700  	0.03310716  	0.05880268  	0.06447192  
2023-06-02 13:46:18.392: [iter 35 : loss : 6.3337 = 0.0167 + 6.3061 + 0.0108, time: 335.022578]
2023-06-02 13:46:29.374: epoch 35:	0.01719277  	0.04122231  	0.03283078  	0.05831625  	0.06407405  
2023-06-02 13:52:10.077: [iter 36 : loss : 6.3330 = 0.0161 + 6.3059 + 0.0110, time: 334.915521]
2023-06-02 13:52:22.209: epoch 36:	0.01712153  	0.04107191  	0.03265352  	0.05813365  	0.06379860  
2023-06-02 13:58:05.779: [iter 37 : loss : 6.3326 = 0.0157 + 6.3058 + 0.0111, time: 337.837686]
2023-06-02 13:58:17.655: epoch 37:	0.01698668  	0.04081454  	0.03246032  	0.05786004  	0.06346612  
2023-06-02 14:03:59.874: [iter 38 : loss : 6.3320 = 0.0152 + 6.3055 + 0.0113, time: 336.440088]
2023-06-02 14:04:11.821: epoch 38:	0.01694393  	0.04063705  	0.03227891  	0.05757560  	0.06303798  
2023-06-02 14:09:55.903: [iter 39 : loss : 6.3314 = 0.0146 + 6.3054 + 0.0114, time: 338.355552]
2023-06-02 14:10:07.840: epoch 39:	0.01681381  	0.04031377  	0.03204672  	0.05734719  	0.06273514  
2023-06-02 14:15:49.859: [iter 40 : loss : 6.3309 = 0.0144 + 6.3050 + 0.0116, time: 336.250942]
2023-06-02 14:16:01.509: epoch 40:	0.01675493  	0.04012446  	0.03189896  	0.05719291  	0.06255758  
2023-06-02 14:21:42.007: [iter 41 : loss : 6.3305 = 0.0139 + 6.3049 + 0.0117, time: 334.715637]
2023-06-02 14:21:55.337: epoch 41:	0.01665141  	0.03986782  	0.03168376  	0.05681171  	0.06218776  
2023-06-02 14:27:37.495: [iter 42 : loss : 6.3302 = 0.0135 + 6.3048 + 0.0118, time: 336.436440]
2023-06-02 14:27:49.624: epoch 42:	0.01657923  	0.03972055  	0.03156960  	0.05665654  	0.06206984  
2023-06-02 14:33:32.053: [iter 43 : loss : 6.3298 = 0.0132 + 6.3047 + 0.0119, time: 336.744178]
2023-06-02 14:33:43.969: epoch 43:	0.01649469  	0.03946828  	0.03134518  	0.05629522  	0.06168913  
2023-06-02 14:39:24.356: [iter 44 : loss : 6.3295 = 0.0130 + 6.3045 + 0.0120, time: 334.627083]
2023-06-02 14:39:36.543: epoch 44:	0.01642157  	0.03919907  	0.03122755  	0.05616346  	0.06160668  
2023-06-02 14:45:20.310: [iter 45 : loss : 6.3291 = 0.0126 + 6.3044 + 0.0121, time: 338.106810]
2023-06-02 14:45:31.888: epoch 45:	0.01629715  	0.03892971  	0.03102972  	0.05578496  	0.06124363  
2023-06-02 14:51:15.736: [iter 46 : loss : 6.3290 = 0.0125 + 6.3043 + 0.0122, time: 338.092024]
2023-06-02 14:51:27.771: epoch 46:	0.01618508  	0.03856183  	0.03086641  	0.05584471  	0.06124314  
2023-06-02 14:57:11.798: [iter 47 : loss : 6.3285 = 0.0121 + 6.3041 + 0.0123, time: 338.308911]
2023-06-02 14:57:24.353: epoch 47:	0.01613379  	0.03845785  	0.03072372  	0.05560853  	0.06089265  
2023-06-02 15:03:07.313: [iter 48 : loss : 6.3284 = 0.0120 + 6.3039 + 0.0124, time: 337.234783]
2023-06-02 15:03:18.498: epoch 48:	0.01606732  	0.03830055  	0.03061383  	0.05545704  	0.06072683  
2023-06-02 15:09:02.130: [iter 49 : loss : 6.3283 = 0.0118 + 6.3040 + 0.0125, time: 337.962868]
2023-06-02 15:09:14.191: epoch 49:	0.01602838  	0.03817916  	0.03056269  	0.05548770  	0.06080449  
2023-06-02 15:14:54.686: [iter 50 : loss : 6.3280 = 0.0116 + 6.3038 + 0.0126, time: 334.818044]
2023-06-02 15:15:06.648: epoch 50:	0.01594481  	0.03798559  	0.03044968  	0.05548146  	0.06072646  
2023-06-02 15:20:50.667: [iter 51 : loss : 6.3279 = 0.0115 + 6.3038 + 0.0127, time: 338.322799]
2023-06-02 15:21:04.014: epoch 51:	0.01585742  	0.03781632  	0.03028531  	0.05519368  	0.06037953  
2023-06-02 15:26:44.270: [iter 52 : loss : 6.3275 = 0.0112 + 6.3036 + 0.0127, time: 334.579546]
2023-06-02 15:26:57.676: epoch 52:	0.01581087  	0.03762869  	0.03021204  	0.05518498  	0.06042692  
2023-06-02 15:32:41.506: [iter 53 : loss : 6.3275 = 0.0111 + 6.3036 + 0.0128, time: 338.092474]
2023-06-02 15:32:53.354: epoch 53:	0.01572350  	0.03738230  	0.03006347  	0.05508566  	0.06032844  
2023-06-02 15:38:35.514: [iter 54 : loss : 6.3276 = 0.0110 + 6.3037 + 0.0129, time: 336.443367]
2023-06-02 15:38:48.927: epoch 54:	0.01570070  	0.03733998  	0.02996437  	0.05485035  	0.06000708  
2023-06-02 15:44:33.159: [iter 55 : loss : 6.3271 = 0.0107 + 6.3035 + 0.0129, time: 338.460491]
2023-06-02 15:44:46.207: epoch 55:	0.01569597  	0.03734437  	0.02994680  	0.05483880  	0.06011579  
2023-06-02 15:50:26.565: [iter 56 : loss : 6.3270 = 0.0107 + 6.3033 + 0.0130, time: 334.665933]
2023-06-02 15:50:38.480: epoch 56:	0.01560194  	0.03713068  	0.02980108  	0.05461290  	0.05981192  
2023-06-02 15:56:22.300: [iter 57 : loss : 6.3268 = 0.0106 + 6.3032 + 0.0130, time: 338.084760]
2023-06-02 15:56:35.420: epoch 57:	0.01556584  	0.03702253  	0.02970642  	0.05447083  	0.05963716  
2023-06-02 16:02:19.301: [iter 58 : loss : 6.3268 = 0.0104 + 6.3033 + 0.0131, time: 338.129955]
2023-06-02 16:02:31.274: epoch 58:	0.01552500  	0.03689651  	0.02960660  	0.05424628  	0.05940125  
2023-06-02 16:08:13.597: [iter 59 : loss : 6.3267 = 0.0103 + 6.3032 + 0.0132, time: 336.579975]
2023-06-02 16:08:25.295: epoch 59:	0.01550126  	0.03686615  	0.02953697  	0.05413537  	0.05926055  
2023-06-02 16:14:06.337: [iter 60 : loss : 6.3265 = 0.0101 + 6.3032 + 0.0132, time: 335.243834]
2023-06-02 16:14:18.162: epoch 60:	0.01544617  	0.03675562  	0.02944862  	0.05408157  	0.05907305  
2023-06-02 16:20:00.083: [iter 61 : loss : 6.3266 = 0.0101 + 6.3032 + 0.0132, time: 336.147820]
2023-06-02 16:20:11.826: epoch 61:	0.01545567  	0.03673831  	0.02942036  	0.05389075  	0.05897621  
2023-06-02 16:25:52.382: [iter 62 : loss : 6.3264 = 0.0100 + 6.3031 + 0.0133, time: 334.733617]
2023-06-02 16:26:04.293: epoch 62:	0.01538728  	0.03652608  	0.02935393  	0.05397582  	0.05898704  
2023-06-02 16:31:47.030: [iter 63 : loss : 6.3264 = 0.0099 + 6.3031 + 0.0133, time: 336.892362]
2023-06-02 16:31:57.138: epoch 63:	0.01535404  	0.03647279  	0.02925929  	0.05373967  	0.05878180  
2023-06-02 16:37:40.786: [iter 64 : loss : 6.3262 = 0.0097 + 6.3031 + 0.0134, time: 337.799927]
2023-06-02 16:37:52.087: epoch 64:	0.01533504  	0.03638992  	0.02920308  	0.05373638  	0.05874070  
2023-06-02 16:43:32.915: [iter 65 : loss : 6.3262 = 0.0098 + 6.3030 + 0.0134, time: 334.935735]
2023-06-02 16:43:44.736: epoch 65:	0.01527806  	0.03619616  	0.02908313  	0.05358528  	0.05853004  
2023-06-02 16:49:27.342: [iter 66 : loss : 6.3259 = 0.0096 + 6.3029 + 0.0134, time: 336.758860]
2023-06-02 16:49:39.447: epoch 66:	0.01529800  	0.03626333  	0.02909324  	0.05354863  	0.05847004  
2023-06-02 16:55:20.401: [iter 67 : loss : 6.3258 = 0.0095 + 6.3028 + 0.0135, time: 335.141747]
2023-06-02 16:55:32.109: epoch 67:	0.01522203  	0.03607431  	0.02894665  	0.05329236  	0.05824150  
2023-06-02 16:55:32.109: Early stopping is triggered at epoch: 67
2023-06-02 16:55:32.109: best_result@epoch 17:

2023-06-02 16:55:32.109: Loading from the saved model.
2023-06-02 16:55:43.978: 		0.01967272  	0.04757546  	0.03764350  	0.06528816  	0.07171323  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
