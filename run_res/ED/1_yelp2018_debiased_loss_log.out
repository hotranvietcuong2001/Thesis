seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-01 20:20:30.296: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-01 20:20:30.296: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=0.5
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=1
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=2
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using debiased loss
2023-06-01 20:20:33.989: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-01 20:20:38.547: 		0.00028578  	0.00059422  	0.00044439  	0.00098633  	0.00098831  
2023-06-01 20:22:13.854: [iter 1 : loss : 1.8082 = 0.6931 + 1.1151 + 0.0000, time: 92.447252]
2023-06-01 20:22:18.325: epoch 1:	0.00200042  	0.00406605  	0.00341162  	0.00774359  	0.00794650  
2023-06-01 20:22:18.325: Found a better model.
2023-06-01 20:22:18.325: Save model to file as pretrain.
2023-06-01 20:23:52.322: [iter 2 : loss : 1.8060 = 0.6930 + 1.1130 + 0.0000, time: 90.128448]
2023-06-01 20:23:56.481: epoch 2:	0.00306780  	0.00623939  	0.00524413  	0.01177362  	0.01237395  
2023-06-01 20:23:56.481: Found a better model.
2023-06-01 20:23:56.481: Save model to file as pretrain.
2023-06-01 20:25:30.921: [iter 3 : loss : 1.8057 = 0.6929 + 1.1128 + 0.0000, time: 90.433341]
2023-06-01 20:25:35.167: epoch 3:	0.00405305  	0.00825258  	0.00697021  	0.01516457  	0.01621406  
2023-06-01 20:25:35.167: Found a better model.
2023-06-01 20:25:35.167: Save model to file as pretrain.
2023-06-01 20:27:10.275: [iter 4 : loss : 1.8054 = 0.6927 + 1.1127 + 0.0000, time: 90.988792]
2023-06-01 20:27:14.461: epoch 4:	0.00497990  	0.01034647  	0.00866946  	0.01811907  	0.01944532  
2023-06-01 20:27:14.461: Found a better model.
2023-06-01 20:27:14.461: Save model to file as pretrain.
2023-06-01 20:28:50.899: [iter 5 : loss : 1.8051 = 0.6924 + 1.1127 + 0.0000, time: 92.563277]
2023-06-01 20:28:54.727: epoch 5:	0.00551516  	0.01181860  	0.00969142  	0.01925546  	0.02087326  
2023-06-01 20:28:54.727: Found a better model.
2023-06-01 20:28:54.727: Save model to file as pretrain.
2023-06-01 20:30:29.997: [iter 6 : loss : 1.8044 = 0.6916 + 1.1128 + 0.0000, time: 91.509452]
2023-06-01 20:30:33.179: epoch 6:	0.00400725  	0.00892103  	0.00687996  	0.01259258  	0.01374914  
2023-06-01 20:32:06.407: [iter 7 : loss : 1.8018 = 0.6885 + 1.1133 + 0.0000, time: 90.754760]
2023-06-01 20:32:10.450: epoch 7:	0.00467040  	0.01049087  	0.00823067  	0.01470238  	0.01641595  
2023-06-01 20:33:44.332: [iter 8 : loss : 1.7932 = 0.6777 + 1.1155 + 0.0000, time: 91.069031]
2023-06-01 20:33:47.945: epoch 8:	0.00815984  	0.01845657  	0.01517435  	0.02673206  	0.03064414  
2023-06-01 20:33:47.945: Found a better model.
2023-06-01 20:33:47.946: Save model to file as pretrain.
2023-06-01 20:35:23.943: [iter 9 : loss : 1.7549 = 0.6305 + 1.1242 + 0.0002, time: 92.033559]
2023-06-01 20:35:28.051: epoch 9:	0.01354799  	0.03045722  	0.02499547  	0.04479693  	0.05092058  
2023-06-01 20:35:28.051: Found a better model.
2023-06-01 20:35:28.051: Save model to file as pretrain.
2023-06-01 20:37:03.506: [iter 10 : loss : 1.6761 = 0.5373 + 1.1383 + 0.0004, time: 91.589342]
2023-06-01 20:37:07.519: epoch 10:	0.02312554  	0.05017361  	0.04159975  	0.07732004  	0.08704108  
2023-06-01 20:37:07.519: Found a better model.
2023-06-01 20:37:07.519: Save model to file as pretrain.
2023-06-01 20:38:42.430: [iter 11 : loss : 1.5669 = 0.4097 + 1.1562 + 0.0009, time: 90.786383]
2023-06-01 20:38:46.718: epoch 11:	0.02695364  	0.05957134  	0.04889333  	0.09016973  	0.10126566  
2023-06-01 20:38:46.718: Found a better model.
2023-06-01 20:38:46.718: Save model to file as pretrain.
2023-06-01 20:40:23.092: [iter 12 : loss : 1.4456 = 0.2699 + 1.1741 + 0.0015, time: 92.464771]
2023-06-01 20:40:27.408: epoch 12:	0.02802557  	0.06192472  	0.05086405  	0.09376395  	0.10503577  
2023-06-01 20:40:27.408: Found a better model.
2023-06-01 20:40:27.408: Save model to file as pretrain.
2023-06-01 20:42:02.254: [iter 13 : loss : 1.3618 = 0.1832 + 1.1765 + 0.0022, time: 91.092446]
2023-06-01 20:42:06.395: epoch 13:	0.02874864  	0.06379337  	0.05244011  	0.09667795  	0.10833915  
2023-06-01 20:42:06.395: Found a better model.
2023-06-01 20:42:06.396: Save model to file as pretrain.
2023-06-01 20:43:41.076: [iter 14 : loss : 1.3113 = 0.1383 + 1.1702 + 0.0027, time: 90.847575]
2023-06-01 20:43:45.346: epoch 14:	0.02941958  	0.06534585  	0.05372490  	0.09867874  	0.11066663  
2023-06-01 20:43:45.346: Found a better model.
2023-06-01 20:43:45.346: Save model to file as pretrain.
2023-06-01 20:45:20.141: [iter 15 : loss : 1.2778 = 0.1115 + 1.1631 + 0.0033, time: 90.935124]
2023-06-01 20:45:24.482: epoch 15:	0.02979057  	0.06622707  	0.05450619  	0.10002368  	0.11230738  
2023-06-01 20:45:24.482: Found a better model.
2023-06-01 20:45:24.482: Save model to file as pretrain.
2023-06-01 20:47:00.599: [iter 16 : loss : 1.2539 = 0.0934 + 1.1568 + 0.0037, time: 92.181265]
2023-06-01 20:47:04.343: epoch 16:	0.03001950  	0.06669097  	0.05495693  	0.10082643  	0.11318375  
2023-06-01 20:47:04.343: Found a better model.
2023-06-01 20:47:04.343: Save model to file as pretrain.
2023-06-01 20:48:39.915: [iter 17 : loss : 1.2367 = 0.0808 + 1.1517 + 0.0042, time: 91.596223]
2023-06-01 20:48:43.418: epoch 17:	0.03009843  	0.06708515  	0.05522981  	0.10132690  	0.11391444  
2023-06-01 20:48:43.418: Found a better model.
2023-06-01 20:48:43.418: Save model to file as pretrain.
2023-06-01 20:50:21.294: [iter 18 : loss : 1.2228 = 0.0707 + 1.1475 + 0.0046, time: 94.090259]
2023-06-01 20:50:25.546: epoch 18:	0.03027364  	0.06753736  	0.05548657  	0.10166935  	0.11427653  
2023-06-01 20:50:25.547: Found a better model.
2023-06-01 20:50:25.547: Save model to file as pretrain.
2023-06-01 20:52:00.988: [iter 19 : loss : 1.2120 = 0.0631 + 1.1440 + 0.0050, time: 91.534759]
2023-06-01 20:52:05.215: epoch 19:	0.03039995  	0.06784521  	0.05570447  	0.10179718  	0.11472362  
2023-06-01 20:52:05.215: Found a better model.
2023-06-01 20:52:05.215: Save model to file as pretrain.
2023-06-01 20:53:41.967: [iter 20 : loss : 1.2032 = 0.0568 + 1.1411 + 0.0053, time: 92.861409]
2023-06-01 20:53:46.059: epoch 20:	0.03042359  	0.06785671  	0.05571824  	0.10170442  	0.11492888  
2023-06-01 20:53:46.059: Found a better model.
2023-06-01 20:53:46.059: Save model to file as pretrain.
2023-06-01 20:55:22.179: [iter 21 : loss : 1.1959 = 0.0515 + 1.1386 + 0.0057, time: 92.187267]
2023-06-01 20:55:26.269: epoch 21:	0.03042675  	0.06773216  	0.05572110  	0.10172489  	0.11504649  
2023-06-01 20:57:00.661: [iter 22 : loss : 1.1896 = 0.0470 + 1.1366 + 0.0060, time: 91.659604]
2023-06-01 20:57:04.724: epoch 22:	0.03035729  	0.06744335  	0.05552221  	0.10145596  	0.11453451  
2023-06-01 20:58:39.516: [iter 23 : loss : 1.1846 = 0.0435 + 1.1348 + 0.0063, time: 92.061222]
2023-06-01 20:58:43.537: epoch 23:	0.03028939  	0.06726450  	0.05536744  	0.10125228  	0.11423451  
2023-06-01 21:00:17.525: [iter 24 : loss : 1.1806 = 0.0408 + 1.1332 + 0.0066, time: 91.258452]
2023-06-01 21:00:21.703: epoch 24:	0.03019311  	0.06705384  	0.05519296  	0.10101611  	0.11387601  
2023-06-01 21:01:56.475: [iter 25 : loss : 1.1767 = 0.0379 + 1.1319 + 0.0069, time: 92.028019]
2023-06-01 21:02:00.536: epoch 25:	0.03015525  	0.06672678  	0.05505341  	0.10070367  	0.11368047  
2023-06-01 21:03:34.924: [iter 26 : loss : 1.1733 = 0.0354 + 1.1307 + 0.0072, time: 91.631559]
2023-06-01 21:03:38.903: epoch 26:	0.03015681  	0.06673813  	0.05497967  	0.10062053  	0.11344352  
2023-06-01 21:05:13.761: [iter 27 : loss : 1.1704 = 0.0333 + 1.1297 + 0.0074, time: 92.143063]
2023-06-01 21:05:17.660: epoch 27:	0.02999738  	0.06629077  	0.05468263  	0.10032240  	0.11301342  
2023-06-01 21:06:53.289: [iter 28 : loss : 1.1681 = 0.0317 + 1.1287 + 0.0077, time: 92.905890]
2023-06-01 21:06:57.206: epoch 28:	0.02983949  	0.06600206  	0.05438158  	0.09993013  	0.11245018  
2023-06-01 21:08:32.135: [iter 29 : loss : 1.1660 = 0.0302 + 1.1279 + 0.0079, time: 92.183070]
2023-06-01 21:08:36.188: epoch 29:	0.02970058  	0.06567989  	0.05415583  	0.09979632  	0.11231299  
2023-06-01 21:10:11.888: [iter 30 : loss : 1.1640 = 0.0288 + 1.1272 + 0.0081, time: 92.958088]
2023-06-01 21:10:15.898: epoch 30:	0.02961216  	0.06535459  	0.05395827  	0.09953842  	0.11206014  
2023-06-01 21:11:50.135: [iter 31 : loss : 1.1623 = 0.0275 + 1.1265 + 0.0083, time: 91.470495]
2023-06-01 21:11:54.101: epoch 31:	0.02952848  	0.06506906  	0.05376309  	0.09931850  	0.11173955  
2023-06-01 21:13:29.571: [iter 32 : loss : 1.1608 = 0.0264 + 1.1259 + 0.0085, time: 92.764056]
2023-06-01 21:13:33.751: epoch 32:	0.02943534  	0.06479222  	0.05353905  	0.09895211  	0.11128699  
2023-06-01 21:15:08.088: [iter 33 : loss : 1.1595 = 0.0255 + 1.1253 + 0.0087, time: 91.610528]
2023-06-01 21:15:12.366: epoch 33:	0.02932482  	0.06459056  	0.05332443  	0.09858268  	0.11092969  
2023-06-01 21:16:47.843: [iter 34 : loss : 1.1583 = 0.0245 + 1.1248 + 0.0089, time: 92.707491]
2023-06-01 21:16:51.792: epoch 34:	0.02926324  	0.06433024  	0.05313973  	0.09825103  	0.11047246  
2023-06-01 21:18:26.174: [iter 35 : loss : 1.1568 = 0.0234 + 1.1243 + 0.0091, time: 91.654303]
2023-06-01 21:18:30.557: epoch 35:	0.02916694  	0.06402771  	0.05291862  	0.09814806  	0.11011993  
2023-06-01 21:20:05.378: [iter 36 : loss : 1.1557 = 0.0226 + 1.1239 + 0.0093, time: 92.110510]
2023-06-01 21:20:09.542: epoch 36:	0.02905484  	0.06374010  	0.05271339  	0.09785208  	0.10979118  
2023-06-01 21:21:43.844: [iter 37 : loss : 1.1549 = 0.0219 + 1.1235 + 0.0094, time: 91.575033]
2023-06-01 21:21:47.510: epoch 37:	0.02895223  	0.06348060  	0.05257070  	0.09777013  	0.10979389  
2023-06-01 21:23:21.873: [iter 38 : loss : 1.1541 = 0.0213 + 1.1232 + 0.0096, time: 91.599257]
2023-06-01 21:23:25.977: epoch 38:	0.02886067  	0.06322911  	0.05233737  	0.09733951  	0.10920352  
2023-06-01 21:25:00.291: [iter 39 : loss : 1.1532 = 0.0207 + 1.1228 + 0.0097, time: 91.554827]
2023-06-01 21:25:04.291: epoch 39:	0.02875017  	0.06306632  	0.05212533  	0.09677368  	0.10879675  
2023-06-01 21:26:39.245: [iter 40 : loss : 1.1523 = 0.0199 + 1.1225 + 0.0099, time: 92.243160]
2023-06-01 21:26:43.312: epoch 40:	0.02865861  	0.06277518  	0.05192556  	0.09651917  	0.10849227  
2023-06-01 21:28:17.709: [iter 41 : loss : 1.1519 = 0.0197 + 1.1222 + 0.0100, time: 91.609389]
2023-06-01 21:28:21.665: epoch 41:	0.02852285  	0.06248020  	0.05170013  	0.09622219  	0.10812497  
2023-06-01 21:29:56.620: [iter 42 : loss : 1.1511 = 0.0191 + 1.1219 + 0.0102, time: 92.220731]
2023-06-01 21:30:00.897: epoch 42:	0.02842813  	0.06221194  	0.05145108  	0.09566674  	0.10757506  
2023-06-01 21:31:35.206: [iter 43 : loss : 1.1504 = 0.0185 + 1.1216 + 0.0103, time: 91.555794]
2023-06-01 21:31:39.288: epoch 43:	0.02823238  	0.06175135  	0.05114682  	0.09537388  	0.10723472  
2023-06-01 21:33:13.555: [iter 44 : loss : 1.1501 = 0.0184 + 1.1214 + 0.0104, time: 91.497576]
2023-06-01 21:33:17.806: epoch 44:	0.02816450  	0.06154134  	0.05097628  	0.09502170  	0.10682850  
2023-06-01 21:34:51.637: [iter 45 : loss : 1.1494 = 0.0178 + 1.1211 + 0.0105, time: 91.067466]
2023-06-01 21:34:55.708: epoch 45:	0.02804925  	0.06116585  	0.05076630  	0.09496734  	0.10658422  
2023-06-01 21:36:30.573: [iter 46 : loss : 1.1492 = 0.0176 + 1.1209 + 0.0107, time: 92.156315]
2023-06-01 21:36:34.643: epoch 46:	0.02804295  	0.06110852  	0.05068141  	0.09474111  	0.10625739  
2023-06-01 21:38:09.851: [iter 47 : loss : 1.1488 = 0.0172 + 1.1208 + 0.0108, time: 92.432483]
2023-06-01 21:38:13.817: epoch 47:	0.02794824  	0.06086661  	0.05055974  	0.09467532  	0.10624499  
2023-06-01 21:39:48.905: [iter 48 : loss : 1.1482 = 0.0168 + 1.1205 + 0.0109, time: 92.266589]
2023-06-01 21:39:52.457: epoch 48:	0.02786141  	0.06063943  	0.05029140  	0.09401349  	0.10545156  
2023-06-01 21:41:26.922: [iter 49 : loss : 1.1477 = 0.0164 + 1.1203 + 0.0110, time: 91.712182]
2023-06-01 21:41:30.906: epoch 49:	0.02782984  	0.06059320  	0.05019384  	0.09383027  	0.10514192  
2023-06-01 21:43:05.915: [iter 50 : loss : 1.1475 = 0.0162 + 1.1202 + 0.0111, time: 92.274987]
2023-06-01 21:43:09.899: epoch 50:	0.02777300  	0.06043947  	0.05005521  	0.09366103  	0.10490613  
2023-06-01 21:44:44.995: [iter 51 : loss : 1.1470 = 0.0158 + 1.1200 + 0.0112, time: 92.347757]
2023-06-01 21:44:49.249: epoch 51:	0.02772248  	0.06025732  	0.04989668  	0.09344195  	0.10462604  
2023-06-01 21:46:23.654: [iter 52 : loss : 1.1468 = 0.0156 + 1.1199 + 0.0113, time: 91.579875]
2023-06-01 21:46:27.601: epoch 52:	0.02759777  	0.05996088  	0.04971020  	0.09314270  	0.10443594  
2023-06-01 21:48:01.845: [iter 53 : loss : 1.1465 = 0.0154 + 1.1198 + 0.0114, time: 91.481312]
2023-06-01 21:48:06.014: epoch 53:	0.02750145  	0.05970221  	0.04950048  	0.09283544  	0.10399809  
2023-06-01 21:49:39.903: [iter 54 : loss : 1.1462 = 0.0152 + 1.1196 + 0.0115, time: 91.140979]
2023-06-01 21:49:43.885: epoch 54:	0.02737674  	0.05938913  	0.04931442  	0.09269340  	0.10389949  
2023-06-01 21:51:19.605: [iter 55 : loss : 1.1459 = 0.0150 + 1.1194 + 0.0115, time: 92.909572]
2023-06-01 21:51:23.471: epoch 55:	0.02734359  	0.05935103  	0.04926470  	0.09252805  	0.10384994  
2023-06-01 21:52:57.882: [iter 56 : loss : 1.1457 = 0.0147 + 1.1193 + 0.0116, time: 91.658356]
2023-06-01 21:53:02.100: epoch 56:	0.02720468  	0.05900251  	0.04904581  	0.09229832  	0.10359158  
2023-06-01 21:54:36.437: [iter 57 : loss : 1.1455 = 0.0145 + 1.1192 + 0.0117, time: 91.568036]
2023-06-01 21:54:40.301: epoch 57:	0.02718417  	0.05882531  	0.04896183  	0.09209025  	0.10345543  
2023-06-01 21:56:14.759: [iter 58 : loss : 1.1450 = 0.0142 + 1.1191 + 0.0118, time: 91.710868]
2023-06-01 21:56:19.149: epoch 58:	0.02710205  	0.05861274  	0.04879249  	0.09193430  	0.10306074  
2023-06-01 21:57:54.185: [iter 59 : loss : 1.1450 = 0.0142 + 1.1190 + 0.0119, time: 92.255050]
2023-06-01 21:57:57.680: epoch 59:	0.02705155  	0.05844900  	0.04865290  	0.09142161  	0.10278799  
2023-06-01 21:59:31.369: [iter 60 : loss : 1.1448 = 0.0140 + 1.1189 + 0.0119, time: 90.921476]
2023-06-01 21:59:35.350: epoch 60:	0.02698841  	0.05824417  	0.04852059  	0.09138384  	0.10260911  
2023-06-01 22:01:11.207: [iter 61 : loss : 1.1446 = 0.0138 + 1.1188 + 0.0120, time: 93.053037]
2023-06-01 22:01:15.443: epoch 61:	0.02690634  	0.05800437  	0.04828762  	0.09088904  	0.10199932  
2023-06-01 22:02:49.760: [iter 62 : loss : 1.1444 = 0.0137 + 1.1186 + 0.0121, time: 91.534565]
2023-06-01 22:02:53.790: epoch 62:	0.02690629  	0.05805995  	0.04829603  	0.09092765  	0.10207015  
2023-06-01 22:04:28.824: [iter 63 : loss : 1.1442 = 0.0134 + 1.1186 + 0.0121, time: 92.275373]
2023-06-01 22:04:32.739: epoch 63:	0.02682422  	0.05780222  	0.04814500  	0.09068122  	0.10180370  
2023-06-01 22:06:07.125: [iter 64 : loss : 1.1441 = 0.0134 + 1.1185 + 0.0122, time: 91.578865]
2023-06-01 22:06:11.199: epoch 64:	0.02674213  	0.05763955  	0.04800082  	0.09048106  	0.10165220  
2023-06-01 22:07:45.813: [iter 65 : loss : 1.1440 = 0.0133 + 1.1184 + 0.0123, time: 91.825371]
2023-06-01 22:07:49.901: epoch 65:	0.02672794  	0.05760258  	0.04796002  	0.09029385  	0.10149235  
2023-06-01 22:09:24.257: [iter 66 : loss : 1.1436 = 0.0129 + 1.1183 + 0.0123, time: 91.531316]
2023-06-01 22:09:28.295: epoch 66:	0.02668688  	0.05755893  	0.04789933  	0.09026300  	0.10139437  
2023-06-01 22:11:03.364: [iter 67 : loss : 1.1436 = 0.0129 + 1.1183 + 0.0124, time: 92.270131]
2023-06-01 22:11:07.551: epoch 67:	0.02659531  	0.05730304  	0.04773858  	0.09019347  	0.10121883  
2023-06-01 22:12:42.086: [iter 68 : loss : 1.1435 = 0.0129 + 1.1182 + 0.0124, time: 91.723135]
2023-06-01 22:12:46.041: epoch 68:	0.02651956  	0.05713766  	0.04758335  	0.08986382  	0.10085956  
2023-06-01 22:14:19.761: [iter 69 : loss : 1.1433 = 0.0127 + 1.1181 + 0.0125, time: 90.948928]
2023-06-01 22:14:23.986: epoch 69:	0.02645009  	0.05694613  	0.04746602  	0.08980634  	0.10075256  
2023-06-01 22:15:59.193: [iter 70 : loss : 1.1430 = 0.0124 + 1.1180 + 0.0125, time: 92.431911]
2023-06-01 22:16:02.984: epoch 70:	0.02640430  	0.05678592  	0.04733827  	0.08951040  	0.10047109  
2023-06-01 22:16:02.984: Early stopping is triggered at epoch: 70
2023-06-01 22:16:02.984: best_result@epoch 20:

2023-06-01 22:16:02.985: Loading from the saved model.
2023-06-01 22:16:06.979: 		0.03042359  	0.06785671  	0.05571824  	0.10170442  	0.11492888  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-01 22:27:28.237: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-01 22:27:28.237: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=0.5
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=1
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=2
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=1
save_flag=1

2023-06-01 22:27:31.737: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
0.00017     	0.00057     	0.00155     	0.00297     	0.00467     	0.00594     	0.00706     	0.00989     	0.01222     	0.02286     
2023-06-01 22:27:39.226: 		0.03042359  	0.06785671  	0.05571824  	0.10170442  	0.11492888  
2023-06-01 22:28:41.846: [iter 1 : loss : 0.0493 = 0.0436 + 0.0000 + 0.0057, time: 59.766205]
0.00007     	0.00032     	0.00105     	0.00208     	0.00340     	0.00478     	0.00587     	0.00919     	0.01238     	0.02656     
2023-06-01 22:28:49.187: epoch 1:	0.02946059  	0.06564651  	0.05409858  	0.09941184  	0.11167258  
2023-06-01 22:29:49.628: [iter 2 : loss : 0.0432 = 0.0372 + 0.0000 + 0.0060, time: 57.588769]
0.00007     	0.00025     	0.00093     	0.00186     	0.00323     	0.00446     	0.00571     	0.00901     	0.01246     	0.02715     
2023-06-01 22:29:56.286: epoch 2:	0.02921272  	0.06508685  	0.05357518  	0.09861711  	0.11059681  
2023-06-01 22:30:57.385: [iter 3 : loss : 0.0408 = 0.0347 + 0.0000 + 0.0061, time: 58.219734]
0.00006     	0.00023     	0.00089     	0.00178     	0.00313     	0.00438     	0.00569     	0.00905     	0.01243     	0.02728     
2023-06-01 22:31:04.842: epoch 3:	0.02909273  	0.06487002  	0.05341230  	0.09850278  	0.11038089  
2023-06-01 22:32:06.428: [iter 4 : loss : 0.0393 = 0.0330 + 0.0000 + 0.0063, time: 58.673917]
0.00006     	0.00022     	0.00088     	0.00180     	0.00310     	0.00442     	0.00574     	0.00900     	0.01246     	0.02722     
2023-06-01 22:32:14.344: epoch 4:	0.02910536  	0.06486626  	0.05337850  	0.09817126  	0.11026229  
2023-06-01 22:33:16.011: [iter 5 : loss : 0.0379 = 0.0316 + 0.0000 + 0.0064, time: 58.789334]
0.00006     	0.00022     	0.00088     	0.00180     	0.00310     	0.00449     	0.00584     	0.00909     	0.01242     	0.02712     
2023-06-01 22:33:23.644: epoch 5:	0.02912747  	0.06498498  	0.05337439  	0.09824066  	0.11017556  
2023-06-01 22:34:25.356: [iter 6 : loss : 0.0365 = 0.0300 + 0.0000 + 0.0064, time: 58.854894]
0.00005     	0.00023     	0.00091     	0.00183     	0.00313     	0.00454     	0.00586     	0.00908     	0.01241     	0.02704     
2023-06-01 22:34:33.020: epoch 6:	0.02913221  	0.06503396  	0.05335935  	0.09802477  	0.11006214  
2023-06-01 22:35:34.748: [iter 7 : loss : 0.0354 = 0.0289 + 0.0000 + 0.0065, time: 58.865554]
0.00006     	0.00023     	0.00092     	0.00187     	0.00320     	0.00459     	0.00585     	0.00921     	0.01233     	0.02690     
2023-06-01 22:35:41.653: epoch 7:	0.02917325  	0.06510748  	0.05344373  	0.09826693  	0.11030215  
2023-06-01 22:36:43.321: [iter 8 : loss : 0.0343 = 0.0277 + 0.0000 + 0.0066, time: 58.831853]
0.00007     	0.00023     	0.00095     	0.00190     	0.00324     	0.00467     	0.00596     	0.00915     	0.01233     	0.02671     
2023-06-01 22:36:50.673: epoch 8:	0.02921903  	0.06517061  	0.05348173  	0.09817885  	0.11026751  
2023-06-01 22:37:52.220: [iter 9 : loss : 0.0329 = 0.0262 + 0.0000 + 0.0067, time: 58.713848]
0.00006     	0.00024     	0.00094     	0.00194     	0.00331     	0.00468     	0.00603     	0.00917     	0.01231     	0.02645     
2023-06-01 22:37:59.766: epoch 9:	0.02916061  	0.06510056  	0.05345210  	0.09824792  	0.11036395  
2023-06-01 22:39:00.797: [iter 10 : loss : 0.0322 = 0.0255 + 0.0000 + 0.0067, time: 58.194808]
0.00006     	0.00026     	0.00095     	0.00186     	0.00340     	0.00478     	0.00606     	0.00926     	0.01227     	0.02622     
2023-06-01 22:39:08.435: epoch 10:	0.02913534  	0.06506830  	0.05341655  	0.09825932  	0.11041661  
2023-06-01 22:40:10.101: [iter 11 : loss : 0.0311 = 0.0243 + 0.0000 + 0.0068, time: 58.801189]
0.00007     	0.00026     	0.00095     	0.00192     	0.00339     	0.00481     	0.00614     	0.00927     	0.01222     	0.02610     
2023-06-01 22:40:17.843: epoch 11:	0.02914639  	0.06509284  	0.05343896  	0.09840204  	0.11055183  
2023-06-01 22:41:19.546: [iter 12 : loss : 0.0304 = 0.0236 + 0.0000 + 0.0069, time: 58.882445]
0.00005     	0.00027     	0.00096     	0.00200     	0.00347     	0.00483     	0.00620     	0.00922     	0.01227     	0.02598     
2023-06-01 22:41:27.280: epoch 12:	0.02920482  	0.06519664  	0.05348153  	0.09833898  	0.11038488  
2023-06-01 22:42:28.153: [iter 13 : loss : 0.0298 = 0.0229 + 0.0000 + 0.0069, time: 58.103625]
0.00005     	0.00027     	0.00095     	0.00204     	0.00347     	0.00491     	0.00623     	0.00929     	0.01226     	0.02580     
2023-06-01 22:42:35.870: epoch 13:	0.02923795  	0.06523142  	0.05357221  	0.09874501  	0.11079968  
2023-06-01 22:43:36.880: [iter 14 : loss : 0.0294 = 0.0224 + 0.0000 + 0.0070, time: 58.170820]
0.00005     	0.00027     	0.00100     	0.00209     	0.00346     	0.00496     	0.00628     	0.00932     	0.01212     	0.02564     
2023-06-01 22:43:44.434: epoch 14:	0.02918586  	0.06515398  	0.05350186  	0.09864575  	0.11057016  
2023-06-01 22:44:45.389: [iter 15 : loss : 0.0289 = 0.0218 + 0.0000 + 0.0071, time: 58.127389]
0.00006     	0.00027     	0.00102     	0.00209     	0.00348     	0.00500     	0.00628     	0.00923     	0.01207     	0.02564     
2023-06-01 22:44:52.872: epoch 15:	0.02919060  	0.06511488  	0.05345649  	0.09838304  	0.11038076  
2023-06-01 22:45:53.943: [iter 16 : loss : 0.0280 = 0.0209 + 0.0000 + 0.0071, time: 58.232304]
0.00007     	0.00028     	0.00104     	0.00211     	0.00352     	0.00504     	0.00633     	0.00925     	0.01209     	0.02543     
2023-06-01 22:46:01.240: epoch 16:	0.02919847  	0.06511936  	0.05351418  	0.09873328  	0.11061227  
2023-06-01 22:47:02.314: [iter 17 : loss : 0.0279 = 0.0207 + 0.0000 + 0.0072, time: 58.251343]
0.00007     	0.00026     	0.00105     	0.00212     	0.00353     	0.00504     	0.00636     	0.00922     	0.01204     	0.02531     
2023-06-01 22:47:10.057: epoch 17:	0.02911796  	0.06496211  	0.05339015  	0.09856471  	0.11037437  
2023-06-01 22:48:11.518: [iter 18 : loss : 0.0273 = 0.0200 + 0.0000 + 0.0072, time: 58.632892]
0.00007     	0.00026     	0.00105     	0.00215     	0.00349     	0.00502     	0.00636     	0.00919     	0.01211     	0.02527     
2023-06-01 22:48:19.187: epoch 18:	0.02911005  	0.06493805  	0.05338569  	0.09867200  	0.11051986  
2023-06-01 22:49:21.508: [iter 19 : loss : 0.0271 = 0.0198 + 0.0000 + 0.0073, time: 59.496820]
0.00008     	0.00028     	0.00106     	0.00214     	0.00357     	0.00504     	0.00639     	0.00924     	0.01211     	0.02508     
2023-06-01 22:49:28.639: epoch 19:	0.02913057  	0.06494014  	0.05337935  	0.09857041  	0.11044876  
2023-06-01 22:50:30.107: [iter 20 : loss : 0.0267 = 0.0193 + 0.0000 + 0.0074, time: 58.665646]
0.00006     	0.00029     	0.00110     	0.00218     	0.00363     	0.00502     	0.00638     	0.00924     	0.01206     	0.02497     
2023-06-01 22:50:38.023: epoch 20:	0.02912269  	0.06488418  	0.05336195  	0.09861985  	0.11046327  
2023-06-01 22:51:39.585: [iter 21 : loss : 0.0285 = 0.0211 + 0.0000 + 0.0074, time: 58.762312]
0.00007     	0.00029     	0.00112     	0.00219     	0.00366     	0.00503     	0.00644     	0.00927     	0.01202     	0.02484     
2023-06-01 22:51:47.065: epoch 21:	0.02914637  	0.06490806  	0.05333272  	0.09832744  	0.11022425  
2023-06-01 22:52:48.592: [iter 22 : loss : 0.0279 = 0.0205 + 0.0000 + 0.0074, time: 58.709049]
0.00007     	0.00032     	0.00112     	0.00219     	0.00368     	0.00496     	0.00641     	0.00932     	0.01196     	0.02481     
2023-06-01 22:52:56.308: epoch 22:	0.02907377  	0.06481291  	0.05333139  	0.09839360  	0.11052090  
2023-06-01 22:53:57.268: [iter 23 : loss : 0.0277 = 0.0202 + 0.0000 + 0.0075, time: 58.143344]
0.00007     	0.00033     	0.00114     	0.00223     	0.00365     	0.00499     	0.00647     	0.00925     	0.01202     	0.02474     
2023-06-01 22:54:04.946: epoch 23:	0.02908798  	0.06485159  	0.05332183  	0.09830640  	0.11034252  
2023-06-01 22:55:06.617: [iter 24 : loss : 0.0278 = 0.0203 + 0.0000 + 0.0075, time: 58.814635]
0.00007     	0.00034     	0.00114     	0.00224     	0.00372     	0.00501     	0.00649     	0.00931     	0.01196     	0.02462     
2023-06-01 22:55:14.109: epoch 24:	0.02908325  	0.06485744  	0.05335047  	0.09850120  	0.11046424  
2023-06-01 22:56:15.064: [iter 25 : loss : 0.0274 = 0.0199 + 0.0000 + 0.0076, time: 58.134848]
0.00007     	0.00036     	0.00112     	0.00231     	0.00373     	0.00509     	0.00650     	0.00932     	0.01207     	0.02459     
2023-06-01 22:56:21.958: epoch 25:	0.02919376  	0.06512776  	0.05343089  	0.09832954  	0.11031082  
2023-06-01 22:57:22.925: [iter 26 : loss : 0.0269 = 0.0193 + 0.0000 + 0.0076, time: 58.154995]
0.00005     	0.00034     	0.00114     	0.00233     	0.00375     	0.00508     	0.00655     	0.00931     	0.01202     	0.02448     
2023-06-01 22:57:30.698: epoch 26:	0.02916692  	0.06502289  	0.05339529  	0.09826565  	0.11032413  
2023-06-01 22:58:31.503: [iter 27 : loss : 0.0267 = 0.0191 + 0.0000 + 0.0077, time: 58.012031]
0.00005     	0.00036     	0.00115     	0.00231     	0.00382     	0.00509     	0.00655     	0.00931     	0.01200     	0.02443     
2023-06-01 22:58:38.910: epoch 27:	0.02916377  	0.06503672  	0.05337433  	0.09822323  	0.11027211  
2023-06-01 22:59:41.174: [iter 28 : loss : 0.0268 = 0.0191 + 0.0000 + 0.0077, time: 59.468019]
0.00006     	0.00035     	0.00120     	0.00233     	0.00380     	0.00519     	0.00659     	0.00926     	0.01196     	0.02423     
2023-06-01 22:59:48.733: epoch 28:	0.02911165  	0.06491743  	0.05333475  	0.09822677  	0.11029331  
2023-06-01 23:00:49.716: [iter 29 : loss : 0.0265 = 0.0188 + 0.0000 + 0.0077, time: 58.207632]
0.00007     	0.00035     	0.00119     	0.00242     	0.00389     	0.00521     	0.00653     	0.00931     	0.01196     	0.02420     
2023-06-01 23:00:56.972: epoch 29:	0.02922692  	0.06508521  	0.05348787  	0.09852985  	0.11071439  
2023-06-01 23:01:57.940: [iter 30 : loss : 0.0263 = 0.0186 + 0.0000 + 0.0078, time: 58.201073]
0.00006     	0.00037     	0.00118     	0.00238     	0.00390     	0.00524     	0.00651     	0.00923     	0.01190     	0.02408     
2023-06-01 23:02:04.303: epoch 30:	0.02911167  	0.06479038  	0.05332085  	0.09815709  	0.11037911  
2023-06-01 23:03:05.374: [iter 31 : loss : 0.0261 = 0.0183 + 0.0000 + 0.0078, time: 58.296662]
0.00006     	0.00038     	0.00118     	0.00244     	0.00390     	0.00522     	0.00652     	0.00926     	0.01182     	0.02402     
2023-06-01 23:03:12.752: epoch 31:	0.02908484  	0.06473749  	0.05331817  	0.09842130  	0.11052739  
2023-06-01 23:04:14.084: [iter 32 : loss : 0.0259 = 0.0181 + 0.0000 + 0.0078, time: 58.591362]
0.00007     	0.00038     	0.00123     	0.00241     	0.00387     	0.00526     	0.00650     	0.00929     	0.01182     	0.02402     
2023-06-01 23:04:21.665: epoch 32:	0.02911327  	0.06481606  	0.05329884  	0.09833823  	0.11040578  
2023-06-01 23:05:22.475: [iter 33 : loss : 0.0259 = 0.0181 + 0.0000 + 0.0079, time: 58.068851]
0.00007     	0.00037     	0.00120     	0.00243     	0.00395     	0.00534     	0.00648     	0.00926     	0.01187     	0.02391     
2023-06-01 23:05:29.683: epoch 33:	0.02914168  	0.06484813  	0.05333729  	0.09842268  	0.11051448  
2023-06-01 23:06:30.471: [iter 34 : loss : 0.0258 = 0.0179 + 0.0000 + 0.0079, time: 58.045213]
0.00004     	0.00037     	0.00121     	0.00243     	0.00390     	0.00531     	0.00656     	0.00927     	0.01181     	0.02379     
2023-06-01 23:06:37.733: epoch 34:	0.02907536  	0.06464368  	0.05317302  	0.09823538  	0.11005575  
2023-06-01 23:07:39.113: [iter 35 : loss : 0.0253 = 0.0173 + 0.0000 + 0.0079, time: 58.636929]
0.00008     	0.00037     	0.00125     	0.00248     	0.00400     	0.00532     	0.00647     	0.00924     	0.01173     	0.02373     
2023-06-01 23:07:45.150: epoch 35:	0.02907537  	0.06461243  	0.05321592  	0.09817389  	0.11017416  
2023-06-01 23:08:45.782: [iter 36 : loss : 0.0250 = 0.0171 + 0.0000 + 0.0080, time: 57.893528]
0.00007     	0.00041     	0.00122     	0.00251     	0.00397     	0.00538     	0.00651     	0.00928     	0.01172     	0.02372     
2023-06-01 23:08:53.458: epoch 36:	0.02907063  	0.06475019  	0.05321858  	0.09817485  	0.11018894  
2023-06-01 23:09:54.311: [iter 37 : loss : 0.0251 = 0.0171 + 0.0000 + 0.0080, time: 58.113463]
0.00007     	0.00042     	0.00124     	0.00249     	0.00397     	0.00538     	0.00655     	0.00925     	0.01174     	0.02360     
2023-06-01 23:10:01.780: epoch 37:	0.02902485  	0.06464965  	0.05322001  	0.09834758  	0.11042921  
2023-06-01 23:11:02.582: [iter 38 : loss : 0.0249 = 0.0169 + 0.0000 + 0.0080, time: 58.083444]
0.00008     	0.00040     	0.00124     	0.00254     	0.00404     	0.00544     	0.00653     	0.00923     	0.01174     	0.02350     
2023-06-01 23:11:09.922: epoch 38:	0.02905168  	0.06470513  	0.05323296  	0.09837378  	0.11039978  
2023-06-01 23:12:11.409: [iter 39 : loss : 0.0248 = 0.0167 + 0.0000 + 0.0080, time: 58.735484]
0.00008     	0.00039     	0.00124     	0.00250     	0.00403     	0.00536     	0.00654     	0.00920     	0.01181     	0.02335     
2023-06-01 23:12:18.908: epoch 39:	0.02899486  	0.06446562  	0.05310043  	0.09814630  	0.11013427  
2023-06-01 23:13:19.007: [iter 40 : loss : 0.0244 = 0.0164 + 0.0000 + 0.0081, time: 57.351724]
0.00008     	0.00040     	0.00130     	0.00253     	0.00400     	0.00537     	0.00659     	0.00921     	0.01169     	0.02335     
2023-06-01 23:13:26.083: epoch 40:	0.02900908  	0.06447037  	0.05309374  	0.09813181  	0.11016205  
2023-06-01 23:14:26.207: [iter 41 : loss : 0.0246 = 0.0165 + 0.0000 + 0.0081, time: 57.387944]
0.00008     	0.00041     	0.00127     	0.00254     	0.00402     	0.00541     	0.00657     	0.00919     	0.01168     	0.02335     
2023-06-01 23:14:33.780: epoch 41:	0.02901067  	0.06448296  	0.05310117  	0.09814432  	0.11012175  
2023-06-01 23:15:35.236: [iter 42 : loss : 0.0243 = 0.0162 + 0.0000 + 0.0081, time: 58.708927]
0.00007     	0.00041     	0.00130     	0.00252     	0.00401     	0.00542     	0.00658     	0.00921     	0.01168     	0.02332     
2023-06-01 23:15:42.764: epoch 42:	0.02895697  	0.06448425  	0.05301946  	0.09798120  	0.10984430  
2023-06-01 23:16:43.642: [iter 43 : loss : 0.0242 = 0.0160 + 0.0000 + 0.0082, time: 58.089666]
0.00006     	0.00042     	0.00130     	0.00257     	0.00399     	0.00548     	0.00661     	0.00919     	0.01161     	0.02323     
2023-06-01 23:16:51.217: epoch 43:	0.02895381  	0.06441057  	0.05295987  	0.09777414  	0.10966164  
2023-06-01 23:17:52.076: [iter 44 : loss : 0.0245 = 0.0163 + 0.0000 + 0.0082, time: 58.103654]
0.00007     	0.00043     	0.00134     	0.00259     	0.00404     	0.00544     	0.00657     	0.00917     	0.01164     	0.02303     
2023-06-01 23:17:59.305: epoch 44:	0.02894118  	0.06429407  	0.05293288  	0.09775976  	0.10964376  
2023-06-01 23:18:59.934: [iter 45 : loss : 0.0239 = 0.0157 + 0.0000 + 0.0082, time: 57.863588]
0.00006     	0.00041     	0.00132     	0.00257     	0.00403     	0.00550     	0.00658     	0.00924     	0.01162     	0.02304     
2023-06-01 23:19:07.949: epoch 45:	0.02898696  	0.06434702  	0.05292698  	0.09775331  	0.10947555  
2023-06-01 23:20:09.465: [iter 46 : loss : 0.0240 = 0.0158 + 0.0000 + 0.0082, time: 58.768451]
0.00010     	0.00045     	0.00129     	0.00258     	0.00407     	0.00554     	0.00654     	0.00925     	0.01164     	0.02297     
2023-06-01 23:20:16.965: epoch 46:	0.02898381  	0.06436843  	0.05296813  	0.09776984  	0.10964780  
2023-06-01 23:21:18.518: [iter 47 : loss : 0.0238 = 0.0156 + 0.0000 + 0.0083, time: 58.791662]
0.00008     	0.00045     	0.00123     	0.00260     	0.00411     	0.00553     	0.00659     	0.00920     	0.01158     	0.02305     
2023-06-01 23:21:25.876: epoch 47:	0.02896328  	0.06437715  	0.05296954  	0.09784439  	0.10964415  
2023-06-01 23:22:27.353: [iter 48 : loss : 0.0237 = 0.0154 + 0.0000 + 0.0083, time: 58.715728]
0.00011     	0.00046     	0.00132     	0.00260     	0.00417     	0.00558     	0.00655     	0.00928     	0.01152     	0.02308     
2023-06-01 23:22:34.789: epoch 48:	0.02906906  	0.06461789  	0.05307773  	0.09774735  	0.10965107  
2023-06-01 23:23:35.467: [iter 49 : loss : 0.0236 = 0.0153 + 0.0000 + 0.0083, time: 57.910686]
0.00012     	0.00048     	0.00133     	0.00266     	0.00415     	0.00556     	0.00659     	0.00917     	0.01149     	0.02296     
2023-06-01 23:23:42.591: epoch 49:	0.02897909  	0.06446731  	0.05298325  	0.09759464  	0.10950210  
2023-06-01 23:24:44.008: [iter 50 : loss : 0.0236 = 0.0153 + 0.0000 + 0.0083, time: 58.673501]
0.00011     	0.00048     	0.00133     	0.00265     	0.00424     	0.00552     	0.00659     	0.00915     	0.01150     	0.02293     
2023-06-01 23:24:51.274: epoch 50:	0.02895224  	0.06446520  	0.05292939  	0.09750521  	0.10936852  
2023-06-01 23:24:51.274: Early stopping is triggered at epoch: 50
2023-06-01 23:24:51.274: best_result@epoch 0:

2023-06-01 23:24:51.274: Loading from the saved model.
0.00017     	0.00057     	0.00155     	0.00297     	0.00467     	0.00594     	0.00706     	0.00989     	0.01222     	0.02286     
2023-06-01 23:24:58.900: 		0.03042359  	0.06785671  	0.05571828  	0.10170456  	0.11492888  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
