seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-01 23:34:46.897: Dataset name: amazon-book
The number of users: 52643
The number of items: 91599
The number of ratings: 2984108
Average actions of users: 56.69
Average actions of items: 32.58
The sparsity of the dataset: 99.938115%
2023-06-01 23:34:46.898: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=0.5
data.input.path=dataset
data.input.dataset=amazon-book
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=1
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.5
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=1
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using decoupled loss
2023-06-01 23:34:53.067: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-01 23:35:06.792: 		0.00014248  	0.00026110  	0.00022095  	0.00054252  	0.00054282  
2023-06-01 23:40:51.623: [iter 1 : loss : 7.0280 = 0.6931 + 6.3349 + 0.0000, time: 338.870802]
2023-06-01 23:41:03.769: epoch 1:	0.00217521  	0.00455776  	0.00374751  	0.00690725  	0.00743395  
2023-06-01 23:41:03.770: Found a better model.
2023-06-01 23:41:03.770: Save model to file as pretrain.
2023-06-01 23:46:49.324: [iter 2 : loss : 7.0187 = 0.6929 + 6.3258 + 0.0000, time: 339.054631]
2023-06-01 23:47:01.428: epoch 2:	0.00278220  	0.00593609  	0.00496402  	0.00952924  	0.01040419  
2023-06-01 23:47:01.428: Found a better model.
2023-06-01 23:47:01.428: Save model to file as pretrain.
2023-06-01 23:52:42.010: [iter 3 : loss : 7.0175 = 0.6927 + 6.3249 + 0.0000, time: 334.112947]
2023-06-01 23:52:53.911: epoch 3:	0.00312607  	0.00651700  	0.00550599  	0.01058563  	0.01135910  
2023-06-01 23:52:53.911: Found a better model.
2023-06-01 23:52:53.911: Save model to file as pretrain.
2023-06-01 23:58:34.389: [iter 4 : loss : 7.0167 = 0.6923 + 6.3243 + 0.0000, time: 334.087173]
2023-06-01 23:58:46.320: epoch 4:	0.00314507  	0.00684414  	0.00555681  	0.01028294  	0.01111133  
2023-06-01 23:58:46.320: Found a better model.
2023-06-01 23:58:46.320: Save model to file as pretrain.
2023-06-02 00:04:29.826: [iter 5 : loss : 7.0159 = 0.6917 + 6.3242 + 0.0000, time: 337.075986]
2023-06-02 00:04:41.762: epoch 5:	0.00253902  	0.00565515  	0.00449470  	0.00760496  	0.00833623  
2023-06-02 00:10:28.118: [iter 6 : loss : 7.0146 = 0.6905 + 6.3241 + 0.0000, time: 340.631103]
2023-06-02 00:10:41.644: epoch 6:	0.00133456  	0.00316136  	0.00243784  	0.00370990  	0.00404785  
2023-06-02 00:16:21.010: [iter 7 : loss : 7.0123 = 0.6877 + 6.3246 + 0.0000, time: 333.697221]
2023-06-02 00:16:34.201: epoch 7:	0.00140580  	0.00332816  	0.00269258  	0.00371360  	0.00416427  
2023-06-02 00:22:16.321: [iter 8 : loss : 7.0062 = 0.6800 + 6.3261 + 0.0001, time: 336.389026]
2023-06-02 00:22:27.888: epoch 8:	0.00269669  	0.00646619  	0.00517855  	0.00753022  	0.00843636  
2023-06-02 00:28:07.259: [iter 9 : loss : 6.9849 = 0.6537 + 6.3310 + 0.0001, time: 333.678231]
2023-06-02 00:28:20.284: epoch 9:	0.00499827  	0.01201552  	0.00947097  	0.01442314  	0.01596021  
2023-06-02 00:28:20.285: Found a better model.
2023-06-02 00:28:20.285: Save model to file as pretrain.
2023-06-02 00:34:02.296: [iter 10 : loss : 6.9285 = 0.5887 + 6.3395 + 0.0004, time: 335.520540]
2023-06-02 00:34:14.207: epoch 10:	0.00984785  	0.02333723  	0.01912222  	0.03308517  	0.03626318  
2023-06-02 00:34:14.207: Found a better model.
2023-06-02 00:34:14.207: Save model to file as pretrain.
2023-06-02 00:39:58.324: [iter 11 : loss : 6.8240 = 0.4727 + 6.3506 + 0.0008, time: 337.679101]
2023-06-02 00:40:10.324: epoch 11:	0.01544265  	0.03623576  	0.02958661  	0.05183220  	0.05711762  
2023-06-02 00:40:10.324: Found a better model.
2023-06-02 00:40:10.324: Save model to file as pretrain.
2023-06-02 00:45:52.922: [iter 12 : loss : 6.7062 = 0.3455 + 6.3592 + 0.0015, time: 336.287536]
2023-06-02 00:46:04.888: epoch 12:	0.01839153  	0.04300413  	0.03494176  	0.06114861  	0.06748858  
2023-06-02 00:46:04.888: Found a better model.
2023-06-02 00:46:04.888: Save model to file as pretrain.
2023-06-02 00:51:46.448: [iter 13 : loss : 6.6114 = 0.2473 + 6.3619 + 0.0022, time: 335.210265]
2023-06-02 00:51:58.332: epoch 13:	0.01936117  	0.04569066  	0.03678952  	0.06406009  	0.07078856  
2023-06-02 00:51:58.332: Found a better model.
2023-06-02 00:51:58.332: Save model to file as pretrain.
2023-06-02 00:57:42.709: [iter 14 : loss : 6.5443 = 0.1804 + 6.3609 + 0.0029, time: 337.984690]
2023-06-02 00:57:55.884: epoch 14:	0.01962433  	0.04675576  	0.03735013  	0.06459817  	0.07139677  
2023-06-02 00:57:55.884: Found a better model.
2023-06-02 00:57:55.884: Save model to file as pretrain.
2023-06-02 01:03:36.176: [iter 15 : loss : 6.4974 = 0.1359 + 6.3578 + 0.0036, time: 333.949617]
2023-06-02 01:03:49.186: epoch 15:	0.01969364  	0.04719441  	0.03760515  	0.06488250  	0.07175604  
2023-06-02 01:03:49.186: Found a better model.
2023-06-02 01:03:49.186: Save model to file as pretrain.
2023-06-02 01:09:29.054: [iter 16 : loss : 6.4646 = 0.1060 + 6.3544 + 0.0043, time: 333.551744]
2023-06-02 01:09:43.596: epoch 16:	0.01972309  	0.04750776  	0.03767598  	0.06482382  	0.07157354  
2023-06-02 01:09:43.596: Found a better model.
2023-06-02 01:09:43.596: Save model to file as pretrain.
2023-06-02 01:15:24.036: [iter 17 : loss : 6.4411 = 0.0850 + 6.3512 + 0.0049, time: 334.002286]
2023-06-02 01:15:35.977: epoch 17:	0.01965085  	0.04743970  	0.03757498  	0.06478950  	0.07148335  
2023-06-02 01:21:19.289: [iter 18 : loss : 6.4236 = 0.0699 + 6.3483 + 0.0054, time: 337.609622]
2023-06-02 01:21:31.396: epoch 18:	0.01951591  	0.04719102  	0.03734681  	0.06448463  	0.07106590  
2023-06-02 01:27:10.669: [iter 19 : loss : 6.4107 = 0.0589 + 6.3458 + 0.0060, time: 333.678724]
2023-06-02 01:27:24.221: epoch 19:	0.01945415  	0.04713189  	0.03727344  	0.06439037  	0.07092798  
2023-06-02 01:33:02.552: [iter 20 : loss : 6.4008 = 0.0505 + 6.3438 + 0.0064, time: 332.576370]
2023-06-02 01:33:14.446: epoch 20:	0.01933540  	0.04686477  	0.03707492  	0.06423873  	0.07066995  
2023-06-02 01:38:56.998: [iter 21 : loss : 6.3932 = 0.0442 + 6.3421 + 0.0069, time: 336.903335]
2023-06-02 01:39:10.398: epoch 21:	0.01922903  	0.04652211  	0.03680031  	0.06379480  	0.07013169  
2023-06-02 01:44:48.420: [iter 22 : loss : 6.3870 = 0.0391 + 6.3406 + 0.0073, time: 332.337938]
2023-06-02 01:44:59.049: epoch 22:	0.01906755  	0.04607652  	0.03648911  	0.06344659  	0.06968782  
2023-06-02 01:50:40.182: [iter 23 : loss : 6.3823 = 0.0350 + 6.3396 + 0.0077, time: 335.382776]
2023-06-02 01:50:53.005: epoch 23:	0.01890990  	0.04566658  	0.03620988  	0.06318545  	0.06932840  
2023-06-02 01:56:34.619: [iter 24 : loss : 6.3783 = 0.0317 + 6.3385 + 0.0081, time: 335.927311]
2023-06-02 01:56:45.985: epoch 24:	0.01875888  	0.04531214  	0.03583349  	0.06257302  	0.06855449  
2023-06-02 02:02:24.425: [iter 25 : loss : 6.3749 = 0.0290 + 6.3375 + 0.0084, time: 332.727700]
2023-06-02 02:02:36.545: epoch 25:	0.01857938  	0.04480866  	0.03549052  	0.06211818  	0.06806996  
2023-06-02 02:08:28.809: [iter 26 : loss : 6.3723 = 0.0268 + 6.3368 + 0.0087, time: 346.545981]
2023-06-02 02:08:41.754: epoch 26:	0.01848535  	0.04452827  	0.03525016  	0.06170598  	0.06756882  
2023-06-02 02:14:22.964: [iter 27 : loss : 6.3701 = 0.0249 + 6.3362 + 0.0090, time: 335.463656]
2023-06-02 02:14:34.605: epoch 27:	0.01832009  	0.04403698  	0.03494211  	0.06128522  	0.06715085  
2023-06-02 02:20:14.517: [iter 28 : loss : 6.3683 = 0.0233 + 6.3356 + 0.0093, time: 334.251752]
2023-06-02 02:20:26.629: epoch 28:	0.01817382  	0.04366827  	0.03470005  	0.06105680  	0.06687464  
2023-06-02 02:26:06.525: [iter 29 : loss : 6.3664 = 0.0218 + 6.3350 + 0.0096, time: 334.151928]
2023-06-02 02:26:18.417: epoch 29:	0.01804276  	0.04337792  	0.03444815  	0.06077617  	0.06640379  
2023-06-02 02:31:58.790: [iter 30 : loss : 6.3649 = 0.0206 + 6.3345 + 0.0098, time: 334.634312]
2023-06-02 02:32:10.781: epoch 30:	0.01786800  	0.04289132  	0.03412577  	0.06042775  	0.06598433  
2023-06-02 02:37:50.110: [iter 31 : loss : 6.3639 = 0.0197 + 6.3342 + 0.0100, time: 333.570883]
2023-06-02 02:38:01.834: epoch 31:	0.01777589  	0.04262272  	0.03392337  	0.06005869  	0.06571386  
2023-06-02 02:43:43.589: [iter 32 : loss : 6.3626 = 0.0186 + 6.3337 + 0.0103, time: 335.936899]
2023-06-02 02:43:55.242: epoch 32:	0.01767713  	0.04236022  	0.03369202  	0.05974044  	0.06526627  
2023-06-02 02:49:36.274: [iter 33 : loss : 6.3618 = 0.0179 + 6.3335 + 0.0105, time: 335.260484]
2023-06-02 02:49:49.941: epoch 33:	0.01757836  	0.04208552  	0.03350355  	0.05950717  	0.06502564  
2023-06-02 02:55:28.326: [iter 34 : loss : 6.3610 = 0.0171 + 6.3333 + 0.0106, time: 332.643851]
2023-06-02 02:55:40.724: epoch 34:	0.01745203  	0.04173878  	0.03327142  	0.05915823  	0.06481835  
2023-06-02 03:01:22.558: [iter 35 : loss : 6.3601 = 0.0165 + 6.3328 + 0.0108, time: 335.975987]
2023-06-02 03:01:35.411: epoch 35:	0.01728012  	0.04133378  	0.03293293  	0.05871231  	0.06420233  
2023-06-02 03:07:13.858: [iter 36 : loss : 6.3594 = 0.0158 + 6.3326 + 0.0110, time: 332.699839]
2023-06-02 03:07:25.629: epoch 36:	0.01714619  	0.04096625  	0.03269181  	0.05850356  	0.06391679  
2023-06-02 03:13:08.561: [iter 37 : loss : 6.3591 = 0.0154 + 6.3325 + 0.0112, time: 337.164633]
2023-06-02 03:13:21.715: epoch 37:	0.01707877  	0.04079320  	0.03255153  	0.05824764  	0.06374410  
2023-06-02 03:19:03.543: [iter 38 : loss : 6.3585 = 0.0149 + 6.3322 + 0.0113, time: 336.085387]
2023-06-02 03:19:15.742: epoch 38:	0.01696860  	0.04045948  	0.03231507  	0.05791629  	0.06329886  
2023-06-02 03:25:02.039: [iter 39 : loss : 6.3580 = 0.0145 + 6.3321 + 0.0114, time: 340.545795]
2023-06-02 03:25:15.693: epoch 39:	0.01680335  	0.03996701  	0.03205164  	0.05770245  	0.06302450  
2023-06-02 03:30:53.883: [iter 40 : loss : 6.3574 = 0.0141 + 6.3317 + 0.0116, time: 332.465854]
2023-06-02 03:31:05.824: epoch 40:	0.01674542  	0.03988399  	0.03189017  	0.05727434  	0.06262311  
2023-06-02 03:36:47.109: [iter 41 : loss : 6.3572 = 0.0138 + 6.3316 + 0.0117, time: 335.565141]
2023-06-02 03:37:00.393: epoch 41:	0.01663904  	0.03961579  	0.03168844  	0.05706235  	0.06230563  
2023-06-02 03:42:40.745: [iter 42 : loss : 6.3568 = 0.0134 + 6.3316 + 0.0118, time: 334.584261]
2023-06-02 03:42:52.554: epoch 42:	0.01653931  	0.03937309  	0.03151057  	0.05682683  	0.06205053  
2023-06-02 03:48:32.029: [iter 43 : loss : 6.3563 = 0.0130 + 6.3314 + 0.0119, time: 333.773796]
2023-06-02 03:48:45.127: epoch 43:	0.01646333  	0.03919037  	0.03130592  	0.05644759  	0.06155326  
2023-06-02 03:54:25.334: [iter 44 : loss : 6.3560 = 0.0127 + 6.3312 + 0.0120, time: 334.495065]
2023-06-02 03:54:37.444: epoch 44:	0.01635127  	0.03896209  	0.03115202  	0.05632107  	0.06141011  
2023-06-02 04:00:18.925: [iter 45 : loss : 6.3558 = 0.0125 + 6.3311 + 0.0121, time: 335.800413]
2023-06-02 04:00:30.616: epoch 45:	0.01628099  	0.03874058  	0.03104199  	0.05632525  	0.06133473  
2023-06-02 04:06:10.603: [iter 46 : loss : 6.3556 = 0.0124 + 6.3310 + 0.0122, time: 334.309248]
2023-06-02 04:06:22.416: epoch 46:	0.01619739  	0.03851270  	0.03084998  	0.05602134  	0.06097094  
2023-06-02 04:12:02.466: [iter 47 : loss : 6.3552 = 0.0121 + 6.3308 + 0.0123, time: 334.340380]
2023-06-02 04:12:14.297: epoch 47:	0.01611003  	0.03826893  	0.03066914  	0.05574436  	0.06068673  
2023-06-02 04:17:54.623: [iter 48 : loss : 6.3550 = 0.0119 + 6.3307 + 0.0124, time: 334.566962]
2023-06-02 04:18:08.084: epoch 48:	0.01607679  	0.03817767  	0.03062036  	0.05566427  	0.06068555  
2023-06-02 04:23:49.425: [iter 49 : loss : 6.3550 = 0.0117 + 6.3308 + 0.0125, time: 335.646698]
2023-06-02 04:24:01.022: epoch 49:	0.01603026  	0.03803793  	0.03050858  	0.05559786  	0.06046952  
2023-06-02 04:29:42.894: [iter 50 : loss : 6.3546 = 0.0115 + 6.3306 + 0.0126, time: 336.164110]
2023-06-02 04:29:56.403: epoch 50:	0.01594668  	0.03785853  	0.03037150  	0.05538862  	0.06026605  
2023-06-02 04:35:36.082: [iter 51 : loss : 6.3545 = 0.0113 + 6.3305 + 0.0127, time: 334.010141]
2023-06-02 04:35:48.972: epoch 51:	0.01587735  	0.03772215  	0.03029053  	0.05539149  	0.06022252  
2023-06-02 04:41:35.137: [iter 52 : loss : 6.3543 = 0.0112 + 6.3304 + 0.0127, time: 340.389194]
2023-06-02 04:41:48.868: epoch 52:	0.01585740  	0.03763587  	0.03024414  	0.05535989  	0.06022289  
2023-06-02 04:47:30.580: [iter 53 : loss : 6.3541 = 0.0110 + 6.3304 + 0.0128, time: 335.984345]
2023-06-02 04:47:43.790: epoch 53:	0.01580611  	0.03746947  	0.03013785  	0.05521818  	0.06000882  
2023-06-02 04:53:23.919: [iter 54 : loss : 6.3542 = 0.0109 + 6.3304 + 0.0129, time: 334.408490]
2023-06-02 04:53:35.839: epoch 54:	0.01574914  	0.03735315  	0.03003408  	0.05494077  	0.05983130  
2023-06-02 04:59:17.081: [iter 55 : loss : 6.3538 = 0.0106 + 6.3302 + 0.0129, time: 335.545526]
2023-06-02 04:59:30.008: epoch 55:	0.01568835  	0.03713509  	0.02985473  	0.05471518  	0.05950918  
2023-06-02 05:05:13.446: [iter 56 : loss : 6.3537 = 0.0107 + 6.3300 + 0.0130, time: 337.678981]
2023-06-02 05:05:24.603: epoch 56:	0.01559242  	0.03697942  	0.02971985  	0.05461144  	0.05933479  
2023-06-02 05:11:04.104: [iter 57 : loss : 6.3535 = 0.0105 + 6.3300 + 0.0130, time: 333.756195]
2023-06-02 05:11:14.310: epoch 57:	0.01558862  	0.03687138  	0.02968462  	0.05457326  	0.05936844  
2023-06-02 05:16:55.730: [iter 58 : loss : 6.3535 = 0.0103 + 6.3301 + 0.0131, time: 335.654717]
2023-06-02 05:17:07.316: epoch 58:	0.01558769  	0.03688803  	0.02965561  	0.05440098  	0.05918215  
2023-06-02 05:22:48.847: [iter 59 : loss : 6.3533 = 0.0102 + 6.3300 + 0.0131, time: 335.730400]
2023-06-02 05:23:00.588: epoch 59:	0.01551835  	0.03675125  	0.02959161  	0.05438596  	0.05916172  
2023-06-02 05:28:37.593: [iter 60 : loss : 6.3532 = 0.0100 + 6.3299 + 0.0132, time: 331.241938]
2023-06-02 05:28:49.641: epoch 60:	0.01548796  	0.03669732  	0.02953553  	0.05439537  	0.05918180  
2023-06-02 05:34:32.666: [iter 61 : loss : 6.3532 = 0.0100 + 6.3300 + 0.0132, time: 337.236225]
2023-06-02 05:34:47.629: epoch 61:	0.01543668  	0.03655113  	0.02949421  	0.05439891  	0.05922926  
2023-06-02 05:40:29.511: [iter 62 : loss : 6.3530 = 0.0099 + 6.3299 + 0.0133, time: 336.041659]
2023-06-02 05:40:41.375: epoch 62:	0.01538728  	0.03642417  	0.02934984  	0.05415397  	0.05881877  
2023-06-02 05:46:22.824: [iter 63 : loss : 6.3531 = 0.0099 + 6.3299 + 0.0133, time: 335.647480]
2023-06-02 05:46:37.487: epoch 63:	0.01537875  	0.03643885  	0.02932332  	0.05395696  	0.05876329  
2023-06-02 05:52:18.908: [iter 64 : loss : 6.3529 = 0.0097 + 6.3299 + 0.0134, time: 335.614086]
2023-06-02 05:52:30.745: epoch 64:	0.01536165  	0.03635948  	0.02925774  	0.05379896  	0.05857951  
2023-06-02 05:58:12.428: [iter 65 : loss : 6.3528 = 0.0096 + 6.3298 + 0.0134, time: 335.809843]
2023-06-02 05:58:26.148: epoch 65:	0.01532651  	0.03628019  	0.02919070  	0.05370656  	0.05853277  
2023-06-02 06:04:10.850: [iter 66 : loss : 6.3526 = 0.0095 + 6.3297 + 0.0134, time: 338.947454]
2023-06-02 06:04:22.675: epoch 66:	0.01533600  	0.03631594  	0.02918537  	0.05370465  	0.05849425  
2023-06-02 06:04:22.675: Early stopping is triggered at epoch: 66
2023-06-02 06:04:22.675: best_result@epoch 16:

2023-06-02 06:04:22.675: Loading from the saved model.
2023-06-02 06:04:34.811: 		0.01972309  	0.04750776  	0.03767598  	0.06482382  	0.07157354  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
