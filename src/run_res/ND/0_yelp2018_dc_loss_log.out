seed= 2021
load saved data...
Item degree grouping...
User degree grouping...
Data loading finished
Evaluate model with cpp
2023-06-05 09:43:14.022: Dataset name: yelp2018
The number of users: 31668
The number of items: 38048
The number of ratings: 1561406
Average actions of users: 49.31
Average actions of items: 41.04
The sparsity of the dataset: 99.870412%
2023-06-05 09:43:14.022: 

NeuRec hyperparameters:
recommender=SGL
config_dir=./conf
gpu_id=1
gpu_mem=1.0
data.input.path=dataset
data.input.dataset=yelp2018
data.column.format=UI
data.convert.separator=','
user_min=0
item_min=0
splitter=given
ratio=0.8
by_time=False
metric=["Precision", "Recall", "NDCG", "MAP", "MRR"]
topk=[20]
group_view=None
rec.evaluate.neg=0
test_batch_size=128
num_thread=8
start_testing_epoch=0
proj_path=./

SGL's hyperparameters:
seed=2021
aug_type=0
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.1
ssl_temp=0.2
ssl_mode=both_side
ssl_loss_type=1
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
init_method=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=50
pretrain=0
save_flag=1

Using decoupled loss
2023-06-05 09:43:17.755: metrics:	Precision@20	Recall@20   	NDCG@20     	MAP@20      	MRR@20      
2023-06-05 09:43:22.279: 		0.00028736  	0.00055250  	0.00041900  	0.00093907  	0.00093959  
2023-06-05 09:44:48.610: [iter 1 : loss : 1.8187 = 0.6931 + 1.1256 + 0.0000, time: 85.797596]
2023-06-05 09:44:52.112: epoch 1:	0.00050051  	0.00097748  	0.00081889  	0.00195112  	0.00199475  
2023-06-05 09:44:52.112: Found a better model.
2023-06-05 09:44:52.113: Save model to file as pretrain.
2023-06-05 09:46:18.394: [iter 2 : loss : 1.8165 = 0.6931 + 1.1235 + 0.0000, time: 85.345411]
2023-06-05 09:46:22.519: epoch 2:	0.00074365  	0.00148650  	0.00124173  	0.00303391  	0.00309959  
2023-06-05 09:46:22.519: Found a better model.
2023-06-05 09:46:22.519: Save model to file as pretrain.
2023-06-05 09:47:33.596: [iter 3 : loss : 1.8162 = 0.6930 + 1.1232 + 0.0000, time: 70.149277]
2023-06-05 09:47:37.277: epoch 3:	0.00081470  	0.00160783  	0.00131236  	0.00303672  	0.00305633  
2023-06-05 09:47:37.277: Found a better model.
2023-06-05 09:47:37.277: Save model to file as pretrain.
2023-06-05 09:48:50.059: [iter 4 : loss : 1.8159 = 0.6929 + 1.1230 + 0.0000, time: 71.937932]
2023-06-05 09:48:54.118: epoch 4:	0.00089364  	0.00180408  	0.00148690  	0.00361886  	0.00364177  
2023-06-05 09:48:54.118: Found a better model.
2023-06-05 09:48:54.118: Save model to file as pretrain.
2023-06-05 09:50:20.310: [iter 5 : loss : 1.8158 = 0.6928 + 1.1230 + 0.0000, time: 85.275675]
2023-06-05 09:50:24.640: epoch 5:	0.00106258  	0.00205276  	0.00166197  	0.00391947  	0.00398604  
2023-06-05 09:50:24.643: Found a better model.
2023-06-05 09:50:24.643: Save model to file as pretrain.
2023-06-05 09:51:35.998: [iter 6 : loss : 1.8156 = 0.6926 + 1.1230 + 0.0000, time: 70.494332]
2023-06-05 09:51:40.090: epoch 6:	0.00105311  	0.00208361  	0.00166064  	0.00381092  	0.00387200  
2023-06-05 09:51:40.090: Found a better model.
2023-06-05 09:51:40.090: Save model to file as pretrain.
2023-06-05 09:52:51.396: [iter 7 : loss : 1.8153 = 0.6923 + 1.1230 + 0.0000, time: 70.450133]
2023-06-05 09:52:55.490: epoch 7:	0.00121573  	0.00241597  	0.00192369  	0.00444377  	0.00453202  
2023-06-05 09:52:55.490: Found a better model.
2023-06-05 09:52:55.490: Save model to file as pretrain.
2023-06-05 09:54:09.467: [iter 8 : loss : 1.8150 = 0.6919 + 1.1230 + 0.0000, time: 73.125796]
2023-06-05 09:54:13.530: epoch 8:	0.00153782  	0.00319494  	0.00241986  	0.00519752  	0.00530331  
2023-06-05 09:54:13.531: Found a better model.
2023-06-05 09:54:13.531: Save model to file as pretrain.
2023-06-05 09:55:27.711: [iter 9 : loss : 1.8144 = 0.6910 + 1.1233 + 0.0000, time: 73.255251]
2023-06-05 09:55:31.936: epoch 9:	0.00273938  	0.00627665  	0.00487530  	0.01052344  	0.01082612  
2023-06-05 09:55:31.936: Found a better model.
2023-06-05 09:55:31.936: Save model to file as pretrain.
2023-06-05 09:56:44.668: [iter 10 : loss : 1.8124 = 0.6884 + 1.1239 + 0.0000, time: 71.800618]
2023-06-05 09:56:48.127: epoch 10:	0.00835880  	0.01943159  	0.01568845  	0.03075375  	0.03342451  
2023-06-05 09:56:48.127: Found a better model.
2023-06-05 09:56:48.127: Save model to file as pretrain.
2023-06-05 09:57:59.697: [iter 11 : loss : 1.7975 = 0.6686 + 1.1288 + 0.0001, time: 70.711027]
2023-06-05 09:58:04.002: epoch 11:	0.01452984  	0.03179865  	0.02641794  	0.05094780  	0.05654329  
2023-06-05 09:58:04.002: Found a better model.
2023-06-05 09:58:04.002: Save model to file as pretrain.
2023-06-05 09:59:14.254: [iter 12 : loss : 1.7234 = 0.5726 + 1.1504 + 0.0004, time: 69.414314]
2023-06-05 09:59:18.186: epoch 12:	0.02046390  	0.04440138  	0.03660787  	0.07006164  	0.07748336  
2023-06-05 09:59:18.186: Found a better model.
2023-06-05 09:59:18.186: Save model to file as pretrain.
2023-06-05 10:00:31.515: [iter 13 : loss : 1.5983 = 0.4166 + 1.1808 + 0.0009, time: 72.477129]
2023-06-05 10:00:35.575: epoch 13:	0.02319806  	0.05055535  	0.04149862  	0.07842144  	0.08692533  
2023-06-05 10:00:35.576: Found a better model.
2023-06-05 10:00:35.576: Save model to file as pretrain.
2023-06-05 10:01:53.665: [iter 14 : loss : 1.4800 = 0.2773 + 1.2011 + 0.0015, time: 77.245833]
2023-06-05 10:01:57.595: epoch 14:	0.02463144  	0.05372947  	0.04421663  	0.08335799  	0.09239939  
2023-06-05 10:01:57.596: Found a better model.
2023-06-05 10:01:57.596: Save model to file as pretrain.
2023-06-05 10:03:16.210: [iter 15 : loss : 1.3990 = 0.1945 + 1.2024 + 0.0022, time: 77.748675]
2023-06-05 10:03:20.328: epoch 15:	0.02557545  	0.05604134  	0.04609390  	0.08627992  	0.09598739  
2023-06-05 10:03:20.328: Found a better model.
2023-06-05 10:03:20.328: Save model to file as pretrain.
2023-06-05 10:04:33.523: [iter 16 : loss : 1.3483 = 0.1504 + 1.1952 + 0.0027, time: 72.279219]
2023-06-05 10:04:37.625: epoch 16:	0.02615324  	0.05770995  	0.04744215  	0.08862809  	0.09880672  
2023-06-05 10:04:37.626: Found a better model.
2023-06-05 10:04:37.626: Save model to file as pretrain.
2023-06-05 10:05:50.840: [iter 17 : loss : 1.3137 = 0.1232 + 1.1873 + 0.0032, time: 72.377268]
2023-06-05 10:05:54.166: epoch 17:	0.02667735  	0.05894275  	0.04846056  	0.09049697  	0.10074180  
2023-06-05 10:05:54.166: Found a better model.
2023-06-05 10:05:54.166: Save model to file as pretrain.
2023-06-05 10:07:12.444: [iter 18 : loss : 1.2884 = 0.1044 + 1.1804 + 0.0036, time: 77.320922]
2023-06-05 10:07:16.423: epoch 18:	0.02713046  	0.06005243  	0.04938756  	0.09205858  	0.10271692  
2023-06-05 10:07:16.423: Found a better model.
2023-06-05 10:07:16.423: Save model to file as pretrain.
2023-06-05 10:08:27.118: [iter 19 : loss : 1.2694 = 0.0907 + 1.1746 + 0.0040, time: 69.853868]
2023-06-05 10:08:31.227: epoch 19:	0.02742728  	0.06073357  	0.05004838  	0.09307824  	0.10430237  
2023-06-05 10:08:31.229: Found a better model.
2023-06-05 10:08:31.229: Save model to file as pretrain.
2023-06-05 10:09:57.385: [iter 20 : loss : 1.2546 = 0.0804 + 1.1697 + 0.0044, time: 85.312172]
2023-06-05 10:10:01.413: epoch 20:	0.02761513  	0.06113766  	0.05047428  	0.09391215  	0.10525327  
2023-06-05 10:10:01.413: Found a better model.
2023-06-05 10:10:01.413: Save model to file as pretrain.
2023-06-05 10:11:26.307: [iter 21 : loss : 1.2424 = 0.0718 + 1.1658 + 0.0048, time: 83.961206]
2023-06-05 10:11:30.492: epoch 21:	0.02775564  	0.06149137  	0.05077403  	0.09466822  	0.10596611  
2023-06-05 10:11:30.492: Found a better model.
2023-06-05 10:11:30.492: Save model to file as pretrain.
2023-06-05 10:12:42.405: [iter 22 : loss : 1.2325 = 0.0650 + 1.1623 + 0.0051, time: 71.046167]
2023-06-05 10:12:46.741: epoch 22:	0.02780298  	0.06162250  	0.05096645  	0.09511063  	0.10634673  
2023-06-05 10:12:46.741: Found a better model.
2023-06-05 10:12:46.741: Save model to file as pretrain.
2023-06-05 10:14:11.247: [iter 23 : loss : 1.2242 = 0.0593 + 1.1595 + 0.0055, time: 83.660428]
2023-06-05 10:14:15.417: epoch 23:	0.02789299  	0.06176662  	0.05115562  	0.09556992  	0.10687658  
2023-06-05 10:14:15.417: Found a better model.
2023-06-05 10:14:15.417: Save model to file as pretrain.
2023-06-05 10:15:31.520: [iter 24 : loss : 1.2171 = 0.0544 + 1.1569 + 0.0058, time: 75.230137]
2023-06-05 10:15:35.307: epoch 24:	0.02798611  	0.06188634  	0.05140951  	0.09618512  	0.10770825  
2023-06-05 10:15:35.308: Found a better model.
2023-06-05 10:15:35.308: Save model to file as pretrain.
2023-06-05 10:16:47.194: [iter 25 : loss : 1.2114 = 0.0504 + 1.1549 + 0.0061, time: 70.982415]
2023-06-05 10:16:51.371: epoch 25:	0.02805086  	0.06205943  	0.05150039  	0.09615182  	0.10776503  
2023-06-05 10:16:51.372: Found a better model.
2023-06-05 10:16:51.372: Save model to file as pretrain.
2023-06-05 10:18:16.685: [iter 26 : loss : 1.2059 = 0.0466 + 1.1529 + 0.0064, time: 84.474941]
2023-06-05 10:18:20.792: epoch 26:	0.02804137  	0.06207024  	0.05136232  	0.09583023  	0.10730422  
2023-06-05 10:18:20.792: Found a better model.
2023-06-05 10:18:20.792: Save model to file as pretrain.
2023-06-05 10:19:39.258: [iter 27 : loss : 1.2018 = 0.0438 + 1.1514 + 0.0067, time: 77.609935]
2023-06-05 10:19:43.372: epoch 27:	0.02808245  	0.06212924  	0.05137302  	0.09568477  	0.10709686  
2023-06-05 10:19:43.372: Found a better model.
2023-06-05 10:19:43.372: Save model to file as pretrain.
2023-06-05 10:21:02.227: [iter 28 : loss : 1.1976 = 0.0408 + 1.1499 + 0.0069, time: 77.960643]
2023-06-05 10:21:05.749: epoch 28:	0.02812035  	0.06218058  	0.05147717  	0.09593949  	0.10741482  
2023-06-05 10:21:05.749: Found a better model.
2023-06-05 10:21:05.749: Save model to file as pretrain.
2023-06-05 10:22:31.473: [iter 29 : loss : 1.1945 = 0.0386 + 1.1488 + 0.0072, time: 84.827529]
2023-06-05 10:22:35.602: epoch 29:	0.02809669  	0.06213102  	0.05141763  	0.09584141  	0.10717559  
2023-06-05 10:23:53.193: [iter 30 : loss : 1.1915 = 0.0366 + 1.1474 + 0.0074, time: 77.050944]
2023-06-05 10:23:57.153: epoch 30:	0.02801933  	0.06193051  	0.05121385  	0.09557314  	0.10677865  
2023-06-05 10:25:08.266: [iter 31 : loss : 1.1892 = 0.0348 + 1.1467 + 0.0077, time: 70.578835]
2023-06-05 10:25:12.622: epoch 31:	0.02798933  	0.06168978  	0.05120979  	0.09583054  	0.10708985  
2023-06-05 10:26:23.778: [iter 32 : loss : 1.1865 = 0.0330 + 1.1456 + 0.0079, time: 70.594176]
2023-06-05 10:26:27.953: epoch 32:	0.02796408  	0.06155606  	0.05109668  	0.09553982  	0.10693197  
2023-06-05 10:27:38.458: [iter 33 : loss : 1.1846 = 0.0317 + 1.1448 + 0.0081, time: 69.976767]
2023-06-05 10:27:42.629: epoch 33:	0.02788831  	0.06143401  	0.05100393  	0.09561452  	0.10678689  
2023-06-05 10:29:00.382: [iter 34 : loss : 1.1825 = 0.0302 + 1.1440 + 0.0083, time: 77.237591]
2023-06-05 10:29:04.523: epoch 34:	0.02784095  	0.06113738  	0.05082458  	0.09528718  	0.10638654  
2023-06-05 10:30:15.056: [iter 35 : loss : 1.1808 = 0.0290 + 1.1433 + 0.0085, time: 69.993454]
2023-06-05 10:30:19.193: epoch 35:	0.02782834  	0.06104890  	0.05069241  	0.09484834  	0.10594936  
2023-06-05 10:31:28.720: [iter 36 : loss : 1.1795 = 0.0281 + 1.1427 + 0.0087, time: 68.991434]
2023-06-05 10:31:33.007: epoch 36:	0.02775727  	0.06097740  	0.05065683  	0.09480470  	0.10599305  
2023-06-05 10:32:42.717: [iter 37 : loss : 1.1781 = 0.0270 + 1.1422 + 0.0089, time: 69.176774]
2023-06-05 10:32:47.021: epoch 37:	0.02773991  	0.06082444  	0.05061512  	0.09507182  	0.10626867  
2023-06-05 10:33:55.499: [iter 38 : loss : 1.1767 = 0.0259 + 1.1417 + 0.0091, time: 67.930874]
2023-06-05 10:33:59.740: epoch 38:	0.02760573  	0.06055005  	0.05041564  	0.09464578  	0.10581394  
2023-06-05 10:35:08.182: [iter 39 : loss : 1.1757 = 0.0252 + 1.1412 + 0.0093, time: 67.886545]
2023-06-05 10:35:12.229: epoch 39:	0.02762941  	0.06059114  	0.05041654  	0.09473896  	0.10586829  
2023-06-05 10:36:21.190: [iter 40 : loss : 1.1749 = 0.0245 + 1.1410 + 0.0094, time: 68.441705]
2023-06-05 10:36:25.234: epoch 40:	0.02757417  	0.06046632  	0.05035269  	0.09479496  	0.10589178  
2023-06-05 10:37:34.165: [iter 41 : loss : 1.1736 = 0.0236 + 1.1404 + 0.0096, time: 68.386092]
2023-06-05 10:37:38.166: epoch 41:	0.02742575  	0.06011105  	0.05011981  	0.09460070  	0.10547912  
2023-06-05 10:38:53.801: [iter 42 : loss : 1.1727 = 0.0230 + 1.1400 + 0.0098, time: 75.087032]
2023-06-05 10:38:57.927: epoch 42:	0.02750627  	0.06011327  	0.05009896  	0.09431750  	0.10524265  
2023-06-05 10:40:09.898: [iter 43 : loss : 1.1722 = 0.0226 + 1.1398 + 0.0099, time: 71.429978]
2023-06-05 10:40:14.074: epoch 43:	0.02742103  	0.05987995  	0.05006649  	0.09458023  	0.10536076  
2023-06-05 10:41:32.333: [iter 44 : loss : 1.1711 = 0.0217 + 1.1393 + 0.0101, time: 77.724338]
2023-06-05 10:41:36.578: epoch 44:	0.02733736  	0.05961539  	0.04995272  	0.09467833  	0.10544930  
2023-06-05 10:42:47.961: [iter 45 : loss : 1.1706 = 0.0214 + 1.1390 + 0.0102, time: 70.830123]
2023-06-05 10:42:52.186: epoch 45:	0.02730580  	0.05949055  	0.04986798  	0.09439974  	0.10532738  
2023-06-05 10:44:02.888: [iter 46 : loss : 1.1698 = 0.0208 + 1.1387 + 0.0103, time: 70.150743]
2023-06-05 10:44:07.143: epoch 46:	0.02724105  	0.05941515  	0.04975496  	0.09430993  	0.10496933  
2023-06-05 10:45:17.943: [iter 47 : loss : 1.1691 = 0.0202 + 1.1384 + 0.0105, time: 70.288022]
2023-06-05 10:45:21.957: epoch 47:	0.02712109  	0.05908887  	0.04956692  	0.09407197  	0.10484106  
2023-06-05 10:46:30.672: [iter 48 : loss : 1.1687 = 0.0198 + 1.1383 + 0.0106, time: 68.184147]
2023-06-05 10:46:34.695: epoch 48:	0.02701375  	0.05872517  	0.04932805  	0.09363557  	0.10433386  
2023-06-05 10:47:59.282: [iter 49 : loss : 1.1684 = 0.0195 + 1.1381 + 0.0107, time: 84.069946]
2023-06-05 10:48:03.396: epoch 49:	0.02693324  	0.05859704  	0.04919971  	0.09345697  	0.10406759  
2023-06-05 10:49:13.844: [iter 50 : loss : 1.1679 = 0.0193 + 1.1378 + 0.0108, time: 69.905866]
2023-06-05 10:49:18.123: epoch 50:	0.02689850  	0.05837437  	0.04910990  	0.09335776  	0.10412086  
2023-06-05 10:50:27.428: [iter 51 : loss : 1.1674 = 0.0189 + 1.1376 + 0.0110, time: 68.772603]
2023-06-05 10:50:31.784: epoch 51:	0.02687326  	0.05819255  	0.04902904  	0.09319794  	0.10392216  
2023-06-05 10:51:41.541: [iter 52 : loss : 1.1671 = 0.0186 + 1.1374 + 0.0111, time: 69.239417]
2023-06-05 10:51:45.679: epoch 52:	0.02685273  	0.05808488  	0.04898517  	0.09323713  	0.10413986  
2023-06-05 10:52:55.220: [iter 53 : loss : 1.1667 = 0.0182 + 1.1373 + 0.0112, time: 69.023112]
2023-06-05 10:52:59.468: epoch 53:	0.02681326  	0.05804197  	0.04889797  	0.09298546  	0.10394789  
2023-06-05 10:54:23.086: [iter 54 : loss : 1.1661 = 0.0180 + 1.1369 + 0.0113, time: 83.085098]
2023-06-05 10:54:26.443: epoch 54:	0.02668384  	0.05764828  	0.04870806  	0.09285549  	0.10378804  
2023-06-05 10:55:36.996: [iter 55 : loss : 1.1657 = 0.0174 + 1.1369 + 0.0114, time: 70.020281]
2023-06-05 10:55:41.186: epoch 55:	0.02661122  	0.05744330  	0.04848684  	0.09247588  	0.10323880  
2023-06-05 10:56:59.675: [iter 56 : loss : 1.1652 = 0.0171 + 1.1367 + 0.0115, time: 77.967160]
2023-06-05 10:57:03.733: epoch 56:	0.02663014  	0.05759774  	0.04859956  	0.09262732  	0.10366575  
2023-06-05 10:58:21.889: [iter 57 : loss : 1.1649 = 0.0169 + 1.1365 + 0.0116, time: 77.619482]
2023-06-05 10:58:26.074: epoch 57:	0.02638069  	0.05695808  	0.04815909  	0.09210094  	0.10284242  
2023-06-05 10:59:50.830: [iter 58 : loss : 1.1648 = 0.0166 + 1.1365 + 0.0117, time: 84.240530]
2023-06-05 10:59:54.899: epoch 58:	0.02643437  	0.05697820  	0.04819300  	0.09221683  	0.10305034  
2023-06-05 11:01:13.246: [iter 59 : loss : 1.1646 = 0.0165 + 1.1364 + 0.0117, time: 77.825826]
2023-06-05 11:01:17.470: epoch 59:	0.02638861  	0.05682321  	0.04806780  	0.09199673  	0.10283551  
2023-06-05 11:02:42.864: [iter 60 : loss : 1.1642 = 0.0162 + 1.1361 + 0.0118, time: 84.868536]
2023-06-05 11:02:46.758: epoch 60:	0.02632232  	0.05645370  	0.04790594  	0.09189332  	0.10257619  
2023-06-05 11:04:11.397: [iter 61 : loss : 1.1643 = 0.0162 + 1.1362 + 0.0119, time: 84.127921]
2023-06-05 11:04:15.361: epoch 61:	0.02623390  	0.05626997  	0.04778589  	0.09183979  	0.10263620  
2023-06-05 11:05:41.435: [iter 62 : loss : 1.1637 = 0.0159 + 1.1358 + 0.0120, time: 85.542570]
2023-06-05 11:05:45.533: epoch 62:	0.02619918  	0.05629214  	0.04771955  	0.09157838  	0.10234155  
2023-06-05 11:07:10.232: [iter 63 : loss : 1.1636 = 0.0157 + 1.1358 + 0.0121, time: 84.183373]
2023-06-05 11:07:14.209: epoch 63:	0.02622443  	0.05625860  	0.04769910  	0.09150527  	0.10219034  
2023-06-05 11:08:26.677: [iter 64 : loss : 1.1630 = 0.0152 + 1.1356 + 0.0122, time: 71.956519]
2023-06-05 11:08:30.930: epoch 64:	0.02617710  	0.05609631  	0.04759695  	0.09141231  	0.10210665  
2023-06-05 11:09:39.683: [iter 65 : loss : 1.1633 = 0.0154 + 1.1357 + 0.0122, time: 68.249287]
2023-06-05 11:09:43.621: epoch 65:	0.02610448  	0.05597208  	0.04747693  	0.09104761  	0.10188103  
2023-06-05 11:10:54.519: [iter 66 : loss : 1.1630 = 0.0151 + 1.1356 + 0.0123, time: 70.382251]
2023-06-05 11:10:58.684: epoch 66:	0.02604450  	0.05579826  	0.04731840  	0.09087896  	0.10143840  
2023-06-05 11:12:08.392: [iter 67 : loss : 1.1624 = 0.0146 + 1.1355 + 0.0124, time: 69.181201]
2023-06-05 11:12:12.685: epoch 67:	0.02600186  	0.05568106  	0.04720127  	0.09073507  	0.10113154  
2023-06-05 11:13:22.557: [iter 68 : loss : 1.1626 = 0.0149 + 1.1353 + 0.0124, time: 69.364022]
2023-06-05 11:13:26.796: epoch 68:	0.02604607  	0.05574425  	0.04721881  	0.09079722  	0.10125797  
2023-06-05 11:14:35.400: [iter 69 : loss : 1.1622 = 0.0145 + 1.1351 + 0.0125, time: 68.061900]
2023-06-05 11:14:39.378: epoch 69:	0.02593871  	0.05554594  	0.04700285  	0.09023526  	0.10074084  
2023-06-05 11:15:46.780: [iter 70 : loss : 1.1622 = 0.0145 + 1.1351 + 0.0126, time: 66.889375]
2023-06-05 11:15:51.233: epoch 70:	0.02590082  	0.05553045  	0.04699992  	0.09039796  	0.10092229  
2023-06-05 11:17:05.190: [iter 71 : loss : 1.1618 = 0.0141 + 1.1350 + 0.0126, time: 73.431080]
2023-06-05 11:17:09.126: epoch 71:	0.02589924  	0.05543388  	0.04689478  	0.09004198  	0.10052959  
2023-06-05 11:18:20.091: [iter 72 : loss : 1.1618 = 0.0142 + 1.1349 + 0.0127, time: 70.435995]
2023-06-05 11:18:23.794: epoch 72:	0.02582188  	0.05518662  	0.04679500  	0.09015585  	0.10057117  
2023-06-05 11:19:34.284: [iter 73 : loss : 1.1619 = 0.0141 + 1.1350 + 0.0127, time: 69.977089]
2023-06-05 11:19:38.470: epoch 73:	0.02573032  	0.05498759  	0.04662682  	0.09005330  	0.10031013  
2023-06-05 11:20:49.531: [iter 74 : loss : 1.1616 = 0.0140 + 1.1348 + 0.0128, time: 70.538179]
2023-06-05 11:20:53.704: epoch 74:	0.02571768  	0.05485028  	0.04654109  	0.08976045  	0.10011736  
2023-06-05 11:22:04.987: [iter 75 : loss : 1.1614 = 0.0138 + 1.1347 + 0.0129, time: 70.759023]
2023-06-05 11:22:09.259: epoch 75:	0.02571297  	0.05481972  	0.04653743  	0.08986679  	0.10019261  
2023-06-05 11:23:20.281: [iter 76 : loss : 1.1611 = 0.0136 + 1.1346 + 0.0129, time: 70.484386]
2023-06-05 11:23:24.305: epoch 76:	0.02574928  	0.05484982  	0.04652160  	0.08979713  	0.10018596  
2023-06-05 11:24:49.044: [iter 77 : loss : 1.1611 = 0.0135 + 1.1346 + 0.0130, time: 84.213297]
2023-06-05 11:24:53.011: epoch 77:	0.02581558  	0.05495199  	0.04647197  	0.08926640  	0.09951152  
2023-06-05 11:26:11.050: [iter 78 : loss : 1.1610 = 0.0135 + 1.1345 + 0.0130, time: 77.512936]
2023-06-05 11:26:15.251: epoch 78:	0.02568295  	0.05466816  	0.04628185  	0.08917794  	0.09939447  
2023-06-05 11:26:15.251: Early stopping is triggered at epoch: 78
2023-06-05 11:26:15.251: best_result@epoch 28:

2023-06-05 11:26:15.251: Loading from the saved model.
2023-06-05 11:26:19.696: 		0.02812035  	0.06218058  	0.05147717  	0.09593949  	0.10741482  
/home/Thesis/model/general_recommender/SGL.py:146: RuntimeWarning: divide by zero encountered in power
  d_inv = np.power(rowsum, -0.5).flatten()
